% notation.tex
% Jeremy Barnes, 1999
% $Id$

\chapter{Notation and Acronyms}

\newcommand{\notationskip}{5mm}
\newcommand{\notspace}{\vspace{\notationskip}}

\section*{Notation}
\newcommand{\longexp}[1]{\parbox[t]{3in}{\raggedright #1}}
\begin{tabular}{l l c}
\bf{Symbols}		& \bf{Meaning}		& \bf{Section} \\
\hline \hline
$\bfx \in \calI$	& $\bfx$ is a sample
			& fig \ref{fig:supervised learning} \\

$y \in \calO$		& $y$ is a label
			& " \\

$X = ((\bfx_1, y_1), \ldots))$
			& Labeled samples, length $m$ (training dataset)
			& \ref{sec:formulation} \\

\notspace
$\calI, \calO$		& \longexp{Domain and range (input and output space;
                          usually $\calO \in \{\pm 1\}$)}
			& \ref{sec:domain and range} \\
$h(\cdot) \in \calH : \calI \rightarrow \calO$
			& A hypothesis (classifier)
			& \ref{sec:formulation} \\

$\calH$			& A set of hypotheses $h$
			& \ref{sec:representation of learning machines} \\

$f(\cdot) \in \calF : \calI \rightarrow \bbR$
			& \longexp{A non-thresholded hypothesis; usually 
			  $h(\cdot) = \sign(f(\cdot))$}
			& \ref{sec:margin formulation} \\

$\calF$			& A set of hypotheses $f$ 
			  (also $\calH = \sign(\calF)$)
			& " \\

\notspace
$\bbW : \calI^m \rightarrow (\calI \rightarrow \calO)$
 			& Learning machine ($\bbW(X) = h$)
			& " \\
$q = Q(y, \hat{y})$
			& Loss function
			& \ref{sec:loss function} \\

$R(h)$			& True or expected risk
			& \ref{sec:true risk} \\

$\underset{X}{R_{\emp}}(h)$
			& \longexp{Empirical risk (samples $X$ implicit if 
			  not specified)}
			& \ref{sec:empirical risk} \\

$R_{\emp}^w(h)$		& Weighted empirical risk (training error)
			& \ref{sec:weighted empirical risk} \\

\notspace
$R_{\emp}^{\gamma}(h)$	& Margin risk
			& \ref{sec:margin risk} \\

$\VCdim(\cdot)$		& VC dimension
			& \ref{sec:vcdim} \\

$\covert{\calX}{\epsilon}{d}$ &
			\longexp{Covering number at scale $\epsilon$ of $\calX$
			using norm $d$ (usually $d_{\infty}$; assumed if not
			specified)}
			& \ref{sec:covering numbers} \\


$\covert{\calX}{\epsilon}{k}$ &
			\longexp{
			Uniform covering number of $\calX$ at scale $\epsilon$
			over $k$ points}
			& " \\

\notspace
$\co_{p}(\calX)$, $\co(\calX)$
			& \longexp{$p$-convex hull of set $\calX$; $p=1$
			 assumed if not specified}
			& \ref{sec:p-convex} \\
$\bbB$, $\trainboost$	& AdaBoost, action of AdaBoost
			& fig \ref{fig:boosting algorithm} \\
$F(\bfx)$, $H(\bfx)$	& \longexp{Boosting hypothesis, nonthresholded
			\& thresholded, $H(\cdot) = \sign(F(\cdot))$}
			& " \\

$b_1, \ldots, b_t$	& AdaBoost classifier weights
			& " \\

$w_{t,1}, \ldots, w_{t,m}$ & AdaBoost sample weights
			& " \\

\hline
\end{tabular}
\par\par\noindent
A hat ($\hat{\ \ }$) means ``estimate''.  An asterisk ($\ \ ^{\ast}$) means
``optimal'' in some sense.


\section*{Acronyms}
\label{acronyms}

\begin{tabular}{l l l}

\bf{Acronym} & \bf{Meaning} & \bf{First introduced} \\ \hline \hline

SLT	& Statistical Learning Theory 	& \ref{acr:slt} \\
ERM	& Empirical Risk Minimisation 	& \ref{acr:erm} \\
SRM	& Structural Risk Mininmisation & \ref{acr:srm} \\
VC dimension & Vapnik-Chervonenkis dimension & \ref{acr:vcdim} \\
\hline

\end{tabular}

