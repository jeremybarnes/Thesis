% slt.tex
% Jeremy Barnes, 1999
% $Id$

\chapter{Statistical learning theory}
\label{chapter:slt}

This chapter provides a general theoretical framework upon which more
specific theory in chapter \ref{chapter:boosting} is constructed.  The
central questions that this chapter answers are: ``What is the
\emph{best} learning machine for a particular problem?'', ``How do we
choose it?'', and ``How well does it perform?''.

Some caution is required in using the word ``best''.  Learning
machines are \emph{mathematical models} of the systems they are trying
to learn.  As with any mathematical model they are necessarily an
approximation of the underlying system.  Thus, when we say
``best'' we mean ``an approximation which suits our application
well''.  Throughout this chapter, several quantitative
measures of how well a model suits an application are developed.
These measures are used to define procedures for choosing the ``best''
learning machine--however it is an important point that none of
these is the ``best'' in any absolute sense.  All are subjective to an
extent, as they all rely on \emph{a priori} assumptions.

We begin with a formal overview of the problem and notation used.
Section \ref{sec:comparing and selecting} then considers how to
compare learning machines, and develops an inductive procedure
(\emph{Empirical Risk Minimisation}--ERM) for selecting the ``best'' one.
Section \ref{sec:slt} develops bounds on how closely learning machines
generated by ERM approach the optimal theoretical performance.
Finally, section \ref{sec:overfitting} considers the problem of
overfitting and how to avoid this phenomenon by limiting the
complexity of our learning machines.  This leads to another inductive
procedure, known as \emph{structural risk minimisation} (SRM).


\section{Formulation of the machine learning problem}
\label{sec:formulation}

We first develop some conventions and notation.

\subsection{Learning machines}
\label{sec:learning machines}
Heuristically, a \emph{learning machine} is
%
\begin{quote}
	\ldots an algorithm (usually implemented in software) that
	estimates an unknown mapping (dependency) between a system's
	inputs and outputs from the available data, namely from known
	(input, output) samples. \cite{Cherkassky98}

\end{quote}
%
This description motivates the formal definition of supervised learning,
which is illustrated in figure \ref{fig:supervised learning}.  The
\emph{sample generator} produces samples $\bfx \in \calI$ drawn from a
fixed (but unknown) probability distribution $p(\bfx)$ over $\calI$%
\footnote{In practice, these samples are normally \emph{observed}
rather than \emph{generated}.},
where $\calI$ is the domain, or ``input space'', of the machine
learning problem.

\begin{linefigure}
\begin{center}
\begin{picture}(300,145)(30,30)
\put(30,110){\framebox(60, 60){\parbox{55pt}{\center{Sample generator}}}}
\put(150,110){\framebox(60, 60){\parbox{55pt}{\center{Learning machine
$\bbW$}}}}
\put(150,30){\framebox(60, 40){\parbox{55pt}{\center{Supervisor}}}}
\put(90, 140){\vector(1,0){60}}
\put(120,140){\line(0,-1){90}}
\put(120,50){\vector(1,0){30}}
\put(120,150){$\mathbf{x}$}
\put(210,50){\line(1,0){60}}
\put(270,50){\vector(0,1){20}}
\put(270,130){\line(0,-1){20}}
\put(280,60){$y^{\ast}$}
\put(240,70){\framebox(60, 40){\parbox{55pt}{\center{Label noise}}}}
\put(270,130){\vector(-1,0){60}}
\put(210,150){\vector(1,0){60}}
\put(280,150){$\hat{y}$}
\put(280,120){$y$}
\end{picture}
\end{center}
\caption{Supervised learning}
\label{fig:supervised learning}
\end{linefigure}

The \emph{supervisor} $h_{\sup}$ implements the unknown dependency
which the learning machine is trying to learn.  Given a sample $\bfx$,
it returns a label $y^{\ast} = h_{\sup}(\bfx)$ according to a
\emph{fixed but unknown} probability distribution $D$ on $\calI \times
\calO$, where $\calO$ is the range or ``output space'' of the machine
learning problem (all $y^{\ast} \in \calO$).

We furthermore allow $y^{\ast}$ to be subject to \emph{label noise}, a
random process that modifies $y^{\ast}$ in some manner% 
\footnote{We consider exactly \emph{what} manner shortly.}
to give $y$.  (Often, instead of presenting samples in serial to an
``online'' supervisor as shown in figure \ref{fig:supervised learning}
we are given a limited number of observations $X = ((\bfx_1, y_1),
\ldots, (\bfx_m, y_m))$ and present these in a batch.  No generality
is lost by assuming a serial presentation of samples; the learning
machine could simply store them internally and train in a batch at the
end).

The \emph{learning machine} $\bbW$ receives $\bfx$ as input and
generates an estimate $\hat{y} = h(\bfx)$ based on its current
hypothesis $h$ (which is an approximation to $D$).

The distinction between $h$ and $\bbW$ is subtle.  Formally, the
action of each may represented as
%
\begin{equation}
h : \calI \rightarrow \calO \qquad \bbW : (\calI \times \calO)^m
\rightarrow (\calI \rightarrow \calO)
\end{equation}
%
We see that the learning machine $\bbW$ receives a number $m$ of
(sample, label) pairs (the \emph{training data}) and returns a
hypothesis $h$.  This hypothesis $h$ is a function mapping samples
onto labels.  Thus, the relationship between $\bbW$ and $h$ is that
$\bbW$ \emph{generates} $h$.  (For a concrete example, appendix
\ref{appendix:stumps} describes a very simple learning machine called
\emph{decision stumps}.)

The key feature of $\bbW$ is its learning behaviour: it uses the error
($y - \hat{y}$) as feedback to improve its approximation (by changing
$h$) as further training data is presented.  The goal is for $\bbW$ to
generate a hypothesis $h$ that generates the correct label for any
sample, so that $h(x) - y^{\ast} = 0$ for \emph{all} samples
$\mathbf{x}$, including samples it has not seen%
\footnote{Further discussion of exactly what constitutes ``learning
behaviour'' is beyond the scope of this thesis; see \cite{Anthony98}.}.

Matters are complicated somewhat by the presence of \emph{label
noise}.  This noise $q$ is modelled as a probability over all samples
that the observed sample is wrong:
\footnote{Other more complex models of noise are sometimes used (such
as \emph{attribute noise} or models based upon full probability
density functions).  These are beyond the scope of this thesis.}
%
\begin{equation}
q = \Pr_{\bfx \in \calI}(y \neq y^{\ast})
\end{equation}

In practical applications, figure \ref{fig:supervised learning} tells
only half of the story.  Once training is complete%
\footnote{Section \ref{sec:theoretical overfitting} discusses how to
tell when training is complete}%
, the final hypothesis $h^{\ast}$ is removed from the learning machine
and supervisor, and used \emph{unsupervised} on further (unseen) input
samples.  For example, a hypothesis generated by a learning machine
that had been trained on the ``churn'' dataset described in chapter
\ref{chapter:intro} would then be used to determine if new customers
were likely to churn.  Although this thesis concentrates primarily on
\emph{training} learning machines, it is important to keep in mind that
the final hypothesis may then be used to make predictions on
\emph{unseen} samples.  The training process is not an end in itself.


\subsection{Domain and range of learning machines}
\label{sec:domain and range}

The symbols \calI\ and \calO\ are used to specify the domain and
range of the learning problem: all samples $\bfx \in \calI$; all
labels $y \in \calO$.  $\calI$ depends upon number and
domain of input variables in the problem; in most examples in this
thesis $\mathcal{I} = [0,1]^d$, where $d$ is 1 or 2.  Real world
problems typically have much larger domains: the ``churn'' dataset
introduced in section \ref{sec:churn example} has nine real, seven
integer and two boolean attributes giving $\calI = \bbR^9 \times \bbN^7
\times \{0, 1\}^2$.

$\mathcal{O}$ is a subset of $\mathbb{R}^o$, where $o$ is the number
of output variables.  In many cases, there is only one output
variable ($o=1$) or multiple output variables are independent and
can each be generated by a separate learning machine.  Thus we
restrict our attention to $o=1$.

The distinction between \emph{classification} and \emph{regression}
problems is made on the size of $\calO$.

A learning machine is solving a \emph{regression} problem when
$|\calO|$ is infinite.  Usually, this means that $\calO$ is some
interval on $\bbR$.  This thesis does not consider regression
problems; it is usually straightforward to generalise results to apply
to regression problems.

When $|\calO| < \infty$ we are solving a \emph{classification}
problem.  (Learning machines which solve a classification problem are
often called \emph{classifiers}).  Permissible $y$ values are now
a finite number of discrete \emph{categories}:
%
\begin{equation}
\calO = \{y_1, y_2, \ldots, y_o\}
\end{equation}

A classifier operates as an equivalence relation, splitting the input
space $\calI$ into a set of disjoint regions $\{ \mathcal{R}_{y_1}
\ldots \mathcal{R}_{y_o} \}$, each of which corresponds to a value of
the label.  The ``decision boundary'' (illustrated in figure
\ref{fig:classification problem}) is a useful representation of the
boundary between these regions.

\begin{linefigure}
\begin{center}
\includegraphics{figures/classification_problem.epsg}
\end{center}
\begin{capt}{The classification problem}{fig:classification problem}
Part (a) shows the input $X$ to a learning machine: a set of
labelled points in $\bbR^2 \times \{ \pm 1 \}$.  The $\times$ symbol
represents $y=-1$ and the $\circ$ symbol $y=1$.  Part (b) shows the
output: a decision boundary (dashed line) generated by a hypothesis
$h(\bfx)$.  The two regions $\calR_{-1}$ and $\calR_{+1}$ are also
shown.  The dotted lines indicate the \emph{minimum margin} over the
training samples (section \ref{sec:margin formulation}).  Note that
there are many possible hypotheses (decision boundaries) that will
correctly classify (separate) all of the input data.  Here, $h((x_1,
x_2)) = \sign\{ (x_2 - b) - m x_1 \}$.
\end{capt}
\end{linefigure}

For the remainder of this thesis, we restrict our attention to
\emph{binary classification problems} where $\mathcal{O} = \{\pm
1\}$.  As a result, the terms ``classifier'' and ``hypothesis'' become
equivalent, and are used interchangeably.


\subsection{Representation of learning machines}
\label{sec:representation of learning machines}

As mentioned above, a learning machine $\bbW$ takes a labelled set of
data $X = ((\bfx_1, y_1), \ldots, (\bfx_m, y_m))$, where $x_i \in
\calI$ and $y_i \in \calO$, and outputs a hypothesis $h(\cdot) : \calI
\rightarrow \calO$ chosen according to some deterministic procedure.
It is often convenient to represent learning machines by the set
%
\begin{equation}
\calH = \{h : \left( \exists X : \bbW(X) = h \right) \}
\end{equation}
%
This set contains all hypotheses that the learning machine $\bbW$ could
generate; note however that the deterministic procedure through which
a $h$ is selected from $\calH$ is not explicit in this formulation%
\footnote{As a notational convenience, when we apply an operation to
$\calH$ (for example $\sign(\calH)$), we actually apply that operation
to all elements of that set.}.

\subsection{Alternative formulation for binary classification problems}
\label{sec:margin formulation}

Many binary classifiers produce their output by thresholding a real
valued function.  It is often useful to deal with this continuous
function directly, rather than with the discrete thresholded version
(which contains less information).  We write this relationship as  
%
\begin{equation}
h(\bfx) = \sign \left( f(\mathbf{x}) \right)
\label{eqn:marginal classification formulation}
\end{equation}
%
where $h$ is the thresholded hypothesis($h : \calI \rightarrow \{\pm
1\}$) and the function $f : \calI \rightarrow \mathbb{R}$.  (Note that
$f$ is only used \emph{internally} in (\ref{eqn:marginal
classification formulation}); the \emph{external} representation $h$
still has a discrete output).  We then define the \emph{margin} of a
labeled sample as follows. 

\begin{definition}[Margin of a sample]
Given a labeled sample $(\bfx, y)$ and a real-valued hypothesis
$f$, we define the \emph{margin} of the sample as
%
\begin{equation}
m_{f}(\bfx,y) = y f(\bfx)
\end{equation}
%
which is a real valued function, positive if the hypothesis is correct
on the given sample and negative otherwise.
\end{definition}

In practice, it usually turns out that the magnitude of the margin at
a point is an indication of how ``confident'' the hypothesis is
of its classification.  A large value of the margin indicates that
there is little uncertainty in the classification of the point in
question.  Thus, we would expect that a hypothesis with large margins
would have good generalisation performance.  Geometrically, for ``well
behaved'' functions, the distance between a point and the decision
boundary will roughly correspond to the magnitude of the margin at
that point (see figure \ref{fig:classification problem})%
\footnote{The discussion is simplified for the sake of clarity.
It is possible to generate margins of arbitrarily large size by
scaling $f$ by a constant $\beta$.  We are actually, therefore,
interested in the \emph{normalised margin} $m_F(\bfx, y)/|f|$ where
$|f|$ is (for example) an upper bound over all samples of the
magnitude of $f$.  It is a minor difference in this thesis as the
algorithms considered all normalise $f$ anyway.}.

We will always call the real-valued hypotheses $f$ and the
thresholded hypotheses $h$, where $h = \sign(f)$.  We use similar
notation for the sets of hypotheses: $\calH = \sign \calF = \{
 \sign(f) : f \in \calF \}$.  Many results later in this thesis make
use of the margins formulation; in particular some generalisation
performance bounds in section \ref{sec:slt} are defined in terms of
the margins, and we show that the limiting behaviour of the AdaBoost
algorithm (chapter \ref{chapter:boosting}) is to maximise the minimum
margin over a set of training samples.

\section{Comparing and selecting hypotheses}
\label{sec:comparing and selecting}

Now that we have defined what we mean by a ``learning machine'' and a
``hypothesis'', we consider how to construct a learning machine--in
particular, how the learning machine can compare two hypotheses to
determine which is ``better''.  In this section we discuss suitable
means of comparison, and accordingly construct a procedure that a
learning machine can follow to generate a suitable hypothesis.

\subsection{Loss functions}
\label{sec:loss function}

One appropriate metric that can be used is known as a ``loss
function''.  The input to a loss function is an ordered pair
$(y, y^{\ast})$ where $y$ is the output of the hypothesis , and
$y^{\ast}$ is the ``correct'' output (that generated by the
supervisor--see figure \ref{fig:supervised learning}).  The output of
the loss function is a real $q$ which describes how ``bad'' this
estimate is.  In symbols, the loss function is written
%
\begin{equation}
q = Q(\yh, y)
\end{equation}
%
where $q \in \mathbb{R}$ and $\yh, y \in \calO$.

In this thesis we usually use the ``misclassification loss function'',
which is defined as 
%
\begin{equation}
Q(\yh, y) = \left\{
\begin{array}{ll}
	0	&	\qquad \mbox{if $\yh = y$} \\
	1	&	\qquad \mbox{if $\yh \neq y$}
\end{array}
\right.
\label{eqn:misclassification loss function}
\end{equation}

This loss function is very easy to understand: zero if right, one if
wrong.  Adding together the loss functions over several samples simply
counts the number of mistakes.


\subsection{True risk}
\label{sec:true risk}

We can use our loss function $Q$ to compare the performance of a
number of hypotheses over the entire input space.

\begin{definition}[True risk]
We define the \emph{true risk} of a classifier $h(\bfx)$ given a
supervisor function $h_{\sup}(\bfx)$ as 
%
\begin{equation}
R(\hat{h}) = \int_{\calI} Q(h(\bfx), h_{\sup}(\bfx)) \: p(\bfx) \: d\bfx
\label{eqn:true risk}
\end{equation}
%
where $Q$ is the misclassification loss function
(\ref{eqn:misclassification loss function}) and $p(\bfx)$ is the
probability density function of $\bfx$.
\end{definition}


Using our loss function (\ref{eqn:misclassification loss function}), the
value of $R$ ($0 \leq R \leq 1$) is easily seen to be the proportion
of samples that are misclassified.  If two classifiers are compared
using this metric, then the one with the least probability of making a
mistake will have a lower loss.  This is intuitively a sensible
measure to use.

The goal of machine learning is generate learning machines that
minimise the true risk.  Unfortunately, it is impossible in
principle to evaluate (\ref{eqn:true risk}) as one does not know
either $h(\bfx)$ or $p(\bfx)$.   Instead, we know $m$ (possibly noisy)
observations $(\bfx_1, y_1),\: \ldots, (\bfx_m, y_m)$.

In section \ref{sec:overfitting} we will describe a method (structural
risk minimisation) that comes close to minimising the true risk.
However, for the time-being we will consider a measure that we
\emph{can} actually calculate: the \emph{empirical risk}.


\subsection{Empirical risk}
\label{sec:empirical risk}
The empirical risk is an estimate of true risk, that can be evaluated
over a collection of training samples.

\begin{definition}[Empirical risk]
Given $m$ observations $X = ((\bfx_1, y_1), \ldots, (\bfx_m, y_m))$
and a hypothesis%
\footnote{We drop the hat from our hypothesis $\hat{h}$ when it is
clear that we are not discussing a supervisor function.}
$h \in \calH$, we define the \emph{empirical risk} of $h$ over $X$ as
%
\begin{equation}
R_{\emp}(h) = \underset{X}{R_{\emp}(h)} = \frac{1}{m} \sum_{i=1}^{m}
Q(h(\bfx_i), y_i)
\end{equation}
\end{definition}

The law of large numbers ensures that as $m \rightarrow \infty$, the
empirical risk approaches the true risk (so long as $Q$ is unbiased).
Convergence for finite $m$ is discussed in section \ref{sec:slt}.


\subsection{Weighted empirical risk}
\label{sec:weighted empirical risk}

The empirical risk can be extended to the case where the samples in
$X$ are not equally important (or reliable), by giving each sample a
weight $w_i$.

\begin{definition}[Weighted empirical risk]
\label{def:weighted empirical risk}
Given $m$ observations $X = ((\bfx_1, y_1), \ldots, (\bfx_m,
y_m))$ a normalised vector of weights $W = (w_1, \ldots, w_m) \in
\bbR^m$ where $\sum_{i} w_i = 1$ and a hypothesis $h \in
\calH$, we define the \emph{weighted empirical risk} of $h$ over $X$ as 
%
\begin{equation}
R_{\emp}^W(h) = \sum_{i=1}^{m} w_i Q(h(\bfx_i), y_i)
\end{equation}
\end{definition}
%
It is clear that when all samples are equally weighted ($w_i = 1/m$),
this definition is equivalent to that of empirical risk.

Weighted training samples will not be considered further in this
chapter; in all cases is is a simple matter to extend results using
unweighted samples to accept weighted samples.  They are introduced
because the boosting algorithm uses them (chapter
\ref{chapter:boosting}).

\subsection{Margin risk}
\label{sec:margin risk}

We define another risk functional that will be of use later on.
Recall from section \ref{sec:margin formulation} that the margin is a
real-valued quantity that indicates how confident a classification
is.  Margins with positive values indicate that the classification of
a point was correct.

We use these margins to introduce a stronger form of risk functional.
Specifically, we include in our risk values not only those samples
that were classified wrongly, but also those samples that were
\emph{nearly} classified wrongly. 

\begin{definition}[Margin risk]
\label{def:margin risk}
Given a series of labelled training examples $X = ((\bfx_1, y_1),
\ldots, (\bfx_m, y_m))$ and a margin $\gamma>0$, we define the
\emph{margin risk} of a hypothesis $h = \sign(f)$ as
%
\begin{equation}
R^{\gamma}_{\emp}(h) = \frac{1}{m} \left| \left\{ i : y_i f(\bfx_i) \leq
\gamma \right\} \right| 
\end{equation}
\end{definition}
%
The margin risk is clearly the proportion of samples with a margin
less than or equal to $\gamma$.  From figure \ref{fig:classification
problem} it is clear that $R^{\gamma}_{\emp}(f)$ is nondecreasing with
increasing $\gamma$.


\subsection{Empirical risk minimisation}
\label{sec:erm}
\label{acr:erm}

We have been concerned thus far with a \emph{single} hypothesis
$h$.  However the goal of machine learning is to pick the best
$h$ from a set $\calH$ of possible hypotheses, given a series of
training samples $X$.  One way of doing so is the \emph{empirical risk
minimisation} inductive principle (ERM).

\begin{definition}[Empirical risk minimisation]
Suppose we are given a set $\calH$ of hypotheses, a loss
function $Q$, and a set of training data $X = ((\bfx_1, y_1) \ldots
(\bfx_m,y_m))$.  Then the \emph{empirical risk minimisation} inductive
principle selects the learning machine $h^{\ast} \in \calH$ that
minimises the empirical risk:
%
\begin{equation}
h^{\ast} = \argmin_{h \in \calH} R_{\emp}(h)
\label{eqn:erm}
\end{equation}
\end{definition}

It is an important detail that the set $\calH$ of possible hypotheses
is specified \emph{outside} the ERM principle.  The selection of this
set corresponds to the input of \emph{a priori} knowledge into the
learning process---indeed it can be shown that there cannot be a
useful completely general learning algorithm%
\footnote{This is sometimes called the ``no free lunch''
principle.}. 

In summary, this section considered several ways of comparing the
performance of hypotheses.  The true risk is an ideal measure, but cannot be
evaluated in practice.  The empirical risk \emph{can} be evaluated over a
given training set, and leads to the empirical risk minimisation
procedure.

\section{Statistical learning theory}
\label{sec:slt}
\label{acr:slt}

We now turn to a fundamental question of machine learning: given a
hypothesis $h \in \calH$ that was selected via ERM over a set $X$ of
training examples $X$, how well can we expect it to generalise to 
\emph{unseen} examples?  The theory that attempts to answer this
question is known as \emph{statistical learning theory} (SLT).  This
theory is the subject of several texts \cite{Vapnik98, Cherkassky98,
Bartlett98a}. 

\subsection{Performance bounds over finite $\calH$}

By treating the probability of an error on a particular sample as a
binomial random variable and bounding the tail of this distribution,
the probability of an error for a finite hypothesis class ($|\calH| <
\infty$) can be bounded.  This procedure leads to the following theorem.

\begin{theorem}[Upper bound for $|\calH| < \infty$]
If $h \in \calH$ minimises empirical risk on a training set $X$
with $m$ examples, then with probability at least $1 - \delta$
%
\begin{equation}
R(h) \leq R_{\emp}(h) + \sqrt{\frac{2}{m}
\log \left( \frac{2\  |\calH|}{\delta} \right)}
\end{equation}
\end{theorem}

The requirement that $|\calH| < \infty$ is very restrictive in
practice; even a class of learning machines parameterised by one real
number fails to meet this condition.  The \emph{Vapnik-Chervonenkis
dimension} has less restrictive requirements, and thus turns out to be
a more useful measure.

\subsection{VC dimension}
\label{acr:vcdim}
\label{sec:vcdim}

The VC (Vapnik-Chervonenkis) dimension is another measure of the
complexity of a class of hypotheses $\calH$ that is useful for many
problems.  Its definition relies on the following definition.

\begin{definition}[Shattering]
Consider a set $S$ of $p$ points $\bfx_1 \ldots \bfx_p \in \{\pm 1\}$.
Then a function class $\calH$ is said to \emph{shatter} $S$ if and
only if there exist functions $h_1, \ldots, h_n \in \calH$ such that
each of the $n = 2^p$ possible classifications of the points are
produced: 
%
\begin{equation}
\left\{ (h_i(\bfx_1), \ldots, h_i(\bfx_p)): i=1, \ldots, n \right\}
= \{\pm 1\}^p
\end{equation}
%
\end{definition}

It is an important feature of the definition that \emph{every}
possible classification of the points must produced.  If there exists
even one classification that cannot be produced, then the definition
is \emph{not} satisfied.  Figure \ref{fig:shattering} provides an
example of the shattering of points in $\bbR^2$ plane by the set of
straight lines in $\bbR^2$. 

\begin{linefigure}
\begin{center}
\includegraphics{figures/shattering.epsg}
\end{center}
\begin{capt}{Shattering a set of points}{fig:shattering}
The closed and open circles are data points; the lines are decision
boundaries.  Part (a) shows how three points on $\bbR^2$ can be
shattered by linear decision boundaries.  Part (b) shows that four
points cannot be shattered by a linear decision boundary; in
particular no straight line can separate the $\bullet$ points from the
$\circ$ points (try it!)
\end{capt}
\end{linefigure}

\begin{definition}[VC dimension]
\label{thm:vcdim bound}
Given a function class $\calH$ on $\calI \rightarrow \calO$, the VC
dimension of the class is defined as
%
\begin{equation}
\VCdim(\calH) = \max_{d} : ( \exists S = \{\bfx_1, \ldots, \bfx_d\} 
\mbox{\rm where $\calH$ shatters $S$})
\end{equation}
%
where the set $S$ contains no repetitions ($i \neq j \Rightarrow
\bfx_i \neq \bfx_j$).
\end{definition}

In other words, $\VCdim(\calH)$ is the largest number $d$ of distinct
points that \emph{can} be shattered by $\calH$.  Note that this
definition only requires that there be \emph{one} set of points that
can be shattered; not that all sets of points may be shattered.  An
extension of the ideas in figure \ref{fig:shattering}, the VC
dimension of lines in $\mathbb{R}^d$ is $d+1$.

The VC dimension plays a role in a bound on the generalisation
performance of a hypothesis selected via ERM. 

\begin{theorem}[VC upper bound \cite{Anthony98}]
Let $\calH$ be a class of functions mapping from a set $\calI$ to $\calO =
\{-1, 1\}$ and having VC-dimension $d$.  Then for any probability
distribution on $\calI \times \{-1,1\}$, with probability $1-\delta$
over $m$ random examples $X$, there exists a constant $c$ such that
the hypothesis $h^{\ast}$ selected via ERM satisfies
\begin{equation}
R(h^{\ast}) \leq R_{\emp}(h^{\ast}) + \sqrt{ \frac{c}{m} \left[ d
+ \log \left ( \frac{1}{\delta} \right) \right] }
\end{equation}
\end{theorem}

There is also a lower bound with a similar form that applies to
hypotheses selected by \emph{any} inductive principle.  These two
bounds between them appear to tell the whole story: we know that the
generalisation ability of hypotheses chosen via ERM lies within a
confidence interval of a known size of the true risk, and we know that
a hypothesis chosen by another principle cannot be asymptotically
better.  The story appears to be complete\ldots until we introduce a
new twist: for many function classes $\calH$ the confidence interval
may be large or even infinite, due to properties of the VC dimension
which are now explored.  Learning is still, nevertheless, possible.

\subsection{Covering numbers}
\label{sec:covering numbers}

The problem with the VC dimension is that it is sensitive to behaviour
on an arbitrarily small scale.  Figure \ref{fig:vcdim problems} shows
an extreme example.  We use \emph{covering numbers} to avoid some of
these problems.

\begin{linefigure}
\begin{center}
\includegraphics{figures/vcdim_problems.epsg}
\end{center}
\begin{capt}{Problems with scale insensitive measures of
complexity}{fig:vcdim problems}
We consider the decision boundaries of two hypothesis classes.  Part
(a) shows the decision boundary of class $\calH_{\circ}$,
which consists of circles centered at $(\bar{x}_1, \bar{x}_2)$
(indicated by $\times$) with a radius function $r(\theta) = 1$.  These
decision boundaries are simply circles; $\VCdim(\calH_{\circ}) = 4$.

Part (b) shows a second hypothesis class $\calH_{\ast}$.  This class
has a radius function $r(\theta) = 1 + \alpha \sin(\omega \theta)$.
Despite $\calH_{\circ} \rightarrow \calH_{\ast}$ as $\alpha
\rightarrow 0$, $\VCdim(\calH_{\ast}) = \infty$ for all $\alpha >
0$ (a consequence of the sampling theorem; see \cite{Cherkassky98}).
In other words, two function classes with 
decision boundaries that are arbitrarily close have wildly differing
VC dimensions.  Clearly, in this case the VC dimension is \emph{not} a
good measure of complexity.
\end{capt}
\end{linefigure}

Covering numbers are a way of measuring the effective ``size'' of a
class of functions $\calH$ at a given scale $\epsilon$.  The following
definition is used by the definition of covering numbers:

\begin{definition}[Restriction of a function \cite{Anthony98}]
\label{def:restriction}
Consider a set of points $X = \{\bfx_1, \ldots, \bfx_k\} \in \calI^k$,
and a function $u$ where $u : \calI \rightarrow \mathbb{R}$.  Then
we define the \emph{restriction of $u$ to $X$} as a vector
%
\begin{equation}
\mathbf{u} = u_{|X} = (u(\bfx_1), \ldots, u(\bfx_n))
\end{equation}
\end{definition}

We now can define exactly what we mean by ``to cover'':

\begin{definition}[Covering and covering numbers]
\label{def:covering}
\label{def:covering numbers}
Suppose we are given two function classes $\calH$ and $\calS$ where $\calS
\subseteq \calH$ and $f \in \calH \Rightarrow f : \calI \rightarrow
\mathbb{R}$, a set of points $X = (\bfx_1, \ldots, \bfx_n) : \bfx_i \in
\calI$, and a scale $\epsilon > 0$.  Then we define the metric
$d_{\infty}$ as 
%
\begin{equation}
d_{\infty}(\mathbf{p}, \mathbf{q}) = \max_{i} |p_i - q_i|
\end{equation}
%
(the usual $\infty$ norm).  We say that \emph{$\calS$ is an
$\epsilon$-cover for $\calH$ with respect to $X$} if and only if 
%
\begin{equation}
\forall u \in \calH \exists v \in \calS : d(u_{|X}, v_{|X}) < \epsilon
\end{equation}

We further define the \emph{``$d$ $\epsilon$-covering number of
$\calH$ with respect to $X$''} as
% 
\begin{equation}
\covert{\calH}{\epsilon}{X} = \sup_{\calS \subseteq
\calH}|\calS| : \mbox{$\calS$ is an $\epsilon$-cover for $\calH$
w.r.t. $X$}
\end{equation}
\end{definition}

Figure \ref{fig:covering} illustrates the notion of covering; both
geometrically and for a set of functions.

\begin{linefigure}
\begin{center}
\includegraphics{figures/covering.epsg}
\end{center}
\begin{capt}{Illustration of covering numbers}{fig:covering}
Part (a) shows a geometric representation of a set of points
(indicated by $\times$) covering a set in $\bbR^2$ (after
\cite{Anthony98}).  Part (b) illustrates covering applied to
functions.  Function $f_1$ (thin solid line) is covered by function
$g$ (thick solid line) at the two points indicated with $\bullet$, as
it is within $\epsilon$ at both points. However, function $f_2$
(dashed line) is \emph{not} covered by $g$ as it is not within
$\epsilon$ at the leftmost point.  Note that in part (b) we are only
looking at \emph{single} functions; in the text we are considering
\emph{classes} of functions.
\end{capt}
\end{linefigure}

This definition suffices if we have a fixed $X$ (for example, we want
to develop bounds on the \emph{empirical} risk).  By dropping the
dependence on $X$, and instead taking a maximum over all sets
$X$ of size $m$, we can obtain a result that is applicable to the
\emph{true} risk: 

\begin{definition}[Uniform covering numbers]
Given the same parameters as definition \ref{def:covering}, we define
the \emph{uniform covering number} as
\begin{equation}
\label{eqn:uniform covering numbers}
\covert{\calH}{\epsilon}{m} = \max_{\forall X \in \calI^m} \left\{
\covert{\calH}{\epsilon}{d_{\infty}} \right\}
\end{equation}
\end{definition}

The uniform covering number is used in a similar manner to the VC
dimension, to bound the generalisation performance of a learning
algorithm. 

\subsection{Bounds using covering numbers}
\label{sec:covering number bounds}

The result stated in this section is an important part of the
background of this thesis.  It is derived in Anthony and Bartlett
\cite{Anthony98} by bounding the expectation of the value of the
uniform covering numbers; and converted in a straightforward manner to
the form shown below.

\begin{theorem}[Convergence bound using covering numbers]
\label{thm:covering number bound}
Consider a hypothesis space $\calF : f \in \calF \Rightarrow f : \calI
\rightarrow \mathbb{R}$, a series of $m$ training samples $X \in
(\calI \times \calO)^m$, and a margin $0 \leq \gamma < 1/2$.  Then
with probability at least $1 - \delta$,  
\begin{equation}
R(f) \leq R_{\emp}^{\gamma}(f) + \sqrt{\frac{8}{m} \log \left( \frac{2
\covert{\calH}{\gamma/2}{2m}}{\delta} \right)}
\label{eqn:covering number bound}
\end{equation}
\end{theorem}

There are two important differences between the above theorem and the
corresponding VC dimension bound (theorem \ref{thm:vcdim bound}). 
Firstly, theorem \ref{thm:covering number bound} uses a scale
sensitive dimension (the uniform covering numbers) instead of the VC
dimension.  Secondly, it uses the margin risk (definition
\ref{def:margin risk}) instead of the empirical risk.  These are linked
by the $\gamma$ parameter, which appears both as the margin threshold
for the margin risk and as the scale for the uniform covering numbers.

By inspection of (\ref{eqn:covering number bound}) we can see that the
confidence interval will decrease as $\gamma$ increases.  However,
increasing $\gamma$ will have the opposite effect on
$R_{\emp}^{\gamma}(f)$ as the margin risk is nondecreasing with
$\gamma$.  As a result, there is a tradeoff here between the two
terms.  This is explored in section \ref{sec:overfitting}.

\subsection{Application to $p$-convex hulls ($0 < p < 2$)}
\label{sec:p-convex}

The theoretical motivation behind the algorithms developed in this
thesis uses the covering number of a $p$-convex hull ($\co_p(\calH)$).
This section defines a $p$-convex hull and gives a qualitative result
on the covering numbers for $0 < p < 2$.  A full discussion appears in
\cite{Williamson99}.

\begin{definition}[$p$-convex hull]
\label{def:p-convex hull}
The $p$-convex hull (strictly speaking, the $p$-absolutely convex hull) of
a set $\calH$ of functions (where $p>0$) is defined as
%
\begin{equation}
\cop (\calH) =
 \bigcup _{n \in \mathbb{N}}
\left\{
 \sum_{i=1}^{n}
 \alpha _i
f_i : f_1, \ldots, f_n \in \calH, \quad
 \alpha _1, \ldots, \alpha _n \in \bbR, \quad
 \sum_{i=1}^{n} | a_i |^p \leq 1
\right\}
\end{equation}
\end{definition}

Thus the $p$-convex hull of $\calH$ is a subset of $\lin(\calH)$, the
set of all possible linear combinations of functions in $\calH$.
While the $p$-convex hull of a set of \emph{functions} is hard to
visualise, the $p$-convex hull of points in a Euclidean space is a
useful visual aid.  See figure \ref{fig:p-convex}.  This figure also
illustrates that $p$-convex hulls define a \emph{structure}:  If $0
\leq p_1 \leq p_2 \leq \cdots \leq p_z$, then $\co_{p_1} \subseteq
\co_{p_2} \subseteq \cdots \subseteq \co_{p_z}$.  This property is
used in section 
\ref{sec:theoretical overfitting}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/p_convex.epsg}
\end{center}
\begin{capt}{The $p$-convex hull of two variables}{fig:p-convex}
The figure illustrates the shape of a $p$-convex hull of two variables $u$
and $v$ for various values of $p$.  The smaller $p$ is, the closer to
the axes the hull is.  

We define two sets: the points on the boundary are termed ``on'' the
hull, and those within the boundary ``in'' the hull.  The equation of
the curves is $\left( u^p + v^p \right) ^{(1/p)} = 1$.
\end{capt}
\end{linefigure}

We now state a theorem that gives an approximate bound on the covering
numbers of a $p$-convex hull.  This result is central to the thesis.

\begin{theorem}[Covering numbers of a $p$-convex hull
\cite{Williamson99}]
\label{thm:p-convex bound}
Consider a hypothesis class $\calH$ where the covering numbers grow as
%
\begin{equation}
\cover{\calH}{\epsilon} \approx \left( \frac{1}{\epsilon} \right) ^d
\label{eqn:cop restriction}
\end{equation}
%
and a number $0 < p < 2$.  Then for a fixed integer $m>0$, the
uniform covering number at scale $\epsilon > 0$ can be approximated by
%
\begin{equation}
\label{eqn:approx p-convex bound}
\log_2 \covert{\co_p(\calH)}{\epsilon}{m} \approx c(p) \: d \: \left(\frac{1}
{\epsilon}\right)^{\frac{2p}{2-p}} \log_2 \left(\frac{1}{\epsilon}\right)
\end{equation}
%
where $c(p)$ is a constant that depends only upon $p$.
\end{theorem}

It can be shown that equation (\ref{eqn:cop restriction}) holds for
finitely parameterised classes (a simple geometric argument may help:
in $[0,1]^d$, if the space is tiled with $n$ hypercubes of size
$\epsilon$, then it takes $2^d$ hypercubes of size $\epsilon/2$; thus
(\ref{eqn:cop restriction}) holds).
 
Theorem \ref{thm:p-convex bound} can be made a more precise, but at
the expense of a \emph{lot} more complexity.  This extra complexity is
unnecessary for our purposes; it is the \emph{form} of this bound that
motivates the work in this thesis. In particular, it should be noted
that the approximate bound (\ref{eqn:approx p-convex bound}) decreases
with decreasing $p$ for $0 < p \leq 2$ (assuming that $c(p)$ remains
nearly constant).

Finally, we emphasise that $\co_{p}(\calH)$ is usually a \emph{lot}
``richer'' (contains a greater variety of hypotheses) than $\calH$,
especially for $p \geq 1$.  Extending our hypothesis space in this
manner is one way to markedly improve the empirical risk; indeed this
is exactly what the Boosting algorithm (chapter
\ref{chapter:boosting}) does.


\section{Overfitting}
\label{sec:overfitting}

We have now developed enough theory to analyse the problem that
this thesis is attempting to solve: \emph{overfitting}.  The problem
of overfitting is quite familiar to anyone who has had to fit a
polynomial (say) to noisy data: to what extent should the data be
trusted?  By fitting a high-order polynomial (a complex model), we may
fit the data perfectly, yet the underlying distribution poorly.  We
call this ``overfitting''.  Conversely, by fitting a low-order
polynomial (a less-complex model), we may fit the underlying
distribution well but not our data.  Figure \ref{fig:srm} illustrates
this point.

\begin{linefigure}
\begin{center}
\includegraphics{figures/overfitting.epsg}
\end{center}
\begin{capt}{Overfitting with polynomials: two extreme
examples.}{fig:overfitting}
The $\bullet$ symbols are noisy data points; the dashed line is the
underlying distribution.  In part (a) an order 1 polynomial has been
fitted.  In part (b) an order 10 polynomial has been fitted.  The fit
for the first order polynomial is evidently closer to the underlying
distribution.  Thus, more complex models do not necessarily generalise
better.
\end{capt}
\end{linefigure}

Overfitting is reduced by trading off model complexity against
empirical risk.  In this section, we will first describe how 
overfitting can be detected using two separate datasets (a training
dataset and a test dataset), and give some examples of overfitting
using the algorithms considered in this thesis.  We will then consider
two explanations of overfitting: a qualitative explanation based on
general properties of model complexity, and a more quantitative
explanation based on theory developed earlier in this chapter.


\subsection{Theoretical overfitting and structural risk minimisation}
\label{sec:theoretical overfitting}
\label{acr:srm}

Formally, we consider a series of hypothesis classes $\calH_1, \ldots,
\calH_n, n \leq \infty$ (recall that a hypothesis class contains all
possible output hypotheses of a learning algorithm) and some
\emph{appropriate} measure of complexity  $|\cdot|$.  (By
``appropriate'', we mean ``suitable for the problem at
hand''. Depending upon the situation, we might choose $|\calH_i| =
\VCdim(\calH_i)$ or $|\calH_i| = \cover{\calH_i}{\epsilon}$, for
example).  We also assume without loss of generality that $\calH_1
\subseteq \calH_2 \subseteq \cdots \subseteq \calH_n$, and thus using
any sensible complexity measure, $|\calH_1| \leq |\calH_2| \leq \cdots
\leq \calH_n$.  This sequence of increasingly complex hypothesis
classes is called a \emph{structure}.

Our goal is to find the optimal $\calH_i$, in the sense that the
true risk (\ref{eqn:true risk}) is minimised.
\footnote{We have not yet attempted to tackle the \emph{true risk} in
this manner: previously we have been concerned only with the
\emph{empirical risk}.}
The tools which enable us to proceed are the bounds on generalisation
performance in section \ref{sec:slt}, which all have the general form
and properties given in figure \ref{fig:generalisation bound form}.

\begin{linefigure}
Given a set of $m$ training samples $X$ and a classifier $h^{\ast} \in
\calH$ that minimises empirical risk,
\begin{equation*}
h^{\ast} = \argmin_{h \in
\calH} R_{\emp}(h)
\end{equation*}
with probability of at least $1 - \delta$,
%
\begin{equation}
\underbrace{R(h)}_{\mbox{\small{true risk}}}
\quad \leq \quad
\underbrace{R_{\emp}(\hat{h})}_{\mbox{\small{empirical risk}}}
\quad + \quad
\underbrace{b(\delta, |\calH|, m)}_{\mbox{\small{confidence interval}}}
\label{eqn:general bound}
\end{equation}
%
where the function $b(\delta, |\calH|, m)$ has the following
properties:
%
\begin{enumerate}
\item	$b(\delta, \cdot, \cdot)$ is non-increasing with $\delta$;
\item	$b(\cdot, |\calH|, \cdot)$ is nondecreasing with $|\calH|$;
\item	$b(\cdot, \cdot, m)$ is non-increasing with $m$.
\end{enumerate}
\caption{General form of generalisation performance bounds}
\label{fig:generalisation bound form}
\end{linefigure}

If there are more functions to choose from in $\calH$, we
are more likely to find one which fits the \emph{training} data
exactly.  Another look at figure \ref{fig:overfitting} will solidify
this point.  Thus, we require a large $|\calH|$.  On the other hand,
to minimise the model uncertainty term we want to make $|\calH|$ as
\emph{small} as possible (a principle reminiscent of Occam's Razor:
\emph{the simplest solution is the best}.

Consequently, there is a tradeoff between the two terms in
(\ref{eqn:general bound}); we need to select an $\calH$ with optimal
complexity $|\calH^{\ast}|$. Figure \ref{fig:srm} illustrates the
process%
\footnote{Figure \ref{fig:srm} was generated from experimental results
using the AdaBoost algorithm (chapter \ref{chapter:boosting}).}.
The principle of minimising the RHS of (\ref{eqn:general bound}) by
choosing the optimal $|\calH|$ is known as \emph{structural risk
minimisation}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/srm.epsg}
\end{center}
\begin{capt}{Structural risk minimisation}{fig:srm}
Training errors (empirical risk) and test errors (approximate true
risk) for a dataset.  $|\calH|$ increases along the $x$ axis.
The optimal structure is identified with the dash-dot line and the
$\circ$.  The existence of a clear local minima indicates that
overfitting has occurred.
\end{capt}
\end{linefigure}


\subsection{Avoiding overfitting}

There are two methods that are used to avoid overfitting.  The first
is useful if we have a tight bound on $b(\delta, |\calH|, m)$.  As we
can calculate $R_{\emp}$, we can directly minimise (\ref{eqn:general
bound}).

In the problems considered in this thesis, the bound on $b$ is very
loose.  We instead approximate the true risk directly by using a
\emph{testing dataset} $X_T$.  The learning machine never sees the
labels of the samples in $X_T$; thus this dataset can be used to
approximate the true risk:
%
\begin{equation}
R(f) \approx \underset{X_T}{R_{\emp}(f)}
\end{equation}
%
We then simply stop training when it appears that we have reached the
optimal complexity%
\footnote{There are many methods for doing this efficiently;
these are beyond the scope of this discussion.}.

The main disadvantage of this method is that some data must be put
aside for the test dataset; thus the learning algorithm does not train
on all of the available data%
\footnote{Again, methods for overcoming this problem are beyond the
scope of this thesis.}.

