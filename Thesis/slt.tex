% slt.tex
% Jeremy Barnes, 1999
% $Id$

\chapter{Statistical learning theory}
\label{chapter:slt}

This chapter provides the general theory upon which the mose specific
theory in chapter \ref{chapter:boosting} is built upon.  The goal of
this chapter is to develop both constructive procedures and a
theoretical background to generate the \emph{best} learning machine
for a particular problem.

Section \ref{sec:formulation} is an overview of the formulation of the
problem and the notation used.  Section \ref{sec:erm} then considers
how to compare learning machines, and develops an inductive procedure
for selecting the best one.

We then move on to consider how well a learning machine can learn a
problem.  Section \ref{sec:slt} develops bounds on how closely
learning machines generated by the ERM inductive principle approach
the optimum theoretical performance.

Finally, we consider the problem of overfitting and how to avoid this
phenomonen by limiting the complexity of our learning machines.  This
leads to another inductive principle, known as structural risk
minimisation.

\section{Formulation of the machine learning problem}
\label{sec:formulation}

We first develop some conventions and notation.

\subsection{Learning machines}
\label{sec:learning machines}
A \emph{learning method} (or what we will call here a \emph{learning
machine} is described in Cherkassky and Mulier \cite{Cherkassky98} as
%
\begin{quote}
	\ldots an algorithm (usually implemented in software) that
	estimates an unknown mapping (dependency) between a system's
	inputs and outputs from the available data, namely from known
	(input, output) samples.
\end{quote}
%
This definition motivates the formal definition of supervised learning,
which is illustrated in figure \ref{fig:supervised learning}.  The generator
produces samples $\mathbf{x}$ drawn from a fixed (but unknown)
probability distribution $p(x)$\footnote{In practice, these samples
are normally \emph{observed} rather then \emph{generated}.}.
These samples are presented both to
a supervisor (which produces the ``correct'' result $\mathbf{y}$) and to the
learning machine (which produces its estimate $\hat{\mathbf{y}}$).  The
learning machine can use the difference between these two results
($\bfyh - \bfy$) as feedback to improve its approximation.  The goal
is for the learning machine to always generate the correct
estimate, such that $\mathbf{\hat{y} - y = 0}$ for all samples $\mathbf{x}$.

\begin{linefigure}
\begin{center}
\begin{picture}(270,155)(30,20)
\put(30,110){\framebox(60, 60){\parbox{55pt}{\center{Sample generator}}}}
\put(150,110){\framebox(60, 60){\parbox{55pt}{\center{Learning machine
$\bbW$}}}}
\put(150,30){\framebox(60, 40){\parbox{55pt}{\center{Supervisor}}}}
\put(90, 140){\vector(1,0){60}}
\put(120,140){\line(0,-1){90}}
\put(120,50){\vector(1,0){30}}
\put(120,150){\framebox(0,0){$\mathbf{x}$}}
\put(210,50){\line(1,0){30}}
\put(240,130){\line(0,-1){80}}
\put(250,90){\framebox(0,0){$y$}}
\put(240,130){\vector(-1,0){30}}
\put(210,150){\vector(1,0){60}}
\put(280,150){\framebox(0,0){$\hat{y}$}}
\end{picture}
\end{center}
\caption{Supervised learning}
\label{fig:supervised learning}
\end{linefigure}

The \emph{sample generator} drives the learning process, by generating a
series of samples $\mathbf{x}_1, \ldots$ for the learning machine to
operate on.  We denote the domain of the learning machine $\calI$,
such that each $\mathbf{x}_i \in \mathcal{I}$.

The \emph{supervisor} is a function that always generates the
``correct'' answer $y$ for a given input $\bfx$.  Often, the
``supervisor'' as such does not exist; we are instead given a series of
$(\bfx, y)$ pairs $X = ((\bfx_1, y_1), \ldots, (\bfx_m, y_m))$ that
imply a supervisor function.  We do, however, assume that data points
$(\bfx, y)$ are drawn independently from an identical (but unknown
distribution).  This distribution is often termed the ``underlying
distribution''; the goal of machine learning is to approximate it as
closely as possible.

The \emph{learning machine} is denoted $\bbW$.  It has two key
features:
%
\begin{itemize}
\item	It generates labels $\hat{y}$ from the samples $\bfx$
	according to some \emph{hypothesis} (function) $h$;
\item	It changes the hypothesis $h$ to better match the supervisor
	in response to the error feedback $\hat{y} - y$.
\end{itemize}
%
Further discussion of exactly what constitutes ``learning behaviour''
is beyond the scope of this thesis; the interested reader should
consult eg. Anthony and Bartlett \cite{Anthony98}.


\subsection{Domain and range of learning machines}
\label{sec:domain and range}

The symbols are used to \calI\ and \calO\ specify the domain and
range of the learning problem.  $\calI$ depends upon number and
domain of input variables in the problem; in the datasets considered in
this thesis, usually $\mathcal{I} = [0,1] \times [0,1] \subset
\mathbb{R}^2$.

$\mathcal{O}$ is a subset of $\mathbb{R}^o$, where $o$ is the number
of output variables.  In many cases, there is only one output
variable ($m=o$) or multiple output variables are independent and
can each be generated by $o$ separate learning machines.
Thus we restrict our attention to $o=1$ for the rest of this thesis.

The distinction between \emph{classification} and \emph{regression}
problems is made on $|\calO|$.

A learning machine is solving a \emph{regression} problem when the size of
the output space $\|\mathcal{O}\|$ is infinite.  Usually, this means
that $\mathcal{O}$ is some interval on $\mathbb{R}$.  This thesis does
not consider regression problems, although it is straightforward
to generalise some results to apply to regression problems.

Whenever $\|\calO\| < \infty$ we are solving a \emph{classification}
problem.  (Learning machines which solve a classification problem are
often called \emph{classifiers}).

Our permissible $y$ values are now a finite number of discrete
(\emph{categories}):
%
\begin{equation}
\mathcal{I} = \{y_1, y_2, \ldots, y_c\}, \qquad \mbox{where} \qquad y_1, y_2,
\ldots, y_c \in \mathbb{R}^m
\end{equation}

A solution to the classification problem involves splitting the input space
into a set of disjoint regions $\{ \mathcal{R}_1 \ldots \mathcal{R}_c
\}$ which correspond to the values of the labels.  A useful representation
of these regions is gained by constructing the ``decision boundaries''
as illustrated in figure \ref{fig:classification problem}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/classification_problem.epsg}
\end{center}
\begin{capt}{A classification problem}
The $\times$ symbol represents $y=-1$ and the $\circ$ symbol $y=1$.
The dotted line is the ``decision boundary'', which is the boundary of
adjacent regions of different symbols.  Note that in the data shown,
there are many possible decision boundaries which classify the data
correctly.
\end{capt}
\label{fig:classification problem}
\end{linefigure}

For the remainder of this thesis, only classification problems will be
considered.  In most cases, these will also be \emph{binary}
classification problems (two categories) where $\mathcal{O} = \{-1,
1\}$.


\subsection{Representation of learning machines}
\ref{sec:representation learning machines}

In order to avoid confusion, we will now discuss exactly what we mean
by a \emph{learning machine} and a \emph{hypothesis} (or
\emph{classifier}), and how we represent them.

A learning \emph{machine} $\bbW$ takes a labelled set of data $X =
((\bfx_1, y_1), \ldots, (\bfx_m, y_m))$, where $x_i \in \calI$ and
$y_i \in \calO$, and outputs a hypothesis $h(\cdot) : \calI \mapsto
\calO$ chosen according to some known principle.

It is often convenient to represent learning machines by the set
$\calH = \{h : \left( \exists X : \bbW(X) = h \right)$.  This set
contains all hypotheses that the learning machine $\bbW$ could
generate; note however that the \emph{rule} through which a $h$ is
selected from $\calH$ is not explicit in this formulation. 
When we apply an operation to $\calH$, for example $\sign(\calH)$, we
actually apply that operation to all elements of that set.

In summary, when we are talking about a learning machine we are
talking about some procedure that chooses a hypothesis $h$ from a set
$\calH$ according to some rule.  The terms``hypothesis'' and
``classifier'' are used interchangably.


\subsection{Alternative formulation for binary classification problems}
\label{sec:margin formulation}
The discontinuities in the output of a classifier tend to make
analysis difficult.  The formulation developed in this section
alleviates much of this difficulty by defining the output of a
classifier as the  \emph{sign of a real valued function}.  This real
valued function in known as the \emph{margin}.

We recast (\ref{eqn:learning machine formulation}) in the form
%
\begin{equation}
y = \sign \left( f(\mathbf{x}) \right)
\label{eqn:marginal classification formulation}
\end{equation}
%
where the margin function $f : \mathbb{R}^n \mapsto \mathbb{R}$ and
%
\begin{equation}
\sign (x) = \left\{ \begin{array}{rl}
1	& \qquad \mbox{if $x \geq 0$} \\
-1	& \qquad \mbox{otherwise}
\end{array} \right.
\end{equation}
%

Putting these together explicitly, we find
%
\begin{equation}
m_{\mathbf{x}} = \left\{ \begin{array}{rl}
yf(\mathbf{x})	& \qquad \mbox{if $y = \hat{y}$} \\
-yf(\mathbf{x})	& \qquad \mbox{if $y \neq \hat{y}$}
\end{array} \right.
\label{eqn:margin definition}
\end{equation}

The magnitude of the margin at a point is an indication of how sure the
learning machine is of its classification.  Geometrically, for
``nicely behaved'' functions the distance between a point and the
decision boundary will roughly correspond to the magnitude of the
margin at that point.

In order to distinguish functions returning margins from those that
return categories, we adopt the convention of calling real-valued
functions $f$ and discrete-valued functions $h$.  Thus, we have $h =
\sign(f)$.  We adopt a similar convention for the learning machines,
where $\calH = \sign(\calF)$.

The two most important results of this chapter (sections
\ref{sec:covering number bounds} and \ref{p-convex}) rely upon the
margin formulation.



\subsection{Summary}

In this section we have defined the classification problem and much of
the notation used to describe it.  We also introduced the concept of
the margin, which useful for analysing binary classification
problems and central to the important results of this chapter.



\section{Comparing and selecting learning machines: ERM}
\label{sec:erm}

Now that we have defined what we mean by a ``learning machine'' (and
in particular, what we mean by a ``classifier''), we need a way of
comparing two classifiers to determine which is ``better''.


\subsection{Loss functions}
\label{sec:loss function}

One metric that can be used is known as a ``loss function''.  The
input to a loss function is a triple $(\bfx, \bfyh, \bfy)$ where \bfx\
is a sample, \bfyh\ is the output of the classifier (learning
machine), and \bfy\ is the correct output (that generated by the
supervisor).  See figure \ref{fig:supervised learning}.  The output of
the loss function is a single number $q$ which describes how ``bad''
this estimate is.  In symbols, the loss function is
%
\begin{equation}
q = Q(\bfx, \bfyh, \bfy) \qquad \mbox{where} \qquad q \in \mathbb{R};
\bfx \in \calI; \mbox{ and }\bfyh, \bfy \in \calO
\end{equation}

In this thesis, only one loss function will be considered.  This is
known as the ``misclassification loss function'', and is defined as
%
\begin{equation}
Q(\bfx, \bfyh, \bfy) = Q(\bfyh, \bfy) = \left\{
\begin{array}{ll}
	0	&	\qquad \mbox{if $\bfyh = \bfy$} \\
	1	&	\qquad \mbox{if $\bfyh \neq \bfy$}
\end{array}
\right.
\label{eqn:misclassification loss function}
\end{equation}

This loss function is very easy to understand: given a series of input
samples, the sum of the loss functions will be equal to the number of
samples that were misclassified.  If the performance of two
classifiers are compared using this loss function, then the one with
the lowest risk will be the one that makes the least ``mistakes''.


\subsection{True risk}
\label{sec:true risk}
Given our loss function $Q(\cdot)$, we are now in the position to be
able to compare the performance of a number of classifiers over the
entire input space.  We define the \emph{true risk} of a classifier
$\hat{h}(\bfx)$ given a supervisor $h(\bfx)$ as
%
\begin{equation}
R(\hat{h}) = \int_{\calI} Q(\bfx, \bfyh, \bfy) \: p(\bfx) \: d\bfx
\label{eqn:true risk}
\end{equation}
%
where $p(\bfx)$ is the marginal probability that sample $\bfx$ will be
generated.

Using our loss function \ref{eqn:misclassification loss function}, the
value of $R$ is easily seem to be the proportion of samples that are
misclassified.

The goal of machine learning is generate learning machines that
minimise the true risk (\ref{eqn:true risk}).  Unfortunately, this is
impossible for the following reasons:
%
\begin{enumerate}
\item	Generally, \ref{eqn:true risk} cannot be solved analytically,
	or in finite time, making the minimisation of it impossible.
%
\item	In practice, there are only a finite number of observations
	$(\bfx_1, \bfy_1), \ldots, (\bfx_l, \bfy_l)$ available.  Thus
	it is impossible to evaluate \ref{eqn:true risk}.
\end{enumerate}
%
In section \ref{sec:overfitting} we will describe a method (structural
risk minimisation) that comes close to minimising the true risk.
However, for the timebeing we will consider a measure that we
\emph{can} actually calculate: the \emph{empirical risk}.


\subsection{Empirical risk}
\label{sec:empirical risk}
Empirical risk is a weaker version of true risk, that can be
evaluated.

\begin{definition}[Empirical risk]
Given $m$ observations $X = ((\bfx_1, \bfy_1), \ldots, (\bfx_m,
\bfy_m))$ and a hypothesis $h(\cdot) \in \calH$, we define the
\emph{empirical risk} of $h$ over $X$ as 
%
\begin{equation}
R_{\emp} = \frac{1}{m} \sum_{i=1}^{m} Q(\bfx_i, \bfy_i, \bfyh_i)
\end{equation}
\end{definition}

The following observations concern the empirical risk:
%
\begin{itemize}
\item 	The law of large numbers ensures that as $m \rightarrow
	\infty$, the empirical risk approaches the true risk under
	conditions of ``reasonable'' behaviour of $Q$.  This is
	explored in section \ref{sec:slt}.
\item	$R_{\emp}$ is the ``best that we can do'', in that it uses all
	available observations.
\end{itemize}


\subsection{Weighted empirical risk}
\label{sec:weighted empirical risk}

The empirical risk can be extended slightly to the case where
the samples in $X$ are not equally important, by giving each sample a
weight $w_i$.

\begin{definition}[Weighted empirical risk]
Given $m$ observations $X = ((\bfx_1, \bfy_1), \ldots, (\bfx_m,
\bfy_m))$ a normalised vector of weights $W = (w_1, \ldots, w_m) \in
\bbR^m$ where $\sum_{i} w_i = 1$ and a hypothesis $h(\cdot) \in
\calH$, we define the \emph{weighted empirical risk} of $h$ over $X$ as 
%
\begin{equation}
R_{\emp}^w = \sum_{i=1}^{m} w_i Q(\bfx_i, \bfy_i, \bfyh_i)
\end{equation}
\end{definition}

It is clear that when all $w_i = 1/m$, this definition is equivalent
to that of empirical risk.

Weighted training samples will not be considered further in this
chapter; in all cases is is a simple matter to extend results using
unweighted samples to accept weighted samples.  They are introduced
because the boosting algorithm uses them (chapter
\ref{chapter:boosting}).  Another term commonly used is ``training error''.


\subsection{Margin risk}
\label{sec:margin risk}

We define here a further risk value that will be of use later on.
Recall from section \ref{sec:margin formulation} that the margin is a
real-valued quantity that indicates how confident a classification
is.  Margins with positive values indicate that the classification of
a point was correct.

We use these margins to introduce a stronger form of risk.
Specifically, we include in our risk values not only those samples
that were classified wrongly, \emph{but also those samples with a
small margin}. 

\begin{definition}[Margin risk]
Given a hypothesis $f : \calI \mapsto \mathbb{R}$, a set of
labelled training examples $\{(\bfx_1, y_1), \ldots, (\bfx_m, y_m)\}$,
and a minimum margin $\gamma$, we define the \emph{margin risk} as
%
\begin{equation}
R^{\gamma}_{\emp}(f) = \Pr_{i=1\cdots m}(y_i f(x_i) < \gamma)
\end{equation}
\end{definition}

The margin risk is the proportion of samples with a margin less than
$\gamma$.


\subsection{Empirical risk minimisation}
\label{sec:srm}
\label{acr:erm}

We have been concerned thus far with a \emph{single} learning
machine $\hat{h}(\cdot)$.  However the goal of machine learning is to
pick the optional $\hat{h}$  from a set \calH\ of learning machines.
Given our metric of \emph{empirical risk}, we are able to define an
inductive principle that minimises the empirical risk.  This principle
is known as \emph{empirical risk minimisation} (ERM):

Assume that we have a set $\calH$ of learning machines
%
\begin{equation}
\hat{h}_i \in \calH \qquad \mbox{where} \qquad h_i : \calI \mapsto
\calO
\end{equation}
%
and a loss function $Q$.

Then the principle of \emph{empirical risk minimisation} selects the
optimal learning machine $\hat{h}^{\ast} \in \calH$ in the sense that
the empirical risk is minimised:
%
\begin{equation}
\hat{h}^{\ast} = \arg \min_{\hat{h} \in \calH} R_{\emp}(\hat{h})
\label{eqn:erm}
\end{equation}

\subsection{Summary}

This section considered metrics that can be used to compare different
learning machines.  The true risk is the ideal measure, but cannot be
evaluated in practice.  The empirical risk is a more pragmatic
measure.  The logical next step is the empirical risk minimisation
inductive principle, which simply chooses the learning algorithm with
the minimum empirical risk.





\section{Statistical learning theory}
\label{sec:slt}
\label{acr:slt}

We now have framework and an inductive principle for choosing the
best learning machine $\hat{h}_{\mbox{opt}}$ from a set $\calH$ over a
set of \emph{known training data}.  The ideas that are developed in
this section allow the performance of the learning machine to be
predicted over \emph{unknown} (test) data.  In other words, we are now
trying to measure the generalisation ability of the learning machine.

This section develops important results from statistical learning
theory (SLT).  SLT provides a mathematically rigorous
conceptual framework in which to evaluate the many empirical results.
This theory is covered in much greater detail by Vapnik
\cite{Vapnik98} and Cherkassky \& Mulier \cite{Cherkassky98}.  Results
are also taken from Bartlett et al. \cite{Bartlett98a}.

\subsection{Performance bounds}

By treating the probability of an error on a particular sample as a
binomial random variable, and bounding the tail of this distribution,
we can bound the probability of an error for a finite hypothesis class
(where $|\calH| < \infty$)\footnote{The details are in
\cite{Bartlett99b}.}.  This procedure leads to the following theorem.

\begin{theorem}[Upper bound for $|\calH| < \infty$]
If $\hat{f} \in \calH$ minimises empirical risk on a training set $X$
with $m$ examples, then with probability at least $1 - \delta$
%
\begin{equation}
\Pr(\mathrm{error}|\hat{f}) \leq R_{\emp}(\hat{f}) + \sqrt{\frac{2}{m}
\log (\frac{2\  |\calH|}{\delta})}
\end{equation}
\end{theorem}

The requirement that $|\calH| < \infty$ is very restrictive in
practice; even a class of learning machines parameterised by one real
number fails to meet this condition.  The \emph{VC dimension} has less
restrictive requirements, and thus turns out to be a more useful measure.


\subsection{Vapnik-Chervonenkis dimension}
\label{acr:vcdim}
\label{sec:vcdim}

The VC (VapniK-Chervonenkis) dimension is another measure of the complexity
of a class of hypotheses $\calH$ that turns out to be more useful for
many problems.

\subsubsection{Shattering}

The most convenient definition of the VC dimension is based upon the
concept of shattering.

\begin{definition}[Shattering]
Consider a set $S$ of $h$ points $x_1 \ldots x_h \in \calI$.  Then a
function class $\calH$ is said to \emph{shatter} $S$ if and only if
there exist functions $f_1, \ldots, f_n \in \calH$ such that each of
the $n = 2^h$ possible classifications of the points are produced.
\end{definition}

Figure \ref{fig:shattering} provides an example of the shattering of
points in the plane by the set of lines in the plane.  In essence, a
function class shatters a set of points if the function class is
sufficiently complex.  The definition of the VC dimension uses shattering.

\begin{linefigure}
\begin{center}
\includegraphics{figures/shattering.epsg}
\end{center}
\label{fig:shattering}
\begin{capt}{Shattering a set of points}
The $\times$ symbols are data points; the lines are decision boundaries.
Part (a) shows how three points on $\bbR^2$ can be shattered by linear
decision boundaries.  Part (b) shows that there are no four points
cannot be shattered by a linear decision boundary.
\end{capt}
\end{linefigure}


\subsubsection{VC dimension}

\begin{definition}[VC dimension]
Given a function class $\calH$ on $\calI \mapsto \calO$, the VC
dimension of the class is defined as
%
\begin{equation}
\VCdim{\calH} = \max_{h} : \exists S = \{\bfx_1, \ldots, \bfx_h\},
i \neq j \rightarrow \bfx_i \neq \bfx_j,
\qquad \mbox{where} \qquad \mbox{\calH\ shatters $S$}
\end{equation}
\end{definition}

In other words, $\VCdim(\calH)$ is the largest number $h$ where
a set of $h$ distinct points can be selected that is shattered by
$\calH$.  Thus, as an extrapolation of figure \ref{fig:shattering}, the VC dimension of
lines in $\mathbb{R}^d$ is $d+1$.


\subsubsection{Methods of calculating $\VCdim(\calH)$}

There are many theorems on how to calculate $\VCdim(\calH)$ for
different classes of functions (it is usually difficult to do
directly).  In many cases, a useful estimate for conceptual purposes
is
%
\begin{equation}
\VCdim(\calH) \approx \mbox{number of parameters in $\calH$}
\end{equation}
%
No difficult VC dimension calculations are required in this thesis;
thus these methods will not be considered further.


\subsubsection{Bounds using $\VCdim(\calH)$}

Having identified a good measure of the complexity of a hypothesis
space, we can now use this measure to bound the performance of
classifiers from this hypothesis space.

\begin{theorem}[VC upper bound (from \cite{Bartlett98a})]
Let $H$ be a class of functions mapping from a set $\calI$ to $\calO =
\{-1, 1\}$ and having VC-dimension $h$.  For any probability
distribution on $\calI \times \{-1,1\}$, with probability $1-\delta$
over $m$ random examples $\mathbf{x}$, any hypothesis $f$ in $\calH$
has generalisation error no more than
\begin{equation}
2\mathrm{Er}_{\mathbf{x}}(f) + \frac{1}{m} \left[ 4 \ln 
\left( \frac{4}{\delta} \right) + 4 h \ln \left( \frac{2 e l}{h}
\right) \right]
\end{equation}
provided $h \leq m$.
\end{theorem}

This upper bound holds for \emph{all} probability distributions.
However, in practice it seems to be very pessemistic.

\begin{theorem}[VC lower bound (from \cite{Bartlett98a})]
Let $H$ be a hypothesis space with finite VC dimension $h \geq 1$.
Then for any learning algorithm there exist distributions such that
with probability at least $\delta$ over $l$ random examples, the error
of $f$ is at least
\begin{equation}
\max \left[ \frac{h-1}{32l}, \frac{1}{l} \ln \left( \frac{1}{\delta}
\right) \right]
\end{equation}
\end{theorem}

This lower bound is an \emph{existance} bound; it shows that there
exist distributions in which the error gets very high.

The existence of these two bounds shows that the VC dimension is the
``right'' measure of complexity.  \marginpar{More...}



\subsection{Covering numbers}
\label{sec:covering numbers}

One problem with the VC dimension is that it is sensitive to behaviour
on an arbitrarily small scale.  Figure \ref{fig:vcdim problems} shows
a parasitic example.  For this reason, we use \emph{covering numbers}
as a measure of complexity.

\begin{linefigure}
\begin{center}
\includegraphics{figures/vcdim_problems.epsg}
\end{center}
\begin{capt}{Problems with scale insensitive measures of complexity}
We consider the decision boundaries of two hypothesis classes.  Part
(a) shows the decision boundary of class $\calH_{\circ}$,
which consists of circles centered at $(\bar{x}_1, \bar{x}_2)$ with a
radius function $r(\theta) = 1$.  These decision boundaries are simply
circles; $\VCdim(\calH_{\circ}) = 4$.

Part (b) shows a second hypothesis class $\calH_{\ast}$.  This class
has a radius function $r(\theta) = 1 + \alpha \sin(\omega \theta)$.
Despite $\calH_{\circ} \rightarrow \calH_{\ast}$ as $\alpha
\rightarrow \infty$, $\VCdim(\calH_{\ast}) = \infty$ for all $\alpha >
0$.  In other words, two function classes with decision boundaries
that are arbitrarily close have wildly differing VC dimensions.  Thus,
in this case the VC dimension is \emph{not} a good measure of
complexity.
\end{capt}
\label{fig:vcdim problems}
\end{linefigure}

Covering numbers are a way of measuring the minimum number of
functions from some space that are required to approximate \emph{all}
functions within that space to a certain accuracy (scale), at a number
of specified points.  The key features of covering numbers are:
%
\begin{itemize}
\item	Covering numbers are scale sensitive.  When we look at an
	appropriate scale, intricacies (such as those in figure
	\ref{fig:vcdim problems}) become insignificant.
\item	Covering numbers look at the function locally (around a number
	of specified points) rather than globally.  Covering
	numbers concentrate on the important areas of the function.
\end{itemize}

Let us be more specific.  Assume that we are given a function class
$\calH$ and a set $X$ of ``interesting'' points.  The aim is to select
a set $\calS \subseteq \calH$ that covers $\calH$.

By ``covers'', we  mean that for each function $u \in \calH$ we can
find a function $v \in \calS$ such that, at each point $x \in X$ the
function is distance $\epsilon$ or less, where $\epsilon$ is the
scale.  Figure \ref{fig:covering numbers} shows two examples of
functions covering other function classes.

\begin{linefigure}
\begin{center}
\includegraphics{figures/covering.epsg}
\end{center}
\begin{capt}{Illustration of covering numbers}
Show some functions drawn in solid lines, and another function in the
dashed line covering them.  Show the points at which the cover
operates, and the epsilon length bar either side.
\end{capt}
\end{linefigure}

The following definition formalises the notion that we are only
interested in how a function behaves at certain points.  We follow
Anthony and Bartlett \cite{Anthony98}.

\begin{definition}[Restriction of a function]
\label{def:restriction}
Consider a set of points $X = \{\bfx_1, \ldots, \bfx_k\} \in \calI^k$,
and a function $u$ where $u : \mathbb{R}^n \mapsto \mathbb{R}$.  Then
we define the \emph{restriction of $u$ to $X$} as a vector
%
\begin{equation}
u_{|X} = (u(\bfx_1), \ldots, u(\bfx_n))
\end{equation}
\end{definition}

We now can define exactly what we mean by ``to cover''.

\begin{definition}[Covering and covering number]
\label{def:covering}
\label{def:covering numbers}
For two given function classes $\calH$ and $\calX$ where $\calS
\subseteq \calH$ and $f \in \calH \rightarrow f : \calI \mapsto
\mathbb{R}$, a given set of points $X = \{\bfx_1, \ldots, \bfx_n\} : \bfx_i \in
\calI$, a given metric\footnote{We use the $d_{\infty}$ norm: 
\[ d_{\infty}(u, v) = \max_{i} |u_i - v_i| \]} $d(u, v) : \mathbb{R}^n
\times \mathbb{R}^n \mapsto 
\mathbb{R}$, and a scale $\epsilon > 0$, we say that \emph{$\calS$ is a
$d$ $\epsilon$-cover for $\calH$} if and only if
\begin{equation}
\forall u \in \calH \qquad \exists v \in \calS \qquad : \qquad
d(u_{|X}, v_{|X}) < \epsilon
\end{equation}

We further define the \emph{$d$ $\epsilon$-covering number} of $\calH$
as 
\begin{equation}
\covert{\calH}{\epsilon}{d} = min_{\forall \calS \subseteq
\calH}|\calS| : \mbox{$\calS$ is an $\epsilon$-cover for $\calH$}
\end{equation}
\end{definition}

This definition will suffice if we know our $X$ (for example, we want
to develop bounds on the \emph{empirical} risk).  By dropping the dependence
on $X$, and instead taking a maximum over all possible sets $X$
of size $m$, we can obtain a more useful result that is applicable to the
\emph{true} risk:

\begin{definition}[Uniform covering numbers]
Given the same parameters as definition \ref{def:covering}, and
assuming that $d = d_{\infty}$, we define
the \emph{uniform covering number} as
\begin{equation}
\label{eqn:uniform covering numbers}
\covert{\calX}{\epsilon}{k} = \max_{\forall X \in \calI^k} \left\{
\covert{\calS}{\epsilon}{d_{\infty}} \right\} 
\end{equation}
\end{definition}

Note that the $d$ norm is not explicit in \ref{eqn:uniform covering
numbers}; covering numbers are implicitly assumed to be using
$d_{\infty}$ for the remainder of this thesis.

\subsubsection{Bounds using covering numbers}
\label{sec:covering number bounds}

The result stated in this section is an important part of the
background of this thesis.  It is derived in Anthony and Bartlett
\ref{Anthony98} by bounding the expectation of the value of the
uniform covering numbers; and converted in a straightforward manner to
the form shown below.

\begin{theorem}[Convergence bound using covering numbers]
Consider a hypothesis space $\calH : f \in \calH \rightarrow f : \calI
\mapsto \mathbb{R}$, a vector of samples $X \in \calI^H$, and a margin
$0 \leq \gamma < 1/2$.  Then with probability at least $1 - \delta$, 
\begin{equation}
R_{\emp}^{\gamma}(f) \leq R(f) + \sqrt{\frac{8}{m} \log \left( \frac{2
\covert{\calH}{\gamma/2}{2m}}{\delta} \right)}
\end{equation}
\end{theorem}

This result bounds how far our empirical risk can be from the true
risk.  In order to tighten the bound, we want to decrease the covering
numbers $\covert{\calH}{\gamma/2}{2m}$.  One way to do this is to
increase  $\gamma$; this increases the scale (resulting in a tighter
bound).  However, this may also increase $R_{\emp}^{\gamma}(f)$.

One way to avoid this is to set $\gamma$ to the \emph{minimum margin}
on the training data.  Then we are assured that $R_{\emp}^{\gamma}(f)
= 0$.  By maximising this minimum margin, then, we can obtain optimum
performance.  This concept is revisited in chapter
\ref{chapter:boosting}.


\subsection{Application to $p$-convex hulls ($0 < p < 2$)}
\label{sec:p-convex}

The theoretical motivation behind the algorithms developed in this
thesis uses the covering number of a $p$-convex hull ($\co_p(\calH)$).
This section defines a ``$p$-convex hull'' and gives a result on the covering
numbers for $0 < p < 2$.  See Williamson et. al. \cite{Williamson99}
for a full discussion.

\begin{definition}[$p$-convex hull]
The $p$-convex hull (strictly speaking, the $p$-absolutely convex hull) of
a set $\calH$ of functions (where $p>0$) is defined as
%
\begin{equation}
\cop (\calH) =
 \bigcup _{n \in \mathbb{N}}
\left\{
 \sum_{i=1}^{n}
 \alpha _i
f_i : f_1, \ldots, f_n \in F,
 \alpha _1, \ldots, \alpha _n \in \mathbb{\calH},
 \sum_{i=1}^{n} | a_i |^p \leq 1
\right\}
\end{equation}
\end{definition}

Thus the $p$-convex hull of $\calH$ is a subset of $\lin(\calH)$, the
set of all possible linear combinations of functions in $\calH$.
While the $p$-convex hull of a set of \emph{functions} is hard to
visualise, the $p$-convex hull of euclidian points is a useful visual
aid.  See figure \ref{fig:p-convex}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/p_convex.epsg}
\end{center}
\begin{capt}{The $p$-convex hull of two variables}
The figure details the shape of a $p$-convex hull of two variables $u$
and $v$ for various values of $p$.  The smaller $p$ is, the closer to
the axes the hull is.  Note that we define two sets: the points on the
boundary are termed ``on'' the hull, and those within the boundary
``in'' the hull.  The equation of the curves is $\left( u^p + v^p
\right) ^{(1/p)} = 1$.
\end{capt}
\end{linefigure}

We will see in section \ref{sec:theoretical overfitting} that the
$p$-convex hulls define a \emph{structure}:  If $0 \leq p_1 \leq p_2
\leq \cdots \leq p_z$, then $\cop 1 \subseteq \cop 2 \subseteq \cdots
\subseteq \cop z$.  In other words, the higher that $p$ is, the richer
the set of functions in $\cop{F}$.

We now state a theorem that gives an approximate bound on the covering
numbers of a $p$-convex hull.  This result is central to this thesis.

\begin{theorem}[Covering numbers of a $p$-convex hull]
Consider a hypothesis class $\calH$ and a number $0 < p \leq 2$.  Then
the uniform covering number at scale $\epsilon > 0$ can, under some
assumptions,  be approximated by
%
\begin{equation}
\label{eqn:approx p-convex bound}
\log \cover{\calH}{\epsilon} \approx c(p) d\ (1 /
\epsilon)^{\frac{2p}{2-p}} \log (1 / \epsilon)
\end{equation}
%
where $c(p)$ is a constant that depends upon $p$.
\end{theorem}

This theorem can be made a lot more exact, but at the expense of a lot
more complexity.  This complexity is unnecessary; it is the form of
this bound that motivates the work in this thesis.  In particular, it
should be noted that the approximate bound (\ref{eqn:approx p-convex
bound}) decreases with $p$ for $0 < p \leq 2$.

As a final note, it should always be remembered that the
$\co_{p}(\calH)$ is usually \emph{lot} ``richer'' than $\calH$.
Extending our hypothesis space in this manner is one way to markedly
improve the empirical risk; indeed this is exactly what the Boosting
algorithm (chapter \ref{chapter:boosting}) does.


\subsection{Summary}

In this section we developed bounds on the generalisation performance
of learning algorithms that utilise the ERM inductive principle.  We
began by considering finite hypothesis classes, and then extended
through the VC dimension and covering numbers as useful measures of
the complexity of infinite hypothesis classes.  We then briefly
considered a result on the covering numbers of the $p$-convex hull of
a hypothesis class.


\section{Overfitting}
\label{sec:overfitting}

We have now developed enough theory to understand the problem that
this thesis is attempting to solve: \emph{overfitting}.  The problem
of overfitting is quite familiar to anyone who has had to fit a
polynomial (say) to noisy data: to what extent should the data be
trusted?  By fitting a high-order polynomial (a complex model), we may
fit the data perfectly, yet the underlying distribution poorly.  We
call this ``overfitting'', an undesirable situation.  Conversely, by
fitting a low-order polynomial (a less-complex model), we fit the
underlying distribution well but not our data.  Figure
\ref{fig:overfitting} illustrates this point. 

\begin{linefigure}
\begin{center}
\includegraphics{figures/overfitting.epsg}
\end{center}
\begin{capt}{Overfitting with polynomials: two extreme examples.}
The $\times$ points are the noisy data points.  The dashed line is the
underlying distribution.  In part (a) an order 2 polynomial has bee
fitted.  In part (b) an order 10 polynomial has been fitted.  The fit
is closer to the underlying distribution for the second order
polynomial, which shows that more complex models do not necessarily
perform better.
\end{capt}
\label{fig:overfitting}
\end{linefigure}

Dealing with overfitting involves trading off model complexity against
goodness of fit (training error).  In this section, we will first
describe how overfitting can be detected using two separate datasets
(a training dataset and a test dataset), and give some examples of
overfitting using the algorithms considered in this thesis.  We will
then consider two explanations of overfitting: a qualitative
explanation based on general properties of model complexity, and a
more quantitative explanation based on an investigation of the theory
developed throughout this chapter.


\subsection{Theoretical overfitting and structural risk minimisation}
\label{sec:theoretical overfitting}
\label{acr:srm}

Let us speak more formally.  Consider a series of hypothesis
classes $\calH_1, \ldots, \calH_n, n \leq \infty$ (recall that a
hypothesis class contains all possible paramater sets of a learning
algorithm) and some \emph{appropriate} measure of complexity
$|\cdot|$.  (By ``appropriate'', we mean ``suitable for the problem at
hand''. Depending upon the situation, we might choose $|\calH_i| =
\VCdim(\calH_i)$ or $|\calH_i| = \cover{\lambda}{\calH_i}$, for
example).  We also make the assumption that $\calH_1 \subseteq \calH_2
\subseteq \cdots \subseteq \calH_n$, and thus using any sensible
complexity measure, $|\calH_1| < |\calH_2| < \cdots < \calH_n$.  Thus,
we have a sequence of increasingly complex hypothesis classes.  This
is termed a \emph{structure}.

Our goal is to find the optimal $\calH_i$, in the sense that the
true risk (\ref{eqn:true risk}) is minimised.  This is the
first time that we have tried to tackle the \emph{true risk} in this manner:
previously we have been concerned only with the \emph{empirical risk}.

The tool which enables us to proceed are the bounds on generalisation
performance that we generated in section \ref{sec:slt}.  Recall that
they all have the form in figure \ref{fig:generalisation bound form}.

\begin{linefigure}
Given a set of $m$ training samples $X$ and a classifier $\hat{f} \in \calH$
that minimises empirical risk
\begin{equation*}
\hat{f} = \argmin_{f \in
\calH} R_{\emp}(f)
\end{equation*}
, with probability of at least $1 - \delta$,
%
\begin{equation}
\underbrace{\Pr(\mathrm{error}|\hat{f})}_{\mbox{\small{true risk}}} \leq 
\underbrace{R_{\emp}(\hat{f})}_{\mbox{\small{empirical risk}}} +
\underbrace{b(\delta, |\calH|, m)}_{\mbox{\small{model uncertainty}}}
\label{eqn:general bound}
\end{equation}
%
where the function $b(\delta, |\calH|, m)$ has the following
properties:
\begin{enumerate}
\item	$b(\delta, \cdot, \cdot)$ is nonincreasing with $\delta$;
\item	$b(\cdot, |\calH|, \cdot)$ is nondecreasing with $|\calH|$;
\item	$b(\cdot, \cdot, m)$ is nonincreasing with $m$.
\end{enumerate}
\caption{General form of generalisation performance bounds}
\label{fig:generalisation bound form}
\end{linefigure}

In order to minimise the empirical risk, we want to make $|\calH|$ as
large as possible.  This is intuitively obvious; if there are more
functions to choose from in $\calH$, we are more likely to find one
which fits the \emph{training} data exactly.  Another look at figure
\ref{fig:overfitting} will solidify this point.

On the other hand, to minimise the model uncertainty term we want to make
$|\calH|$ as \emph{small} as possible, due to the second property of
$b(\delta, |\calH|, m)$ listed in figure \ref{fig:generalisation bound
form}.  This is a restatement of Occam's Razor: \emph{the simplest
solution is always the best}.

Thus, there is a tradeoff between the two terms in (\ref{eqn:general
bound}), and we need to select an optimal complexity $|\hat{\calH}|$.
This is illustrated in figure \ref{fig:srm}, which was generated from
experimental results using the Boosting algorithm (chapter
\ref{chapter:boosting}).

The principal of finding the optimal complexity is known as
\emph{structural risk minimisation}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/srm.epsg}
\end{center}
\begin{capt}{Structural risk minimisation}
Training errors (empirical risk) and test errors (approximate true
risk) for a dataset.  $|\calH|$ increases with the number of
iterations.  The theoretical best possible performance is
indicated with the dashed line, and the optimal risk identified with
the $\circ$.  Overfitting is clearly seen to occur.
\end{capt}
\label{fig:overfitting}
\end{linefigure}

\subsection{Avoiding overfitting}

There are two methods that are used to avoid overfitting.  The first
is useful if we have a tight bound on $b(\delta, |\calH|, m)$ and some
method of approximating $R_{\emp}$.  In this case, we can directly
minimise (\ref{eqn:general bound}).

In the problems considered in this thesis, we don't have a tight bound
on $b$.  We instead 
approximate the true risk directly by using a \emph{testing dataset}
$T$.  The learning machine never sees the labels of the samples in
$T$; thus this dataset can be used to approximate the true risk:
%
\begin{equation}
\Pr(\mathrm{error}|\hat{f}) \approx R_{\emp}^{T}(\hat{f})
\end{equation}
%
we then simply stop training when it appears that we have reached the
optimal complexity\footnote{There are many methods for doing this efficiently;
these are beyond the scope of this discussion.}.

The main disadvantage of this method is that some data must be put
aside for the test dataset; thus the learning algorithm does not use
all of the data.\footnote{Again, methods for reducing this problem are
beyond the scope.}.

\subsection{Summary}

In this section we have discussed the problem of overfitting, giving
both theoretical and intuitive explanations of why this occurs.  We
developed an inductive principle, \emph{structural risk minimisation}
which can be used to avoid overfitting, and discussed methods of
implementing this procedure.

\section{Chapter summary}

In this chapter we considered the goal of machine learning: to choose
a hypothesis from a set that minimis