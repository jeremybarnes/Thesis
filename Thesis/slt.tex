% slt.tex
% Jeremy Barnes, 1999
% $Id$

\chapter{Statistical learning theory}
\label{chapter:slt}


\section{Formulation of machine learning}

We first develop some conventions and notation.

\subsection{Learning machines}
A \emph{learning method} (or what we will call here a \emph{learning
machine} is described in Cherkassky and Mulier \cite{Cherkassky98} as
%
\begin{quote}
	\ldots an algorithm (usually implemented in software) that
	estimates an unknown mapping (dependency) between a system's
	inputs and outputs from the available data, namely from known
	(input, output) samples.
\end{quote}
%
This motivates the formal definition of supervised learning,
which is illustrated in figure \ref{fig:supervised learning}.  The generator
produces samples $\mathbf{x}$ drawn from a fixed (but unknown)
probability distribution $p(x)$\footnote{In practice, these samples
are normally \emph{observed} rather then \emph{generated}.}.
These samples are presented both to
a supervisor (which produces the ``correct'' result $\mathbf{y}$) and to the
learning machine (which produces its estimate $\hat{\mathbf{y}}$).  The
learning machine can use the difference between these two results
($\bfyh - \bfy$) as feedback to improve its approximation.  The goal
is for the learning machine to always generate the correct
estimate, such that $\mathbf{\hat{y} - y = 0}$ for all samples $\mathbf{x}$.

\begin{linefigure}
\begin{center}
\begin{picture}(270,155)(30,20)
\put(30,110){\framebox(60, 60){\parbox{55pt}{\center{Sample generator}}}}
\put(150,110){\framebox(60, 60){\parbox{55pt}{\center{Learning machine
$\hat{h}(\mathbf{x})$}}}}
\put(150,30){\framebox(60, 40){\parbox{55pt}{\center{Supervisor
$h(\mathbf{x})$}}}}
\put(90, 140){\vector(1,0){60}}
\put(120,140){\line(0,-1){90}}
\put(120,50){\vector(1,0){30}}
\put(120,150){\framebox(0,0){$\mathbf{x}$}}
\put(210,50){\line(1,0){30}}
\put(240,130){\line(0,-1){80}}
\put(250,90){\framebox(0,0){$\mathbf{y}$}}
\put(240,130){\vector(-1,0){30}}
\put(210,150){\vector(1,0){60}}
\put(280,150){\framebox(0,0){$\hat{\mathbf{y}}$}}
\end{picture}
\end{center}
\caption{Supervised learning}
\label{fig:supervised learning}
\end{linefigure}

\subsubsection{Sample generator}
\label{sec:sample generator}

The sample generator drives the learning process, by generating a
series of samples $\mathbf{x}_1, \ldots$ for the learning machine to
operate on.  Each $\mathbf{x}_i \in \mathcal{I}$ where $\mathcal{I}$
is the ``input space'', and $\mathcal{I} \subseteq \mathbb{R}^n$ where
$n$ is the dimensionality of the sample space.


\subsubsection{Supervisor}
\label{sec:supervisor}

The supervisor is a function
%
\begin{equation}
\mathbf{y} = h(\mathbf{x}) \qquad \mbox{where} \qquad \mathbf{x} \in
\mathcal{I} \qquad \mbox{and} \qquad \mathbf{y} \in \mathcal{O}
\label{eqn:supervisor}
\end{equation}
%
that always generates the ``correct'' answer given an input $\mathbf{x}$.

There is potential for confusion concerning the word ``correct'' in
the previous statement.  The correct answer or distribution is
usually not known, and the data observed from it may be noisy.  This
noisy data, while usually containing errors, is still termed the
correct data.


\subsubsection{Learning machine}
\label{sec:learning machine}

The learning machine is a function operating on the same domain and
range as the supervisor:
%
\begin{equation}
\mathbf{\hat{y}} = \hat{h}(\mathbf{x}) \qquad \mbox{where} \qquad
\mathbf{x} \in \mathcal{I} \qquad \mbox{and} \qquad \mathbf{y} \in
\mathcal{O}
\label{eqn:learning machine formulation}
\end{equation}
%
Any useful learning machine exhibits a learning behaviour (not shown in
equation \ref{eqn:learning machine formulation}, whereby the function
$\hat{h}(\cdot)$ is updated based on the error feedback
$\mathbf{\hat{y} - y = 0}$.


\subsection{Domain and range of learning machines}

The input set \calI\ and output set \calO\ specify the domain and
range of the learning problem.  The input set depends upon number and
range of input variables in the problem; in the datasets considered in
this thesis, usually $\mathcal{I} = [0,1] \times [0,1] \subset
\mathbb{R}^2$.

The ``output set'' $\mathcal{O}$ is defined as $\mathcal{I} \subseteq
\mathbb{R}^m$ where $m$ is the dimensionality of the output space (or
the number of output variables).

In almost all cases, there is only one output variable ($m=1$) or the
multiple output variables are independent and can each
be generated by a separate learning machine with $m=1$.
Thus it is reasonable to restrict our attention to the case where
$m=1$ for the rest of this thesis.

The distinction between \emph{classification} and \emph{regression}
problems is made on the size of $\mathcal{O}$.


\subsubsection{Regression}
\label{sec:regression}

A learning machine is solving a regression problem when the size of
the output space $\|\mathcal{O}\|$ is infinite.  Usually, this means
that $\mathcal{O}$ is some interval on $\mathbb{R}$.  This thesis does
not consider regression problems (although it may not be difficult to 
generalise some of the results to include regression problems); thus
regression is not considered further.


\subsubsection{Classification}
\label{sec:classification}

Whenever $\|\calO\| < \infty$ we are solving a classification
problem.  (Learning machines which solve a classification problem are
often called \emph{classifiers}).

Our permissible $y$ values are now a finite number of discrete values
(\emph{categories}): 
%
\begin{equation}
\mathcal{I} = \{y_1, y_2, \ldots, y_c\}, \qquad \mbox{where} \qquad y_1, y_2,
\ldots, y_c \in \mathbb{R}^m
\end{equation}
%
A sample classification problem is shown in
figure \ref{fig:classification problem}.

A classification problem involves splitting the input space
into a set of disjoint regions $\{ \mathcal{R}_1 \ldots \mathcal{R}_c
\}$ which correspond to the category values.  A useful representation
of these regions is gained by constructing the ``decision boundaries''
as illustrated in figure \ref{fig:classification problem}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/classification_problem.epsg}
\end{center}
\caption{A classification problem}
\label{fig:classification problem}
The $\times$ symbol represents $y=-1$ and the $\circ$ symbol $y=1$.
The dotted line is the ``decision boundary'', which is the boundary of
adjacent regions of different symbols.  Note that in the data shown,
there are many possible decision boundaries which classify the data
correctly.
\end{linefigure}

For the remainder of this thesis, only classification problems will be
considered.  In most cases, these will also be \emph{binary}
classification problems (two categories) where $\mathcal{O} = \{-1,
1\}$.

\subsection{Alternative formulation for binary classification problems}

The discontinuities in the output of a classifier tend to make
analysis difficult.  The formulation developed in this section
alleviates much of this difficulty by defining the output of a
classifier as the  \emph{sign of a real valued function}.  This real
valued function in known as the \emph{margin}.

We recast (\ref{eqn:learning machine formulation}) in the form
%
\begin{equation}
y = \sign \left( f(\mathbf{x}) \right)
\label{eqn:marginal classification formulation}
\end{equation}
%
where the margin function $f : \mathbb{R}^n \mapsto \mathbb{R}$ and
%
\begin{equation}
\sign (x) = \left\{ \begin{array}{rl}
1	& \qquad \mbox{if $x \geq 0$} \\
-1	& \qquad \mbox{otherwise}
\end{array} \right.
\end{equation}
%

Putting these together explicitly, we find
%
\begin{equation}
m_{\mathbf{x}} = \left\{ \begin{array}{rl}
yf(\mathbf{x})	& \qquad \mbox{if $y = \hat{y}$} \\
-yf(\mathbf{x})	& \qquad \mbox{if $y \neq \hat{y}$}
\end{array} \right.
\label{eqn:margin definition}
\end{equation}

The magnitude of the margin at a point is an indication of how sure the
learning machine is of its classification.  Geometrically, for
``nicely behaved'' functions the distance between a point and the
decision boundary will roughly correspond to the magnitude of the
margin at that point.  See figure \ref{fig:margin illustration} for an
illustration of this. 

\begin{linefigure}
\begin{center}
\includegraphics{figures/margin_illustration.epsg}
\end{center}
\caption{The margin of a sample}
\emph{The dotted line denotes the decision boundary and the $\times$ the
point for which we wish to define a margin.}
\label{fig:margin illustration}
\end{linefigure}

\subsection{Summary}

In this section we have defined the classification problem and much of
the notation used to describe it.  We also introduced the concept of
the margin, which useful for analysing binary classification problems.



\section{Comparing and selecting learning machines: ERM}
\label{sec:erm}

Now that we have defined what we mean by a ``learning machine'' (and
in particular, what we mean by a ``classifier''), we need a way of
comparing two classifiers to determine which is ``better''.


\subsection{Loss functions}

One metric that can be used is known as a ``loss function''.  The
input to a loss function is a triple
$(\bfx, \bfyh, \bfy)$ where \bfx\ is a sample, \bfyh\ is the
output of the classifier (learning machine), and \bfy\ is the correct output (that generated by the supervisor).  See figure \ref{fig:supervised
learning}.  The output of the loss function is a single number $q$
which describes how ``bad'' this estimate is.  In symbols, the loss
function is
%
\begin{equation}
q = Q(\bfx, \bfyh, \bfy) \qquad \mbox{where} \qquad q \in \mathbb{R};
\bfx \in \calI; \mbox{ and }\bfyh, \bfy \in \calO
\end{equation}
%

In this thesis, only one loss function will be considered.  This is
known as the ``misclassification loss function'', and is defined as
%
\begin{equation}
Q(\bfx, \bfyh, \bfy) = Q(\bfyh, \bfy) = \left\{
\begin{array}{ll}
	0	&	\qquad \mbox{if $\bfyh = \bfy$} \\
	1	&	\qquad \mbox{if $\bfyh \neq \bfy$}
\end{array}
\right.
\label{eqn:misclassification loss function}
\end{equation}
%
which does \emph{not} depend upon the position in the input space \bfx.

This loss function is very easy to understand: given a series of input
samples, the sum of the loss functions will be equal to the number of
samples that were misclassified.  If the performance of two
classifiers are compared using this loss function, then the one with
the lowest risk will be the one that makes the least ``mistakes''.


\subsection{True risk}
Given our loss function $Q(\cdot)$, we are now in the position to be
able to compare the performance of a number of classifiers over the
entire input space.  We define the \emph{true risk} of a classifier
$\hat{h}(\bfx)$ given a supervisor $h(\bfx)$ as
%
\begin{equation}
R(\hat{h}) = \int_{\calI} Q(\bfx, \bfyh, \bfy) \: p(\bfx) \: d\bfx
\label{eqn:true risk}
\end{equation}
%
where $p(\bfx)$ is the marginal probability that sample $\bfx$ will be
generated.

Using our loss function \ref{eqn:misclassification loss function}, the
value of $R$ is easily seem to be the proportion of samples that are
misclassified.

The goal of machine learning theory is generate learning
machines that minimise the true risk \ref{eqn:true risk}.
Unfortunately, this is impossible:
%
\begin{enumerate}
\item	Generally, \ref{eqn:true risk} cannot be solved analytically,
	or in finite time, making the minimisation of it impossible.
%
\item	In practice, there are only a finite number of observations
	$(\bfx_1, \bfy_1), \ldots, (\bfx_l, \bfy_l)$ available.  Thus
	it is impossible to evaluate \ref{eqn:true risk}.
\end{enumerate}
%
We use something that is the best we can calculate: the
\emph{empirical risk}.


\subsection{Empirical risk}
Empirical risk is a weaker version of true risk, that can be
evaluated.  Given a set of $l$ observations
$\{(\bfx_1, \bfy_1), \ldots, (\bfx_l, \bfy_l)\}$ and setting $\bfyh_i
= \hat{h}(\bfy_i)$ as the output of our learning machine $h(\cdot)$,
we can define our empirical risk as 
%
\begin{equation}
R_{\emp} = \frac{1}{l} \sum_{i=1}^{l} Q(\bfx_i, \bfy_i, \bfyh_i)
\end{equation}
%
The following observations concern the empirical risk:
%
\begin{itemize}
\item 	As $l \rightarrow \infty$, and assuming that $\bfx$ is evenly
	distributed over \calI\, (and that $Q$ is reasonably well
	behaved), we would expect that $R_{\emp}(\cdot) \rightarrow
	R(\cdot)$.

\item	$R_{\emp}$ is the ``best that we can do'', in that it uses all
	available observations.  However, there is further scope for
	\emph{a priori} knowledge of the system to be used via the
	selection of an appropriate model.

\item	Section \ref{sec:slt} explores the rate of convergence of
	empirical risk $R_{\emp}$ to true risk $R$.
\end{itemize}

\subsection{Empirical risk minimisation}

We have been concerned thus far with a \emph{single} learning
machine $\hat{h}(\cdot)$.  However the goal of machine learning is to
pick the optional $\hat{h}$  from a set \calH\ of learning machines.
Given our metric of \emph{empirical risk}, we are able to define an
inductive principle that minimises the empirical risk.  This principle
is known as \emph{empirical risk minimisation} (ERM):

Assume that we have a set of learning machines
%
\begin{equation}
\hat{h}_i \in \calH \qquad \mbox{where} \qquad h_i : \calI \mapsto
\calO
\end{equation}
%
and a loss function $Q$.

Then the principle of \emph{empirical risk minimisation} selects the
optimal learning machine $\hat{h}^{\ast} \in \calH$ in the sense that
the empirical risk is minimised:
%
\begin{equation}
\hat{h}^{\ast} = \arg \min_{\hat{h} \in \calH} R_{\emp}(\hat{h})
\label{eqn:erm}
\end{equation}

\subsection{Summary}

This section considered metrics that can be used to compare different
learning machines.  The true risk is the ideal measure, but cannot be
evaluated in practice.  The empirical risk is a more pragmatic
measure.  The logical next step is the empirical risk minimisation
inductive principle, which simply chooses the learning algorithm with
the minimum empirical risk.





\section{Statistical learning theory}
\label{sec:slt}

We now have framework and an inductive principle for choosing the
best learning machine $\hat{h}_{\mbox{opt}}$ from a set $\calH$ over a
set of \emph{known training data}.  The ideas that are developed in
this section allow the performance of the learning machine to be
predicted over \emph{unknown} (test) data.  In other words, we are now
trying to measure the generalisation ability of the learning machine.

This section develops important results from statistical learning
theory (SLT).  SLT provides a mathematically rigorous
conceptual framework in which to evaluate the many empirical results.
This theory is covered in much greater detail by Vapnik
\cite{Vapnik98} and Cherkassky \& Mulier \cite{Cherkassky98}.  Results
are also taken from Bartlett et al. \cite{Bartlett98a}.

\subsection{Performance bounds}

* Show that form of bounds is training error + other term
* Quick explanation of method for finite hypothesis space: bound the
  tail of the binomial distribution.
* Details are in \cite{Bartlett98b}.


\subsection{Vapnik-Chervonenkis dimension}

The VC dimension is a measure of the complexity of a class of
hypotheses $\calH$ that turns out to be the ``right'' measure for many
problems (this is described in more detail later).

\subsubsection{Shattering}

\begin{definition}[Shattering]
Consider a set $S$ of $h$ points $x_1 \ldots x_h \in \calI$.  Then a
function class $\calH$ is said to \emph{shatter} $S$ if and only if
there exist functions $f_1, \ldots, f_n \in \calH$ such that each of
the $n = 2^h$ possible classifications of the points are produced.
\end{definition}

Figure \ref{fig:shattering} provides an example of the shattering of
points in the plane by the set of lines in the plane.  In essence, a
function class shatters a set of points if the function class is
sufficiently complex.  The definition of the VC dimension is built
upon shattering.

\begin{linefigure}
\begin{center}
\includegraphics{figures/shattering.epsg}
\end{center}
\caption{Shattering a set of points}
\label{fig:shattering}
The $\times$ symbols are points; the lines are decision boundaries.
Part (a) shows how two points can be shattered by linear decision
boundaries.  Part (b) shows that there are no three nonincident points
cannot be shattered by a linear decision boundary.
\end{linefigure}


\subsubsection{VC dimension}

\begin{definition}[VC dimension]
Given a function class $\calH$ on $\calI \mapsto \calO$, the VC
dimension of the class is defined as
%
\begin{equation}
\VCdim{\calH} = \max_{h} : \exists S = \{\bfx_1, \ldots, \bfx_h\},
i \neq j \rightarrow \bfx_i \neq \bfx_j,
\qquad \mbox{where} \qquad \mbox{\calH\ shatters $S$}
\end{equation}
\end{definition}

In other words, $\VCdim(\calH)$ is the largest number $h$ where
a set of $h$ distinct points can be selected that is shattered by
$\calH$.  Thus in figure \ref{fig:shattering}, the VC dimension of
lines in $\mathbb{R}^2$ is 2.

For an example of a VC dimension calculation, see
chapter \ref{chapter:stumps} which calculates the VC dimension of a
very simple classification algorithm.

\subsubsection{Methods of calculating $\VCdim(\calH)$}

There are many theorems on how to calculate $\VCdim(\calH)$ for
different classes of functions (it is usually difficult to do
directly).  In many cases, a useful estimate for conceptual purposes
is
%
\begin{equation}
\VCdim(\calH) \approx \mbox{number of parameters in $\calH$}
\end{equation}
%
No difficult VC dimension calculations are required in this thesis;
thus these methods will not be considered further.

\subsubsection{Bounds using $\VCdim(\calH)$}

Having identified a good measure of the complexity of a hypothesis
space, we can now use this measure to bound the performance of
classifiers from this hypothesis space.

\begin{theorem}[VC upper bound (from \cite{Bartlett98a})]
Let $H$ be a class of functions mapping from a set $\calI$ to $\calO =
\{-1, 1\}$ and having VC-dimension $h$.  For any probability
distribution on $\calI \times \{-1,1\}$, with probability $1-\delta$
over $m$ random examples $\mathbf{x}$, any hypothesis $f$ in $\calH$
has generalisation error no more than
\begin{equation}
2\mathrm{Er}_{\mathbf{x}}(f) + \frac{1}{m} \left[ 4 \ln 
\left( \frac{4}{\delta} \right) + 4 h \ln \left( \frac{2 e l}{h}
\right) \right]
\end{equation}
provided $h \leq m$.
\end{theorem}

This upper bound holds for \emph{all} probability distributions.
However, in practice it seems to be very pessemistic.

\begin{theorem}[VC lower bound (from \cite{Bartlett98a})]
Let $H$ be a hypothesis space with finite VC dimension $h \geq 1$.
Then for any learning algorithm there exist distributions such that
with probability at least $\delta$ over $l$ random examples, the error
of $f$ is at least
\begin{equation}
\max \left[ \frac{h-1}{32l}, \frac{1}{l} \ln \left( \frac{1}{\delta}
\right) \right]
\end{equation}
\end{theorem}

This lower bound is an \emph{existance} bound; it shows that there
exist distributions in which the error gets very high.

The existence of these two bounds shows that the VC dimension is the
``right'' measure of complexity.  \marginpar{More...}


\subsection{Fat-shattering dimension}


The problem with the VC dimension is that it is sensitive to very
small changes to the scale.  For example, consider two hypothesis
spaces.  The first has the unit circle ($r(\theta) = 1$) as its decision
boundary, and $h = 2$.  The second is the set of all decision
boundaries where $|r(\theta) - 1| < \epsilon$, where $\epsilon$ is a
small number.  This has $h = \infty$.  However, it is obvious that as
$\epsilon$ get small enough the two are almost identical.  See figure
\ref{fig:fat-shattering}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/fat_shattering.epsg}
\end{center}
\caption{Fat-shattering a set of points}
\label{fig:fat-shattering}
\end{linefigure}


For this reason, the \emph{fat-shattering dimension} is used.  It
is a scale-sensitive version of the VC dimension.

The differences between fat-shattering dimension and the VC dimension
are:
\begin{itemize}
\item	The fat-shattering dimension is defined at a scale $\gamma$;
\item	The fat-shattering dimension works on the \emph{margins} of a
	classifier (see section \ref{sec:margin}), a real-valued
	quantity, rather than the class ($\{\pm 1\}$).
\item	In order for a hypothesis class to $\gamma$-shatter a set of
	points, it is not sufficient that the points simply lie on the
	correct side of the decision boundary.  They must lie a
	substantial distance ($\gamma$) on that side.
\item	Thus, in general we have $\Fat{\gamma}(\calH) <
	\VCdim(\calH)$.
\end{itemize}

\subsubsection{Definition}

The definition of the fat-shattering dimension relies upon the
concept of $\gamma$-shattering.

\begin{definition}[$\gamma$-shattering]
Consider a set $S$ of $h$ points $x_1 \ldots x_h \in \calI$.  Then a
function class $\calH$ where $\calI \mapsto \calO$ is said to
\emph{$\gamma$-shatter} $S$ if and only if there exist functions $f_1,
\ldots, f_n \in \calH$ such that 
\begin{enumerate}
\item	each of the $n = 2^h$ possible classifications of the points
	are produced;
\end{enumerate}
\end{definition}
\marginpar{This is crap.}

Put a figure of $gamma$-shattering in here for clarity.  Use our
rather dodgy assertion before that the margin ``usually roughly
corresponds to the distance from the decision boundary'', but make
clear that we are using that dodgy assumption.

We now define the fat-shattering dimension in a similar manner to the
VC dimension:

\begin{definition}[Fat-shattering dimension at scale $\gamma$]
Given a function class $\calH$ on $\calI \mapsto \calO$, the
fat-shattering dimension of the class at scale $\gamma$ is defined as
%
\begin{equation}
\Fat{\gamma}(\calH) = \max_{h} : \exists S = \{\bfx_1, \ldots, \bfx_h\},
i \neq j \rightarrow \bfx_i \neq \bfx_j,
\qquad \mbox{where} \qquad \mbox{\calH\ $\gamma$-shatters $S$}
\end{equation}
\end{definition}


\subsubsection{Fat-shattering dimension - alternative definition}
These are copied verbatim from Bartlett et al. \cite{Bartlett98a}.

Let \calF\ be a set of real values functions.  We say that a set of
points $X$ is \emph{$\gamma$-shattered by \calF} if there are real
numbers $r_x$ indexed by $x \in X$ such that for all binary vectors
$b$ indexed by $X$, there is a function $f_b \in \calF$ satisfying
\begin{equation}
f_b(x)  \left\{
	\begin{array}{ll}
		\geq r_x + \gamma & \mbox{if $b_x = 1$} \\
		\leq r_x - \gamma & \mbox{otherwise}
	\end{array}
\right.
\end{equation}
The \emph{fat shattering dimension} $\fat_{\calF}$ of the set \calF\ is
a function from the positive real numbers to the integers which maps a
value $\gamma$ to the size of the largest $\gamma$-shattered set (if
this is finite), or infinity otherwise.


\subsubsection{Bounds using $\Fat{\lambda}(\calH)$}

See the explanation of this in \cite{Bartlett98a} pages 5-6.  This is
all copied directly from there.

\begin{theorem}[Bounds on large-margin classifiers]
Consider a class \calF\ of real-valued functions.  With probability at
least $1 - \delta$ over $l$ independently generated examples
$\mathbf{z}$, if a classifier $\sign (f) \in \sign (\calF)$ has margin
at least $\gamma$ on $\mathbf{z}$, then the error of $\sign (f)$ is no
more than
\begin{equation}
\frac{2}{l} \left[ h \log_2 \left( \frac{8 e l}{h} \right) \log_2(32l)
+ \log_2 \left( \frac{8l}{\delta} \right) \right]
\end{equation}
where $h = \fat_{\calF}(\gamma / 16)$.

Furthermore, with probability at least $1 - \delta$, every classifier
$\sign (f) \in \sign (\calF)$ has error no more than
\begin{equation}
b/l + \sqrt{\frac{2}{l} \left[ h \ln(34el/h) \log_2(578l) +
\ln(4/\delta) \right] }
\end{equation}
where $b$ is the number of labelled training examples with a margin
less than $\gamma$.
\end{theorem}



\subsection{Covering numbers}

Covering numbers are different from the VC dimension and the fat
shattering dimension in that we are only looking at the function in
the vicinity of a few points.

Given a number of points $x_1, \ldots, x_m$, and a class of functions
$\calH$, such that $\forall f \in \calH : \calI \mapsto \mathbb{R}$,
we define 
%
\begin{equation}
f_{|x} = (f(x_1), \ldots, f(x_m)) \in \mathbb{R}^m
\end{equation}
%
In other words, we are looking at the function only where we have a
sample.  Now typically, we will have $|\calH_{|x}| = \infty$.
However, we don't have to calculate $\calH$ exactly... we can make do
with an approximation.

The covering number is the size of an approximating set, such that all
points are within a distance $\epsilon$ of that set.  This is made
formal in the following statement:

\begin{definition}[Covering number]

Given a set $\calH_{|x}$ and a scale $\epsilon > 0$ we define the
covering number as
%
\begin{equation}
\cover{\calH_{|x}}{\epsilon} = \min \left\{
|\hat{\calH}| : \forall f \in \calH \exists \hat{f} \in \hat{calH}
 \qquad \mbox{where} \| f_{|x} - \hat{f}_{|x} \| < \epsilon \right\}
\end{equation}
\end{definition}

Thus, the covering number is a measure of the number of hypotheses
that are needed to get \emph{close enough} to the hypothesis class
$\calH$.


\subsubsection{Bounds using covering numbers}

Page 40 of Peter's slides.

\subsubsection{Methods of calculating}

\subsubsection{Application to $p$-convex hulls ($p < 1$)}

The theoretical motivation behind the algorithms developed in this
thesis uses the covering number of a $p$-convex hull ($\co_p(\calH)$).
This section defines a ``$p$-convex hull'' and gives a result on the covering
numbers for $p < 1$.  

At the moment, this whole section is copied pretty much verbatim from
\cite{Williamson99}.

The $p$-convex hull (strictly speaking, the $p$-absolutely convex hull) of
a set $\calH$ of functions (where $p>0$) is defined as
%
\begin{equation}
\cop (\calH) =
 \bigcup _{n \in \mathbb{N}}
\left\{
 \sum_{i=1}^{n}
 \alpha _i
f_i : f_1, \ldots, f_n \in F,
 \alpha _1, \ldots, \alpha _n \in \mathbb{\calH},
 \sum_{i=1}^{n} | a_i |^p \leq 1
\right\}
\end{equation}

Thus the $p$-convex hull is a subset of the set of all possible linear
combinations of functions of a class $F$, where the members of the
subset are those where $\sum_{i=1}^n |\alpha_i| \leq 1$.  The
following remarks concern $p$-convex hulls:
%
\begin{itemize}
\item	If $0 < p_1 \leq p_2 \leq \cdots \leq p_z \leq 1$, then
	$\cop 1 \subseteq \cop 2 \subseteq \cdots \subseteq \cop z$
	That is to say, the higher that $p$ is, the richer the set of
	functions in $\cop{F}$.
\item	In general, it is very hard to visualise the \emph{convex} hull
	of a set of functions, let alone the $p$-convex hull.  The
	convex hull is always richer than the function class.
\item	As an example (from Williamson et al. \cite{Williamson99}), the convex
	hull of the Heaviside functions on $[0, 1]$ is the set of
	functions of ``bounded variation''.\marginpar{Need a diagram}
\end{itemize}





