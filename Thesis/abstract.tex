% abstract.tex
% Jeremy Barnes, 1999
% $Id$

\chapter{Abstract}

AdaBoost is a machine learning algorithm that generates a hypothesis
by combining a series of so-called weak hypotheses generated by another
algorithm.  The performance of AdaBoost is good to exceptional,
particularly on low-noise data.  However it has been observed that
AdaBoost can generate overly complex hypotheses that overfit noisy
data.

We consider modifying AdaBoost to allow the complexity of its combined
hypotheses to be explicitly controlled via a ``$p$'' parameter. 
The resulting ``$p$-boosting'' algorithm is constrained to generate
only combinations of weak hypotheses that lie on the $p$-convex hull
of its weak hypotheses.

Several algorithms based upon this concept are developed.  Simulations
indicate that the best algorithm outperforms AdaBoost by a modest
amount (at best 18\% test error vs 25\% for AdaBoost) on a
low-dimensional noisy dataset.  The generalisation ability over other
noisy datasets was slightly improved on AdaBoost; AdaBoost performed
better on low-noise datasets.  A number of operational difficulties
were observed, including very long training times.  These difficulties
appear to be due to details of the gradient descent optimisation
technique used internally, and solutions are proposed for those where
the cause was apparent. 

The $p$-convex hull approach appears to be a useful improvement to the
already excellent AdaBoost algorithm.  This approach is complementary
to other AdaBoost variants also designed to improve performance on
noisy datasets, allowing the possibility of a combined approach
superior to either.

