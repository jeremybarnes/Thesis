% boosting.tex
% Jeremy Barnes, 29/8/1999
% $Id$

% The chapter of my thesis on Boosting

\chapter{Boosting and similar algorithms}

The name ``Boosting'' is applied to many different machine learning
algorithms, which combine many weak hypotheses to form a ``strong''
hypothesis that significantly outperforms them.

We first describe the classic algorithm \emph{AdaBoost}.  A full
description is given, along with some examples of its performance and
a discussion of why it performs so well.  We also give a formal
definition of what we mean by the term ``boosting''.

A useful viewpoint of the boosting algorithm as an optimisation via
gradient descent in a cost space is then considered.  A result from
Mason et al. \cite{Mason99} is reproduced which shows that boosting is
indeed performing gradient descent.

We then consider \emph{normed boosting algorithms} where the
weights of the hypotheses are constrained by some norm.  Several
properties of these algorithms which are different from those of
unconstrained boosting algorithms are discussed.

We then turn to questions of the performance of boosted algorithms; in
particular guarantees on the training and generalisation error.
Several results are presented.

Finally, we motivate the $p$-boosting algorithm by considering the
problem of overfitting in boosting.  Both practical and theoretical
explanations for this phenomenon are given.


\section{Definition of boosting}

In order to achieve a formal definition of boosting, we need the
following definitions.  This discussion follows \cite{Duffy99}.

Probably Approximately Correct

(The basic gist of it is that the training error must approach zero
for it to be strictly called ``boosting''.  Otherwise, it is an
``AdaBoost Like Algorithm'' (ADA).

\section{The original boosting algorithm: AdaBoost}

\begin{linefigure}
\noindent{\bf Input:} $l$ examples $(\bfx_1, \bfy_1), \ldots, (\bfx_l,
\bfy_l)$
\par
\noindent{\bf Initialisation:} $w_i|_{t=1} = 1/l$ for $i=1 \ldots l$
\par
\noindent {\bf Do for} $t=1 \ldots T$:


\begin{enumerate}

\item	Train weak learner on the weighted sample set 
	$(\bfx_1, \bfy_1, w_1) \ldots (\bfx_l, \bfy_l, w_l)$
	and obtain weak learner $f_t(\cdot) : \calI \mapsto \{\pm 1\}$

\item	Calculate the training error $\epsilon_t$ of $f_t(\cdot)$:
	%
	\begin{equation}
	\epsilon_t = \sum_{j=1}^l w_j |_t 
	I \left( f_t(\bfx_j) \neq \bfy_j\right)
	\end{equation}

	If $\epsilon_t = 0$ (a single weak learner can correctly learn
	the relationship) or $\epsilon_t \geq \phi - \Delta$ (where
	$\Delta$ is a small positive constant; thus the weak
	learner is performing as badly as random guessing) then abort
	the training process.

\item	Calculate our classifier weight $b_t$:
	\begin{equation}
	b_t = \log \frac{\epsilon_t}{1 - \epsilon_t}
	\end{equation}

\item	Update the weights $w_i$:
	%
	\begin{equation}
	w_i|_{t+1} = \left\{
	\begin{array}{cl}
		w_i|_t / Z_t	&	\qquad \qquad \mbox{if
		$f_t(x_i) = y_i$} \\
		w_i|_t / Z_t \exp \left\{ -b_t \right\} & \qquad \qquad
		\mbox{otherwise} \\
	\end{array} \right.
	\end{equation}

	where $Z_t$ is a normalisation constant, such that
	$\sum_{i=1}^{l} w_i|_{t+1} = 1$
\end{enumerate}

\par
\noindent {\bf Output:} 
\begin{equation}
F'(\bfx) = \sign \left\{ \sum_{i=1}^T w_i f_i(\bfx) \right\}
\end{equation}
\caption{The AdaBoost algorithm}
\end{linefigure}

AdaBoost is a machine learning algorithm
that combines many ``weak'' hypotheses to generate a hypothesis 
that usually performs significantly better than any of the weak
hypotheses.  Even the simplest of weak algorithms, when
``boosted'', will usually outperform the best unboosted algorithms.
\footnote{As a result, the focus of recent research effort has shifted from the
development of \emph{learning} algorithms (which are all more
or less equivalent when boosted) to the development of better \emph{boosting}
algorithms.}

These weak hypotheses are combined in a linear combination.
The weight of each weak hypothesis (the
\emph{classifier weight}) depends upon the performance of that
classifier on the training dataset.  Section \ref{sec:classifier
weights} describes these classifier weights in more detail.

Each sample in the training dataset is given a weight (\emph{sample
weight}) which is modified depending upon how ``hard'' that
sample is to classify.  Section \ref{sec:sample weights} describes
these sample weights in more detail.

A complete and explicit description of the AdaBoost algorithm is
provided in figure \ref{fig:adaboost}.  Further details are also
available in Freund and Schapire's paper \cite{Freund96}.


\subsection{Classifier weights}
\label{sec:classifier weights}

The AdaBoost algorithm combines a number of ``weak'' classifiers
$f_1(\cdot), \ldots, f_n(\cdot)$ in a linear combination to produce a
``stronger'' classifier $F(\cdot) = \sum_{i=1}^{n} b_i f_i(\cdot)$.
The coefficients $b_i$ are subject to the condition that they produce a
convex combination where $\|b\|_1 = \sum_{i=1}^{t} b_i = 1$.

AdaBoost's training process is divided into iterations.  On iteration
$t$, one weak learner $f_t(\cdot)$ is added to the linear
combination ($f_t$ is chosen by the weak learning algorithm).  The
coefficient $b_t$ of $f_t$ is calculated from the 
training error $\epsilon_t$ of $f_t$ as 
%
\begin{equation}
b_t = - \log \frac{\epsilon_t}{1 - \epsilon_t}
\label{eqn:theory:bt}
\end{equation}
%
When $\epsilon_t = 1/2$, the classifier only does as
well as random guessing, and has $b_t = 0$.  As
$\epsilon_t$ approaches zero, $b_t$ increases without bound.  The
effect is that $F$ becomes dominated by those weak hypotheses that
performed well on the training samples
\footnote{Training is halted if $\epsilon_t = 0$ or $\epsilon_t \geq 1/2$.}.  The weights are normalised once training is completed.


\subsection{Sample weights}
\label{sec:sample weights}

Each sample in the training set has a corresponding weight $w_j$ which
is updated to reflect the difficulty of that sample.  On each iteration the
weights of samples which $f_t$ classified incorrectly are increased
(scaled by $\exp \{ -b_t \}$); the whole lot are then normalised.
This function has the following effect on distribution of weights:

\begin{itemize}

\item	Training samples which are misclassified often, or are one of 
	few points to get misclassified, increase their proportion of
	the weight (are identified as ``hard samples'');

\item	Other samples decrease their proportion of the weight, and are
	identified as ``easy'' samples.

\end{itemize}

The overall effect is for those samples near the decision boundary to
increase in weight, and those far from it to decrease.  This forces
the algorithm to concentrate on the hard samples, and is one reason
why it works well on low-noise datasets.  Unfortunately, those samples
corrupted by noise are usually hard also, and concentrating on those
is not desirable.


\section{Boosting as gradient descent}
\label{sec:theory:gradient descent}

Recent work has shown boosting to be an implementation of gradient
descent in an inner product space \cite{Mason99}\footnote{This
discussion follows Mason et al. \cite{Mason99} rather closely with
some changes in notation and omission of details such as stopping
criteria.}.
An inner product space requires both a universal set
$\mathcal{X}$ and an inner product operator \ip{\cdot}{\cdot}.  We
define
%
\begin{equation}
\calX = 
\mathrm{co} (\calF) \doteq
 \bigcup _{n \in \mathbb{N}}
\left\{
 \sum_{i=1}^{n}
  b_i
f_i : f_1, \ldots, f_n \in \calF,
 b_1, \ldots, b_n \in \mathbb{R},
 \sum_{i=1}^{n} | b_i | = 1
\right\} \cup \emptyset
\end{equation}
%
(which is the convex hull of \calF), and
%
\begin{equation}
\ip{F}{G} = \frac{1}{l} \sum_{i=1}^{l} F(\bfx_i)G(\bfx_i) \qquad
F, G \in \calX
\end{equation}
%
We also choose the cost function
%
\begin{equation}
C(F) = \frac{1}{l} \sum_{i=1}^{l} \exp
\left\{ -y_i F(\bfx_i) \right\}
\label{eqn:theory:cost function}
\end{equation}

At iteration $t$ of gradient descent, we wish to choose a function $f_t$ and
a weight $b_t$ such that $C(F + b_t f_t)$ decreases (the choice of
$f_t$ is made indirectly through the choice of the sample weights
$w|_t$).  In other words, we are asking for sample weights $w|_t$ that
choose a \emph{direction} $f_t \in \calF$ such that $C(F + b_t f_t)$
decreases as fast as possible.  This direction is the negative of the
functional derivative of $C$:
%
\begin{equation}
f = -\nabla C(F)(\bfx) = \left. \frac{\partial C(F + \alpha
1_{\bfx})}{\partial \alpha} \right|_{\alpha = 0}
\label{eqn:functional derivative}
\end{equation}
%
where $1_{\bfx}$ is the indicator function of $\bfx$; this is
necessary as we can only evaluate \ref{eqn:functional derivative} at
points where we have a sample.  Since the
optimal $f_t$ will not necessarily be in $\calF$, we choose the
$f \in \calF$ with the greatest inner product $\ip{-\nabla
C(F)}{f}$.

Having chosen our direction, we now choose a step size $b_t$.
AdaBoost uses a line search for the minimum of the cost functional
along this line
%
\begin{equation}
b_t = \arg \min_{b_t} \sum_{i=1}^{l} C(y_i F(\bfx_i) + y_i b_t f_t(\bfx_i)
\end{equation}
%
which (for the cost function (\ref{eqn:theory:cost
function})) has a closed-form solution  (\ref{eqn:theory:bt}).  Thus, the
boosting algorithm implements gradient descent.  The process is
illustrated in figure \ref{fig:gradient descent}.

\begin{figure}
\begin{center}
\includegraphics{figures/descent.epsg}
\caption{Schematic representation of gradient descent.  Points within the blue
circle are in \calF, the height of the surface the value of
(\ref{eqn:theory:cost function}) at that point.  (a) is a 3D view, (b)
is top view. Red lines show line searches; markers minimums.  Descent
from $\circ$ to $\Box$ to $\Diamond$, where it terminates due to
local minimum.}
\label{fig:gradient descent}
\end{center}
\end{figure}


\section{Performance evaluation}


\subsection{Convergence of boosting}

\begin{theorem}
Given a weaklearner $\mathcal{L}$ and a training set $S$,
then either:
\begin{enumerate}
\item	The boosting algorithm will terminate; or
\item	There exists an iteration $t_{zero}$ such that for all $t \geq
	t_{zero}$ the training error $\epsilon_t = 0$.
\end{enumerate}

\proof If for any training iteration $t$ we have $\epsilon_t > 1/2$ then the
boosting algorithm will terminate.  Assuming that this is not the
case, let us look at $\|b\|$.  As $t \rightarrow \infty$, $\|b\|
\rightarrow \infty$.  Thus, taking the normalised hypotheses $\bar{h}_i
= b_i \frac{h}{\sum b}$ we get the cost function as
\[
C(F_{t+1}) = \sum_{i=1}^{m} \exp\left\{ -y_i b_i h_i(x_i)
\right\}^{\|b\|}
\]
Now, we know from theorem (number?) that the boosting algorithm will
always find the global minimum of the cost function.  Since all $b_i >
0$, this is achieved when all training examples are classified
correctly...

Need a lot more work on this proof.  Do I want to show that since it
gets steeper and steeper, the cost of a negative sample gets too
large?  I don't think that the way I am doing it is going to work,
really.  I need to show that because
\begin{itemize}
\item	The weights for wrong samples are increasing exponentially,
	and
\item	The power that the cost function is raised to makes the wrong
	margin get worse and worse,
\end{itemize}
then the thingy gets minimised...

I need to bring the training error < 1/2 into it somewhere.  I think
that I can show that if
\begin{itemize}
\item	$\|b\|$ increases without bound; and
\item	$\epsilon_t$ is always < 1/2, then
\end{itemize}
I don't know!

\end{theorem}



\subsection{Size of $b$}

The following theorem shows that the total weight of the boosting
algorithm is unbounded as the number of iterations increases.  It is
used to prove results on the minimum margin and final training error
of boosting.

\begin{theorem}
The size of the $b$ weight vector, $\|b\| \rightarrow \infty$
as $t \rightarrow \infty$ (assuming the boosting algorithm doesn't
terminate).

\proof ?
\end{theorem}


\subsection{Minimum margin}

The following proof shows that the solution that AdaBoost converges to
is the one that maximises the minimum margin on the training samples.

\begin{theorem}
Given a particular learning algorithm $F$ and a training
set $S$, define the minimum margin as
\[
m_{\min} = min_{\{x,y\} \in S} y_i F(x_i)
\]
Then the boosting algorithm will converge as $t \rightarrow \infty$ to
the solution which maximised the minimum margin.

\proof For boosting we can write $F_t = b_1 f_1 + \cdots + b_t f_t$.
Then defining our \emph{normalised hypotheses} $\bar{f}_i$ as
\[
\bar{f} = \frac{b_i f_i}{\|b\|}
\]
such that $\hat{F}_t = \bar{f}_1 + \cdots + \bar{f}_t$, we can write
our cost function (reference?) as 
\[
C(b, S) = \sum_{i=1}^{m} \exp\{-y_i \bar{F}_t(x_i)\}^{\|b\|}
\]
We already know from the gradient descent theory (reference?) that we
are trying to minimise the cost function.  Now as $\|b\| \rightarrow
\infty$ (from the previous theorem) the largest value of $\exp\{-y_i
\bar{F}_t(x_i)\}$ will dominate, and so $C(b, S) \rightarrow exp\{\max
-y_i \bar{F}_t(x_i)\}$.  Thus, by minimising $C(b, S)$, we are making
$\min y_i \bar{F}_t(x_i)$ as large as possible; that is we are
maximising the minimum margin.
\end{theorem}


\subsection{Scale invariance of strong hypotheses}
\begin{theorem}
The hypothesis returned by the AdaBoost is independent of a linear
scaling of the $b$ values.  In particular, for all $k > 0$, the
scaled hypothesis $kF$ is equivalent to the unscaled hypothesis $F$.

\proof In order to prove this equivalence, we show that for any
training sample $\{x, y\}$ both $F$ and $kF$ return the same result.

(Proof continues noting that the sign function is independent of
scaling)
\end{theorem}

We denote this equivalence by writing
\[
kF \equiv F
\]


\subsection{Scale invariance of AdaBoost}

The following theorem takes the previous result one step further,
showing that is AdaBoost is given a scaled hypothesis $kF$ instead of
$F$, it will choose a $b$ value that is also scaled by the same
amount.

\begin{theorem}
Assume a strong hypothesis $F_t(\cdot) = b_1 f_1(\cdot) + \cdots + b_m
f_m(\cdot)$.  Denote the training action of the AdaBoost algorithm as
$F_t(\cdot) \Rightarrow F_{t+1}(\cdot) = F_t(\cdot) +
b_{t+1}f_{t+1}(\cdot)$.  If
\[
	F_t(\cdot) \Rightarrow F_t(\cdot) + b_{t+1}f_{t+1}(\cdot)
\]
then
\[
	kF_t(\cdot) \Rightarrow k\left( F_t(\cdot) +
	b_{t+1}f_{t+1}(\cdot) \right)
\]
and thus
\[
	kF_{t+1} \equiv F_{t+1}
\]

\proof We show that the minimum of the cost function occurs at
$kb_{t+1}$ instead of $b_{t+1}$.
\end{theorem}



\section{Normed boosting algorithms}



\section{Old work}


\section{The boosting algorithm}

The boosting algorithm will markedly improve the performance of most
learning machines.  It does this by 

* increases the hypothesis space by using a vote among many weak
  learners

* Even very poor weak learners are useful when boosted (for example,
  decision stumps)

* Seems quite resistant to over-fitting

* Arcing is a variant; usually outperforms ``bagging''.

* A very hot topic recently


Key features of the boosting algorithm:

* Cannot stand alone -- relies on another algorithm (the ``weak
  learning algorithm''.

* Hypothesis space is increased by adding more learning algorithms
  (rather than making the existing learning algorithms more complex).

* Many versions of the weak learning algorithm are trained, \emph{but
  each on different data}.

* Uses a weighted voting scheme amongst the weak learners to make a
  decision, with the weight depending upon how well each weak learner
  classified the training data.

* Data points that are often misclassified or misclassified by a weak
  learner that performs well are given a higher weight.  In this way,
  the boosting algorithm forces the weak learning algorithm to
  concentrate on the ``hard'' datapoints.


\subsection{Description of the boosting algorithm}



The key features of the boosting algorithm are as follows

The boosting algorithm works as follows:

\begin{enumerate}

\item	Several ``weak'' learning algorithms are trained, and each of
	these has its performance evaluated.  Those that perform well
	have a higher ``weight'' when the final decision is ``voted''
	on.

\item	Within the training data set, each example is given a weight
	which decides how ``hard'' to classify that example is.  This
	weight is determined by how often the weak learner gets it
	wrong and how well that particular weak learner performed on
	the overall dataset.  For example, an example point that was
	classified incorrectly by a 

\end{enumerate}

The algorithm may be written as in Ratsch et al. \cite{Ratsch98}

The input to the algorithm is $l$ examples $\mathbf{Z} = \langle
(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_l, y_l) \rangle$ where
$\mathbf{x}$ is the independent variable and $y$ is the category
class.

The weight vector for step $t=1$ is initialised to $w_1(\mathbf{z}_i)
= 1/l$ for all $i=1 \ldots l$

Do for $t=1 \ldots T$:


\begin{enumerate}

\item	Train weak learner on the weighted sample set $\{\mathbf{Z},
	\mathbf{w}\}$ and obtain ``weak'' hypothesis $h_t : \mathbf{x}
	\mapsto \{\pm 1\}$

\item	Calculate the training error $\epsilon_t$ of $h_t$:
	
	\begin{equation}
	\epsilon_t = \sum_{i=1}^l w_t (\mathbf{z}_i)
	I \left( h_t(\mathbf{x}_i) \neq y_i\right)
	\end{equation}

	If $\epsilon_t = 0$ (a single weak learner can correctly learn
	the relationship) or $\epsilon_t \geq \phi - \Delta$ (where
	$\Delta$ is a small positive constant, and thus the weak
	learner is performing worse than random guessing) then abort
	the training process.

\item	Calculate how well this weak learner performed:

	\begin{equation}
	b_t = \log \frac{\epsilon_t (1 - \phi)}{\phi(1 - \epsilon_t)}
	\end{equation}

\item	Update our weights $\mathbf{w}_t$:

	\begin{equation}
	w_{t+1}(\mathbf{z}_i) = w_t(\mathbf{z}_i) \exp 
	\left\{ -b_t I \left( h_t(\mathbf{x}_i) = y_i \right) \right\}
	/ Z_t
	\end{equation}

	where $Z_t$ is a normalisation constant, such that
	$\sum{i=1}{l} w_{t+1}(\mathbf{z}_i) = 1$.

\end{enumerate}



\subsection{Facts about boosting}


These are from \cite{Ratsch98}:

\begin{enumerate}

\item 	The weights $w_t(\mathbf{z}_i)$ in the $t$-th iteration
	are chosen such that the previous hypothesis has exactly a
	weighted training error $\epsilon$ of $1/2$ (from
	\cite{Schapire97}).

\item 	The weight $c_t$ of a hypothesis is chosen such that it
	minimises the function

	\begin{equation}
	G(b_t, \mathbf{b}^{t-1}) = \sum_{i=1}^l \exp 
	\left\{ d(\mathbf{z}_i, \mathbf{b}^t) - \phi | \mathbf{b}^t |
	\right\}
	\end{equation}

	where $\phi$ is a constant.  This function depends upon the
	rate of incorrect classification of all patterns.

	This functional can be minimised analytically by setting
	$\frac{\partial G(\mathbf{b}^t)}{\partial \mathbf{b}^t}=0$ and
	is described in references that I don't have (page 4 of
	\cite{Schapire97}).

	The final equation that we end up with is our familiar

	\begin{equation}
	b_t = \log \frac{\epsilon_t (1-\phi)}
	{\phi (1 - \epsilon_t)}
	\end{equation}

	where in my paper, $\phi=1/2$.

\end{enumerate}
 




\subsection{Application to boosting}
By proceeding I mean somewhere in Peter's book chapter

The preceding ideas can be applied to the boosting algorithm.  Let us
take in particular the boosted classifier of the form
\begin{equation}
x \mapsto \sign \left( \sum_{i=1}^{N} w_i f_i(x) \right)
\end{equation}
that have large margins on the training examples, where $w_i > 0$,
$\sum_i w_i = 1$, and $f_i$ are classifiers in some class $H$.  A
VC-dimension analysis would suggest that the generalisation error
would of these classifiers would increase with $N$, the number of base
hypotheses $f_i$ that are combined.  The following result, which
follows easily from techniques developed in \cite{Bartlett98}, shows
that the fat-shattering dimension of the class of convex combinations
of classifiers is independent of the number of base hypotheses.

\subsection{Bounds on fat-shattering dimension in boosting}

There is a constant $c$ so that for all classes $H$ of functions
mapping from $X$ to $\{-1, 1\}$, the class of convex combinations of
functions from $H$,
\begin{equation}
\calF = \left\{
	x \mapsto \sum_{i=1}^N w_i f_i(x) : f_i \in H, w_i > 0, \sum_i
	w_i = 1
\right\}
\end{equation}
satisfies
\begin{equation}
\fat_{\calF} \leq c \frac{h}{\gamma^2} \ln (1/\gamma)
\end{equation}

where $h$ is the VC-dimension of the class $H$ of base hypotheses.

\subsection{Bounds on error in boosting}

There is a constant $c$ such that, for the class \calF\ of convex
combinations of classifiers from a class $H$ with VC-dimension $h$,
for all probability distributions, with probability at least
$1-\delta$ over $l$ independently generated training examples, every
classifier $\sign(f) \in \sign(\calF)$ has error no more than
\begin{equation}
b/l + \sqrt{\frac{c}{l} \left[ \frac{h \ln^2 (l/h)}{\gamma^2} +
\ln(1/\gamma) \right] }
\end{equation}
where $b$ is the number of labelled training examples with margin less
that $\gamma$.


