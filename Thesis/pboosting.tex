% pboosting.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\chapter{$p$-boosting}
\label{chapter:pboosting}

This chapter describes the concepts behind and a theoretical
justification of \emph{$p$-boosting}, a generalisation of the boosting
algorithm.  The treatment is somewhat abstract; further chapters will
be more concrete as practical issues are considered.

\section{Motivation: avoiding overfitting}



\subsection{Qualitative arguments}

Choo

\subsection{Quantitative arguments}
* By reducing covering numbers we get a better bound
* Thus use a $p$-convex hull instead of a $1$-convex hull
* Trade off: minimum margin may be reduced
* Thus, we would expect to see a curve (draw a graph, shaped like a parabola)
* Optimal $p$ value there, gives us the best error
* Draw a picture of the classes being smaller

\subsection{Summary}

\section{Generalisation performance of hypotheses in $\co_p(\calH)$}

Combining the covering-number generalisation performance bound
(\ref{eqn:covering number bound}) with the approximate values of
covering numbers for $p$-convex hulls (\ref{eqn:approx p-convex
bound}), we obtain the following theorem

\begin{theorem}[Approximate bound on generalisation performance]
The generalisation error of a hypothesis $F \in \co_p(\calH)$ where
$\calH$ has VC dimension???????? $d$ over a set of $m$ independent training
samples with probability at least $1 - \delta$ can be approximated by
%
\begin{equation}
R(F) \lesssim R_{\emp}^{\gamma}(F) + \sqrt{ \frac{8}{m} \left[ \log 2
+ c(p) \: d \: \left( \frac{2}{\gamma} \right)^{\frac{2p}{2-p}} \log
\left( \frac{2}{\gamma} \right) - \log \delta \right]}
\end{equation}
where $c(p)$ is a constant that depends upon $p$. 
%
\end{theorem}

This result is necessarily an approximation, due to the approximate
nature of the theorems it is based upon.

\section{Development of algorithms}

\subsection{Naive algorithm}

The naive algorithm (named with the benefit of hindsight) attempts to
modify the $b$ values of AdaBoost in a manner that will produce a
``$p$-convex like'' distribution of parameters on $p$.

The key feature of the algorithm is the calculation of the classifier
weights $b$.  Denoting the classifier weight that AdaBoost would have
used as $b'_t$, we use the formula
%
\begin{equation}
b_t = (b'_t)^{\frac{1}{p}} = \left[ - 1/2 \log \left( \frac{\epsilon_t}{1
- \epsilon_t} \right) \right]^\frac{1}{p}
\end{equation}
%
Thus, for $p < 1$ the hypotheses with low training error (and hence
high classifier weights) have these weights ``stretched'' out.  The
effect of $p$ is plotted figure \ref{fig:naive b values}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/naive.epsg}
\end{center}
\begin{capt}{Effect of $p$ on classifier weights for naive algorithm}{fig:naive b values}
The three lines show $b_t$ against $\epsilon_t$ for $p \in \{ 0.5,
0.7. 1.0 \}$.
\end{capt}
\end{linefigure}

There are two options for updating the sample weights $w$: based upon
the value of $b_t$, or based upon $b'_t$ (unchanged from AdaBoost).
The first option yields the equation
%
\begin{equation}
w_i|_{t+1} = \left\{
\begin{array}{cl}
	w_i|_t / Z_t \exp \left\{ (b'_t)^{1/p} \right\} & \qquad \qquad \mbox{if
	$f_t(x_i) = y_i$} \\
	w_i|_t / Z_t \exp \left\{ -(b'_t)^{1/p} \right\} 	& \qquad \qquad
	\mbox{otherwise} \\
\end{array} \right.
\end{equation}
%
The important point to notice is that the $1/p$ power (which is
greater than one for $p < 1$) is inside an exponential; this could
(and does) lead to the exponential function getting extremely large
(outside the limits of IEEE floating point numbers).  Thus we update
our sample weights based upon $b'_t$; they are identical to those
chosen by the boosting algorithm.

The naivity of the algorithm becomes apparent when considered in
the gradient descent framework.  As our sample weights are calculated
the same as AdaBoost; thus given an identical initial state both
algorithms would proceed in the same ``direction''.  However the step
sizes are different: AdaBoost performs a line search for the minimum
in this direction and moves to that point; whereas the naive algorithm
also searches for that point \emph{and then purposely avoids it!}
Experiments with this algorithm prove to have indifferent performance.

\subsection{Strict algorithm}

The strict algorithm is the purest of all $p$-boosting algorithms
considered in this thesis.  It performs gradient descent, using the
same cost function and inner product as AdaBoost, but uses the the
universal set $\calX = \co_p(\calH)$.  It is called the ``strict''
algorithm because it also performs its line searches along lines
$\ell$ where each point $p$ on $\ell$, $p \in \calX$.  In other words,
it confines its line search to the $p$-convex hull.  This idea is
illustrated in figure \ref{fig:strict line search}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/strict_search.epsg}
\end{center}
\begin{capt}{Line search for strict $p$-boosting algorithm}{fig:strict line search}
The strict $p$-boosting algorithm confines its line search to the
$p$-convex hull of points, whereas AdaBoost searches orthogonal to the
current hypothesis.
\end{capt}
\end{linefigure}

Algebraicaly, we are seeking a minimum of the expression
%
\begin{equation}
C(F_{t+1}) = \sum_{i=1}^{m} \exp \left\{ -y_i F_{t+1}(\bfx_i) \right\}
\end{equation}
%
subject to the constraint that
%
\begin{equation}
\| b \|_{p} = 1
\end{equation}

Substituting in the constraint, and performing some manipulations, we
are looking for
%
\begin{equation}
b_{t+1} = \frac{\alpha}{\left( 1 + \alpha^p \right) ^ {1/p}}
\end{equation}
%
where
%
\begin{equation}
\alpha = \argmin_{\alpha} \sum_{i=1}^{m} \exp \left\{ -y_i \left(
\frac{F_{t}(\bfx_i) + \alpha h_{t+1}(\bfx_i)}{\left( 1 + \alpha ^p
\right) ^{1/p}} \right) \right\}
\label{eqn:strict minimise function}
\end{equation}

Unfortunately, it is not possible to minimise this algeberically as
for AdaBoost; we instead have to be content with a numerical
optimisation.  The method chosen is Newton-Raphson iteration, which
searches for $\partial C(F) / \partial \alpha = 0$ by repeatedly
approximating it with a parabola until a tolerance is reached.

What we have \emph{not} done is to consider whether or not a solution
to (\ref{eqn:strict minimise function}) exists.  The following theorem
does so.

\begin{theorem}[Existance of a minimum for strict boost cost]
I sure hope that I can come up with something.  I know that it always
finds a minimum for $p \geq 1$ (from observations); it sometimes works
for $p < 1$; the cutoff seems to be at approximately $p = 0.8$.
Whether this is dependent upon the data or the weaklearner I don't
know; I suspect that it has something to do with how well the
weaklearner works.

The reason a solution doesn't exist is that it is for $\alpha < 0$,
which means we are trying to optimise a hill instead of a valley (so
we get to the top).
\end{theorem}

Finally, we need to consider the sample weight updates.  The gradient
descent theory gives us the sample weights (\ref{eqn:gradient sample
weights}).

\subsection{Sloppy algorithm}

Given the problems with the strict boost algorithm, it is reasonable
to consider using an algorithm with a similar effect but a simpler
form for the cost function (in particular, one for which a minimum is
guaranteed to exist).

The \emph{sloppy $p$-boosting algorithm} is one such algorithm.  It 

L1 and L2 algorithms are special cases of this algorithm.

\begin{linefigure}
\begin{center}
\includegraphics{figures/sloppy_search.epsg}
\end{center}
\begin{capt}{Line search for sloppy $p$-boosting algorithm}{fig:sloppy line search}
Comparison between the line search for the sloppy and strict
algorithms.  The strict alrogithm searches directly along the
$p$-convex hull.  The sloppy algorithm searches along the same line as
AdaBoost, and then normalises to put the point back onto the
$p$-convex hull.
\end{capt}
\end{linefigure}


\subsection{Gravity algorithm}

  Another possibility is to adjust the cost function
(\ref{eqn:theory:cost function}) to be of the form
%
\begin{equation}
C(F) = \frac{1}{l} \sum_{i=1}^{l} \exp
\left\{ -\bfy_iF(\bfx_i) \right\} \quad + \quad \lambda \|b\|_p
\label{eqn:regularisation}
\end{equation}
%
where $\lambda$ indicates how important a small $p$-norm is.  One
obvious problem with this method is that there are now \emph{two}
parameters to optimise ($p$ and $\lambda$).  The sharp ``corners'' of a
$p$-convex hull with $p \rightarrow 0$ may also introduce practical
problems with gradient descent (it may get stuck in a corner).

A further refinement of this solution would be to choose a much larger
universe \calX\ (such as $\|\mathbf{b}\|_2 < 1$) without the corners,
and again use the cost function (\ref{eqn:regularisation}).

Problems with this algorithm

\subsection{Summary}

\section{Chapter summary}



