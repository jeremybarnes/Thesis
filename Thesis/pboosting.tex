% pboosting.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\chapter{Development of $p$-boosting algorithms}
\label{chapter:pboosting}

This chapter describes the concepts behind and a theoretical
justification of \emph{$p$-boosting}, a generalisation of the boosting
algorithm.  The treatment is somewhat abstract; further chapters will
be more concrete as practical issues are considered.

\section{Avoiding overfitting of boosting algorithms}

Issues of overfitting have been addressed several times in this
thesis.  Recall that when given an algorithm, we avoid overfitting by
halting training at the point where the generalisation error is at a
minimum (figure \ref{fig:overfitting}).  Theoretically, we see that
guaranteed generalisation performance is the sum of the empirical risk
(which \emph{decreases} as $t \rightarrow \infty$) and a confidence
interval (which \emph{increases} as $t \rightarrow \infty$); figure
\ref{fig:boosting generalisation bound form} provides more detail.

\begin{linefigure}
Given a set of $m$ training samples $X$, a classifier $F \in \calX$
chosen by a boosting algorithm  with probability of at least $1 - \delta$,
%
\begin{equation}
\underbrace{\Pr(\mathrm{error}|\hat{f})}_{\mbox{\small{true risk}}}
\quad \leq \quad
\underbrace{R_{\emp}^{\gamma}(\hat{f})}_{\mbox{\small{empirical risk}}}
\quad + \quad
\underbrace{b(\delta, |\calH|, m)}_{\mbox{\small{confidence interval}}}
\label{eqn:boosting general bound}
\end{equation}
%
where the function $b(\delta, |\calH|, m)$ has the following
properties:
\begin{enumerate}
\item	$b(\delta, \cdot, \cdot)$ is nonincreasing with $\delta$;
\item	$b(\cdot, |\calH|, \cdot)$ is nondecreasing with $|\calH|$;
\item	$b(\cdot, \cdot, m)$ is nonincreasing with $m$.
\end{enumerate}
\caption{Form of generalisation performance bounds for boosting (after
figure \ref{fig:generalisation bound form})}
\label{fig:boosting generalisation bound form}
\end{linefigure}

We attack the problem by concentrating on the confidence interval.
It was shown in chapter \ref{chapter:booosting} that this confidence
interval increases with the complexity of the hypothesis class
$\calX$.  One obvious way, therefore, to avoid overfitting is to limit
the complexity of the class of hypotheses that our learning algorithm
can choose.  This process is known as \emph{capacity control}.
Specifically, instead of choosing $\calX = \co (\calH)$ as AdaBoost
does, we choose $\calX = \co_p(\calH)$ for some $p > 0$.%
\footnote{For $p=1$, the hypothesis space matches that of Boosting.}
Then, for $p < 1$, we have $|\co_p (\calH)| < |\co (\calH)|$.  In
effect, we have added a parameter $p$ to our algorithm which can be
adjusted to tune the capacity of $\calX$.  Using the language of
section \ref{sec:srm}, the $p$ parameter allows us to define a
structure on $\calX$.

By decreasing $p$, we decrease our confidence interval.  However, the
empirical risk is likely to \emph{increase} as $p \rightarrow 0$.
We have a tradeoff between the two terms; we need to use structural
risk minimisation to find the optimal $p$ value.  Figure
\ref{fig:optimal p value} illustrates this point.

\begin{linefigure}
\begin{center}
\includegraphics{figures/optimal_p_value.epsg}
\end{center}
\begin{capt}{The effect of $p$ on true risk}{fig:optimal p value}
The true risk (thick line) is bounded by the sum of the empirical risk
(solid line) and confidence interval (dashed line).  The point chosen
by the SRM principle is indicated; this is the optimal $p$ value in
the sense that it leads to the best generalisation performance.
\end{capt}
\end{linefigure}

\subsection{Generalisation performance of $p$-convex boosting
algorithms}

We now combine the covering-number generalisation performance bound
(\ref{eqn:covering number bound}) with the approximate values of
covering numbers for $p$-convex hulls (\ref{eqn:approx p-convex
bound}), to obtain the following theorem on the generalisation
performance of classifiers over $p$-convex hulls.

\begin{theorem}[Generalisation performance over $p$-convex hulls]
The generalisation error of a hypothesis $F \in \co_p(\calH)$ where
$\calH$ has VC dimension???????? $d$ over a set of $m$ independent training
samples with probability at least $1 - \delta$ can be approximated by
%
\begin{equation}
R(F) \lesssim R_{\emp}^{\gamma}(F) + \sqrt{ \frac{8}{m} \left[ \log 2
+ c(p) \: d \: \left( \frac{2}{\gamma} \right)^{\frac{2p}{2-p}} \log
\left( \frac{2}{\gamma} \right) - \log \delta \right]}
\end{equation}
where $c(p)$ is a constant that depends upon $p$. 
%
\end{theorem}

This result is necessarily an approximation, due to the approximate
nature of the theorems it is based upon.  However, it is clear from
the formulation that the confidence interval will decrease as $p
\rightarrow 0$.


\section{Development of algorithms}

Having established that boosting algorithms which operate on
$p$-convex hulls have desireable property of a tunable capacity via
the $p$ parameter, we turn to
questions of how to modify AdaBoost to operate on a $p$-convex hull.
We first descrive a ``naive $p$-boosting'' algorithm, which attempts
to directly modify the classifier weights to produce this effect.
After considering shortcomings of the naive algorithm, we derive a	
gradient descent algorithm ``strict $p$-boosting'', using a restricted
domain $\calX = \co_p (\calH)$.  Unfortunately, the cost space
optimised by this algorithm has undesirable properties which leads to
the algorithm getting ``stuck'' for $p < 1$.  As a result, we develop
the ``sloppy $p$-boosting'' algorithm which sacrifices theoretical
integrity for practical viability.  Finally, we briefly consider a
different type of $p$-boosting algorithm (still based on gradient
descent) that uses regularisation to achieve $\calX \approx
\co_p(\calH)$.  This algorithm is called the ``gravity $p$-boosting
algorithm''.


\subsection{Naive algorithm}

The naive algorithm (named with the benefit of hindsight) attempts to
modify the $b$ values of AdaBoost in a manner that will produce a
``$p$-convex like'' distribution of parameters on $p$.

The key feature of the algorithm is the calculation of the classifier
weights $b$.  Denoting the classifier weight that AdaBoost would have
used as $b'_t$, we use the formula
%
\begin{equation}
b_t = (b'_t)^{\frac{1}{p}} = \left[ - 1/2 \log \left( \frac{\epsilon_t}{1
- \epsilon_t} \right) \right]^\frac{1}{p}
\end{equation}
%
Thus, for $p < 1$ the hypotheses with low training error (and hence
high classifier weights) have these weights ``stretched'' out.  The
effect of $p$ is plotted figure \ref{fig:naive b values}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/naive.epsg}
\end{center}
\begin{capt}{Effect of $p$ on classifier weights for naive algorithm}{fig:naive b values}
The three lines show $b_t$ against $\epsilon_t$ for $p \in \{ 0.5,
0.7. 1.0 \}$.
\end{capt}
\end{linefigure}

There are two options for updating the sample weights $w$: based upon
the value of $b_t$, or based upon $b'_t$ (unchanged from AdaBoost).
The first option yields the equation
%
\begin{equation}
w_i|_{t+1} = \left\{
\begin{array}{cl}
	w_i|_t / Z_t \exp \left\{ (b'_t)^{1/p} \right\} & \qquad \qquad \mbox{if
	$f_t(x_i) = y_i$} \\
	w_i|_t / Z_t \exp \left\{ -(b'_t)^{1/p} \right\} 	& \qquad \qquad
	\mbox{otherwise} \\
\end{array} \right.
\end{equation}
%
The important point to notice is that the $1/p$ power (which is
greater than one for $p < 1$) is inside an exponential; this could
(and does) lead to the exponential function getting extremely large
(outside the limits of IEEE floating point numbers).  Thus we update
our sample weights based upon $b'_t$; they are identical to those
chosen by the boosting algorithm.

The naivity of the algorithm becomes apparent when considered in
the gradient descent framework.  As our sample weights are calculated
the same as AdaBoost; thus given an identical initial state both
algorithms would proceed in the same ``direction''.  However the step
sizes are different: AdaBoost performs a line search for the minimum
in this direction and moves to that point; whereas the naive algorithm
also searches for that point \emph{and then purposely avoids it!}
Experiments with this algorithm prove to have indifferent performance.

\subsection{Strict algorithm}

The strict algorithm is the purest of all $p$-boosting algorithms
considered in this thesis.  It performs gradient descent, using the
same cost function and inner product as AdaBoost, but uses the the
universal set $\calX = \co_p(\calH)$.  It is called the ``strict''
algorithm because it also performs its line searches along lines
$\ell$ where each point $p$ on $\ell$, $p \in \calX$.  In other words,
it confines its line search to the $p$-convex hull.  This idea is
illustrated in figure \ref{fig:strict line search}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/strict_search.epsg}
\end{center}
\begin{capt}{Line search for strict $p$-boosting algorithm}{fig:strict line search}
The strict $p$-boosting algorithm confines its line search to the
$p$-convex hull of points, whereas AdaBoost searches orthogonal to the
current hypothesis.
\end{capt}
\end{linefigure}

Algebraicaly, we are seeking a minimum of the expression
%
\begin{equation}
C(F_{t+1}) = \sum_{i=1}^{m} \exp \left\{ -y_i F_{t+1}(\bfx_i) \right\}
\end{equation}
%
subject to the constraint that
%
\begin{equation}
\| b \|_{p} = 1
\end{equation}

Substituting in the constraint, and performing some manipulations, we
are looking for
%
\begin{equation}
b_{t+1} = \frac{\alpha}{\left( 1 + \alpha^p \right) ^ {1/p}}
\end{equation}
%
where
%
\begin{equation}
\alpha = \argmin_{\alpha} \sum_{i=1}^{m} \exp \left\{ -y_i \left(
\frac{F_{t}(\bfx_i) + \alpha h_{t+1}(\bfx_i)}{\left( 1 + \alpha ^p
\right) ^{1/p}} \right) \right\}
\label{eqn:strict minimise function}
\end{equation}

Unfortunately, it is not possible to minimise this algeberically as
for AdaBoost; we instead have to be content with a numerical
optimisation.  The method chosen is Newton-Raphson iteration, which
searches for $\partial C(F) / \partial \alpha = 0$ by repeatedly
approximating it with a parabola until a tolerance is reached.

What we have \emph{not} done is to consider whether or not a solution
to (\ref{eqn:strict minimise function}) exists.  The following theorem
does so.

\begin{theorem}[Existance of a minimum for strict boost cost]
I sure hope that I can come up with something.  I know that it always
finds a minimum for $p \geq 1$ (from observations); it sometimes works
for $p < 1$; the cutoff seems to be at approximately $p = 0.8$.
Whether this is dependent upon the data or the weaklearner I don't
know; I suspect that it has something to do with how well the
weaklearner works.

The reason a solution doesn't exist is that it is for $\alpha < 0$,
which means we are trying to optimise a hill instead of a valley (so
we get to the top).
\end{theorem}




Finally, we need to consider the sample weight updates.  The gradient
descent theory gives us the sample weights (\ref{eqn:gradient sample
weights}).

\subsection{Sloppy algorithm}

Given the problems with the strict boost algorithm, it is reasonable
to consider using an algorithm with a similar effect but a simpler
form for the cost function (in particular, one for which a minimum is
guaranteed to exist).

The \emph{sloppy $p$-boosting algorithm} is one such algorithm.  It 

L1 and L2 algorithms are special cases of this algorithm.

\begin{linefigure}
\begin{center}
\includegraphics{figures/sloppy_search.epsg}
\end{center}
\begin{capt}{Line search for sloppy $p$-boosting algorithm}{fig:sloppy line search}
Comparison between the line search for the sloppy and strict
algorithms.  The strict alrogithm searches directly along the
$p$-convex hull.  The sloppy algorithm searches along the same line as
AdaBoost, and then normalises to put the point back onto the
$p$-convex hull.
\end{capt}
\end{linefigure}


\subsection{Gravity algorithm}

  Another possibility is to adjust the cost function
(\ref{eqn:theory:cost function}) to be of the form
%
\begin{equation}
C(F) = \frac{1}{l} \sum_{i=1}^{l} \exp
\left\{ -\bfy_iF(\bfx_i) \right\} \quad + \quad \lambda \|b\|_p
\label{eqn:regularisation}
\end{equation}
%
where $\lambda$ indicates how important a small $p$-norm is.  One
obvious problem with this method is that there are now \emph{two}
parameters to optimise ($p$ and $\lambda$).  The sharp ``corners'' of a
$p$-convex hull with $p \rightarrow 0$ may also introduce practical
problems with gradient descent (it may get stuck in a corner).

A further refinement of this solution would be to choose a much larger
universe \calX\ (such as $\|\mathbf{b}\|_2 < 1$) without the corners,
and again use the cost function (\ref{eqn:regularisation}).

Problems with this algorithm

\subsection{Summary}

\section{Chapter summary}



