% pboosting.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\chapter{Development of $p$-boosting algorithms}
\label{chapter:pboosting}

This chapter describes the concepts behind and a theoretical
justification of \emph{$p$-boosting}, a generalisation of the boosting
algorithm.  The treatment is somewhat abstract; further chapters will
be more concrete as practical issues are considered.

\section{Avoiding overfitting of boosting algorithms}

Issues of overfitting have been addressed several times in this
thesis.  Recall that when given an algorithm, we avoid overfitting by
halting training at the point where the generalisation error is at a
minimum (figure \ref{fig:overfitting}).  Theoretically, we see that
guaranteed generalisation performance is the sum of the empirical risk
(which \emph{decreases} as $t \rightarrow \infty$) and a confidence
interval (which \emph{increases} as $t \rightarrow \infty$); figure
\ref{fig:boosting generalisation bound form} provides more detail.

\begin{linefigure}
Given a set of $m$ training samples $X$, a classifier $F \in \calX$
chosen by a boosting algorithm  with probability of at least $1 - \delta$,
%
\begin{equation}
\underbrace{\Pr(\mathrm{error}|\hat{f})}_{\mbox{\small{true risk}}}
\quad \leq \quad
\underbrace{R_{\emp}^{\gamma}(\hat{f})}_{\mbox{\small{empirical risk}}}
\quad + \quad
\underbrace{b(\delta, |\calH|, m)}_{\mbox{\small{confidence interval}}}
\label{eqn:boosting general bound}
\end{equation}
%
where the function $b(\delta, |\calH|, m)$ has the following
properties:
\begin{enumerate}
\item	$b(\delta, \cdot, \cdot)$ is nonincreasing with $\delta$;
\item	$b(\cdot, |\calH|, \cdot)$ is nondecreasing with $|\calH|$;
\item	$b(\cdot, \cdot, m)$ is nonincreasing with $m$.
\end{enumerate}
\caption{Form of generalisation performance bounds for boosting (after
figure \ref{fig:generalisation bound form})}
\label{fig:boosting generalisation bound form}
\end{linefigure}

We attack the problem by concentrating on the confidence interval.
It was shown in chapter \ref{chapter:boosting} that this confidence
interval increases with the complexity of the hypothesis class
$\calX$.  One obvious way, therefore, to avoid overfitting is to limit
the complexity of the class of hypotheses that our learning algorithm
can choose.  This process is known as \emph{capacity control}.
Specifically, instead of choosing $\calX = \co (\calH)$ as AdaBoost
does, we choose $\calX = \co_p(\calH)$ for some $p > 0$.%
\footnote{For $p=1$, the hypothesis space matches that of Boosting.}
Then, for $p < 1$, we have $|\co_p (\calH)| < |\co (\calH)|$.  In
effect, we have added a parameter $p$ to our algorithm which can be
adjusted to tune the capacity of $\calX$.  Using the language of
section \ref{sec:theoretical overfitting}, the $p$ parameter allows us
to define a structure on $\calX$.

By decreasing $p$, we decrease our confidence interval.  However, the
empirical risk is likely to \emph{increase} as $p \rightarrow 0$.
We have a tradeoff between the two terms; we need to use structural
risk minimisation to find the optimal $p$ value.  Figure
\ref{fig:optimal p value} illustrates this point.

\begin{linefigure}
\begin{center}
\includegraphics{figures/optimal_p_value.epsg}
\end{center}
\begin{capt}{The effect of $p$ on true risk}{fig:optimal p value}
The true risk (thick line) is bounded by the sum of the empirical risk
(solid line) and confidence interval (dashed line).  The point chosen
by the SRM principle is indicated; this is the optimal $p$ value in
the sense that it leads to the best generalisation performance.
\end{capt}
\end{linefigure}

\subsection{Generalisation performance of $p$-convex boosting
algorithms}

We now combine the covering-number generalisation performance bound
(\ref{eqn:covering number bound}) with the approximate values of
covering numbers for $p$-convex hulls (\ref{eqn:approx p-convex
bound}), to obtain the following theorem on the generalisation
performance of classifiers over $p$-convex hulls.

\begin{theorem}[Generalisation performance over $p$-convex hulls]
\label{thm:p convex generalisation}
Consider a a hypothesis $F \in \co_p(\calH)$ where the covering
numbers of $\calH$ grow as $\cover{\calH}{\epsilon} \approx \left
( \frac{1}{\epsilon} \right) ^d$.% 
\footnote{See theorem \ref{thm:p-convex bound}.}
Then the generalisation error over a set of $m$ independent training
samples with probability at least $1 - \delta$ can be approximated by 
%
\begin{equation}
R(F) \lesssim R_{\emp}^{\gamma}(F) + \sqrt{ \frac{8}{m} \left[ \log 2
+ c(p) \: d \: \left( \frac{2}{\gamma} \right)^{\frac{2p}{2-p}} \log
\left( \frac{2}{\gamma} \right) - \log \delta \right]}
\end{equation}
where $c(p)$ is a constant that depends upon $p$. 
%
\end{theorem}

This result is necessarily an approximation, due to the approximate
nature of the theorems it is based upon.  However, it is clear from
the formulation that the confidence interval will decrease as $p
\rightarrow 0$; and thus we can use the $p$ parameter as an explicit
parameter for capacity control.


\section{Development of algorithms}

Having established that boosting algorithms which operate on
$p$-convex hulls have desireable property of a tunable capacity via
the $p$ parameter, we turn to
questions of how to modify AdaBoost to operate on a $p$-convex hull.
We first descrive a ``naive $p$-boosting'' algorithm, which attempts
to directly modify the classifier weights to produce this effect.
After considering shortcomings of the naive algorithm, we derive a	
gradient descent algorithm ``strict $p$-boosting'', using a restricted
domain $\calX = \co_p (\calH)$.  Unfortunately, the cost space
optimised by this algorithm has undesirable properties which leads to
the algorithm getting ``stuck'' for $p < 1$.  As a result, we develop
the ``sloppy $p$-boosting'' algorithm which sacrifices theoretical
integrity for practical viability.  Finally, we briefly consider a
different type of $p$-boosting algorithm (still based on gradient
descent) that uses regularisation to achieve $\calX \approx
\co_p(\calH)$.  This algorithm is called the ``gravity $p$-boosting
algorithm''.


\subsection{Na\"{\i}ve algorithm}

The na\"{\i}ve algorithm (named with the benefit of hindsight) attempts to
modify the $b$ values of AdaBoost in a manner that will produce a
``$p$-convex like'' distribution of parameters on $p$.

The key feature of the algorithm is the calculation of the classifier
weights $b$.  Denoting the classifier weight that AdaBoost would have
used as $b'_t$, we use the formula
%
\begin{equation}
b_t = (b'_t)^{\frac{1}{p}} = \left[ - 1/2 \log \left( \frac{\epsilon_t}{1
- \epsilon_t} \right) \right]^\frac{1}{p}
\end{equation}
%
Thus, for $p < 1$ the hypotheses with low training error (and hence
high classifier weights) have these weights ``stretched'' out.  The
effect of $p$ is plotted figure \ref{fig:naive b values}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/naive.epsg}
\end{center}
\begin{capt}{Effect of $p$ on classifier weights for naive algorithm}{fig:naive b values}
The three lines show $b_t$ against $\epsilon_t$ for $p \in \{ 0.5,
0.7. 1.0 \}$.
\end{capt}
\end{linefigure}

There are two options for updating the sample weights $w$: based upon
the value of $b_t$, or based upon $b'_t$ (unchanged from AdaBoost).
The first option yields the equation
%
\begin{equation}
w_i|_{t+1} = \left\{
\begin{array}{cl}
	w_i|_t / Z_t \exp \left\{ (b'_t)^{1/p} \right\} & \qquad \qquad \mbox{if
	$f_t(x_i) = y_i$} \\
	w_i|_t / Z_t \exp \left\{ -(b'_t)^{1/p} \right\} 	& \qquad \qquad
	\mbox{otherwise} \\
\end{array} \right.
\end{equation}
%
When using this first option, the $1/p$ power (which is
greater than one for $p < 1$) is inside an exponential; and could
(and does) lead to the exponential function getting extremely large
(outside the limits of IEEE floating point numbers).  Thus the sample
weights are updated based upon $b'_t$; they are identical to those
chosen by the boosting algorithm.

The na\"{\i}vity of the algorithm becomes apparent when considered in
the gradient descent framework.  The sample weights are calculated
the same as AdaBoost; thus given an identical initial state both
algorithms would proceed in the same ``direction''.  However the step
sizes are different: AdaBoost performs a line search for the minimum
in this direction and moves to that point; whereas the naive algorithm
also searches for that point \emph{and then purposely avoids it!}
Experiments with this algorithm prove to have indifferent performance.

\section{Strict algorithm}

The strict algorithm is the purest of all $p$-boosting algorithms
considered in this thesis (although not the best performing).  It
performs gradient descent, using the 
same cost function and inner product as AdaBoost, but uses the the
universal set $\calX = \co_p(\calH)$.  It is called the ``strict''
algorithm because it confines its line search to the $p$-convex hull
(AdaBoost performs its line search along a line orthogonal to the
combined hypothesis $F_{t}$).  This idea is illustrated in figure
\ref{fig:strict line search}. 

\begin{linefigure}
\begin{center}
\includegraphics{figures/strict_search.epsg}
\end{center}
\begin{capt}{Line search for strict $p$-boosting algorithm}{fig:strict line search}
The strict $p$-boosting algorithm confines its line search to the
$p$-convex hull of points, whereas AdaBoost searches orthogonal to the
current combined hypothesis.
\end{capt}
\end{linefigure}

\subsection{Classifier weights}

We derive an expression for the classifier weights, using the gradient
descent framework of chapter \ref{chapter:boosting}.  Algebraicaly, we
are seeking a minimum of the expression 
%
\begin{equation}
\label{eqn:classifier weight cost function approximation}
C(F_{t+1}) = \sum_{i=1}^{m} \exp \left\{ -y_i F_{t+1}(\bfx_i) \right\}
\end{equation}
%
subject to the constraint that
%
\begin{equation}
\| b \|_{p} = 1
\end{equation}

Substituting in the constraint, and performing some manipulations, we
are looking for
\todo{Emphasise that \emph{all} weights are updated, not just $b_{t+1}$.}
%
\begin{equation}
b_{t+1} = \frac{\alpha}{\left( 1 + \alpha^p \right) ^ {1/p}}
\end{equation}
%
where
%
\begin{equation}
\alpha = \argmin_{\alpha} \sum_{i=1}^{m} \exp \left\{ -y_i \left(
\frac{F_{t}(\bfx_i) + \alpha h_{t+1}(\bfx_i)}{\left( 1 + \alpha ^p
\right) ^{1/p}} \right) \right\}
\label{eqn:strict minimise function}
\end{equation}

Unfortunately, it is not possible to minimise this algeberically as
for AdaBoost; we instead have to be content with a numerical
optimisation.  The method chosen is Newton-Raphson iteration, which
searches for $\partial C(F) / \partial \alpha = 0$ by repeatedly
approximating it with a parabola until a tolerance is reached.
Several technical points concerned with making the Netwon-Raphson
method robust are addressed in appendix \ref{appendix:newton-raphson}
and \cite{Heath97}.

\subsection{Sample weights}

As the strict algorithm is performing gradient descent, the sample
weights are calculated using the results from theorem
\ref{thm:gradient descent weights and direction}:
%
\begin{equation}
w_{t,i} = \frac{C'(y_i F_t (\bfx_i))}{\sum_{j=1}^{m} C'(y_j
F(\bfx_j))} = \frac{- \exp \{ y_i F_t (\bfx_i)\}}{\sum_{j=1}^{m} -
\exp \{y_j F(\bfx_j)\}}
\end{equation}

\subsection{Existance of an optimal $b_{t+1}$ for $p < 1$}

As we shall see in chapter \ref{chapter:results}, when $p < 1$ the
strict $p$-boosting algorithm suffers from a cost surface that is in
many cases strictly increasing along the $p$-convex hull after a
handfull of iterations.  The problem with a strictly increasing cost
function is that it results in $F_{t+1} = F_t$, and the algorithm must
be aborted (or continue in steady state endlessly).

In order to determine the mechanism through this phenomenon occurs,
some very simple simulations were performed.  These simulations
trained the strict algorithm with $p = \frac{1}{2}$ on a simple
(random) dataset in $\bbR^2$ containing 10 samples.  It was observed
that sometimes there was no optimal solution at $t=2$ (that is, 
\emph{no} $p$-convex combination of the first two weaklearners produced a
smaller cost than the first weaklearner only.  It was furthermore
observed that a sufficient condition for there to be no solution was:
%
\begin{enumerate}
\item	The weaklearner $h_1$ classified 9 out of 10 training samples
	correctly; 
\item	The weaklearner $h_2$ classified 8 out of 10 training samples
	correctly, including the sample it classified
	\emph{incorrectly} on the first round.
\end{enumerate}
%
Furthermore, adjusting the $p$ parameter revealed that this behaviour
occurred for $p \leq 0.6695$, and did \emph{not} occur for $p \geq
0.6698$.  Thus, there appears to be a threshold value of $p$ above
which this behaviour is not observed.

Recalling that the cost functional $C(F)$ is the sum over the $m$
training samples of the cost function approximation
(\ref{eqn:classifier weight cost function approximation}), it is
useful to look at the contribution of each \emph{sample} $S = (\bfx,
y)$ to the total cost, as a function of $\alpha$:
%
\begin{equation}
\label{eqn:sample cost function}
c_S(\alpha, p) = \exp \left\{ \frac{-y F_t(\bfx) - \alpha y h_{t+1}
(\bfx)}{(1 + \alpha^p)^{\frac{1}{p}}} \right\}
\end{equation}
%
When $\alpha = 0$, (\ref{eqn:sample cost function}) reduces to
%
\begin{equation}
c_S(0, p) = \exp \left\{ -y F_t(\bfx) \right\}
\end{equation}
%
It is useful to consider the meaning behind each part of
(\ref{eqn:sample cost function}).  The term $y F_t(\bfx)$ is simply
the margin of the sample $S$ at iteration $t$.  We write this $m_{S,
t}$.  The term $y \alpha f_{t+1}(\bfx)$ is equal to $\alpha$ if the
weaklearner $h_{t+1}$ classifies $S$ correctly, or $-\alpha$
otherwise.  The denominator $(1 + \alpha^p)^{\frac{1}{p}}$ is equal to
$1$ at $\alpha=0$ and approaches $\alpha$ as $\alpha \rightarrow
\infty$.

When applied to our simple example, this analysis yields some
insight into the processes that are occurring.  Of the ten samples,
seven were classified correctly by both weaklearning hypotheses; one
was classified wrongly on the first iteration but correctly on the
second; and two were classified correctly at first but wrongly on the
second.

When performing the Netwon-Raphson search for $t=2$, the margin
$m_{(\bfx_i, y_i), 1}$ will evaluate to one of two values: $+1$ if 
$h_1(\bfx_i) = y_i$ and $-1$ otherwise.  Likewise, the margin of the
weaklearning hypothesis $h_2$ will be $\pm 1$ depending upon whether
it classified the sample correctly or not.  As a result, there are
four possibilities for the sample cost function, depending upon the
history of the first two weak learning hypotheses:
%
\providecommand{\ra}{\Rightarrow}
\providecommand{\Pn}{(1 + \alpha^p)^{\frac{1}{p}}}
\begin{equation}
\begin{array}{rclrcl}
c_{(-1 \ra -1),i}(p,\alpha) & = & \exp \left\{ \frac{ 1 + \alpha}{\Pn} \right\} &
c_{(-1 \ra +1),i}(p,\alpha) & = & \exp \left\{ \frac{ 1 - \alpha}{\Pn} \right\} \\
c_{(+1 \ra -1),i}(p,\alpha) & = & \exp \left\{ \frac{-1 + \alpha}{\Pn} \right\} &
c_{(+1 \ra +1),i}(p,\alpha) & = & \exp \left\{ \frac{-1 - \alpha}{\Pn} \right\}
\end{array}
\end{equation}
%
where the subscript to $c$ gives the history: $(u \ra v)$ means that
hypothesis $H_1 = \sign(F_1)$ had margin $yF_1(\bfx) = u$, and that
the weak learning hypothesis $h_{2}$ had margin $yh_2(\bfx) = v$. These
functions are plotted against alpha in figure \ref{fig:alpha
function}.

\begin{linefigure}
\begin{center}
\hspace*{-1cm}\includegraphics{figures/alpha_function.epsg}
\end{center}
\begin{capt}{Sample cost contributions}{fig:alpha function}
Parts (a) through (d) show the sample cost at iteration $t=2$ as a
function of the history of the margins through weaklearners $h_1$ and
$h_2$.  The dashed line indicates $p=0.5$, the thick line $p=1$, and
the thin solid line $p=2$.  Part (e) plots the cost \emph{functional}
for the examples (10 samples) described in the text (in the example,
$p=0.5$ is used; the other curves are plotted for reference).
\end{capt}
\end{linefigure}

The shape of the $(-1, +1)$ and $(+1, -1)$ curves is
sensible: as $\alpha$ increases, samples which were wrong and are now
classified correctly decrease in weight, while the opposite occurs for
samples which were right and are now wrong.  The main qualitative
difference between $p=0.5$ and $p=1$ is the shape of the $(-1 \ra -1)$
and $(+1 \ra +1)$ curves: these are flat for $p=1$ (indicating that
the algorithm is indifferent to samples which do not change their
margin), but form a hill for $p < 1$.

* $(+1 \ra -1)$ has $\frac{\partial c(\alpha)}{\partial \alpha} > 0$
  always.

Part (d) of figure \ref{fig:alpha function} displays the cost
functional 
\begin{equation}
C(\alpha,p) = \sum_{i=1}^{10} c_{(\bfx, y)}(\alpha, p) 
= 7c_{(1 \ra 1)} + 2c_{(1 \ra -1)} + c_{(-1 \ra 1)}
\end{equation}
for different values of $p$.  Note that a well-defined minimum exists
for $p \geq 1$, but the cost function is monotonic increasing for $p =
0.5$.  It is clear that the contribution of $2c_{(+1 \ra -1)}$
prevents a minima from occurring.

While this analysis is by no means complete, some general observations
can be made on likely reasons for the strict algorithm terminating
early.  Choosing $p < 1$ appears to produce a \emph{maxima} in the
per-sample cost functional for samples that had a positive margin on
iteration $t$ and were again classified correctly on iteration $t+1$.

We have already seen from chapter \ref{chapter:boosting} that as $t
\rightarrow \infty$, the ``easy'' samples tend to be classified
correctly very often (this result is true for AdaBoost, and by
extrapolation true also for the strict algorithm, as both update their
sample weights in the same manner).  As a result, we would expect that
many samples would have a cost function of the shape shown in figure
\ref{fig:alpha function} (d).  On the other hand, samples that are
``hard'' tend to oscillate: as a result, there are few samples with a
cost function in the shape of figure \ref{fig:alpha function} (a)
(which could balance out the effect).  The net result is that as the
number of training iterations increases, the cost functional becomes
less and less likely to have a minima, and thus training terminates
early.

\section{Sloppy algorithm}

Given the problems with the strict boost algorithm, it is reasonable
to consider using an algorithm with a similar effect but a simpler
form for the cost function (in particular, one for which a minimum is
guaranteed to exist).

The \emph{sloppy $p$-boosting algorithm} is one such algorithm.  The
simplification involves searching along the same line as AdaBoost, and
then normalising to the $p$-convex hull \emph{after} a suitable point
has been found.  In this way, the hypothesis $F_t$ returned after
round $t$ is still $F_t \in \co_P(\calH)$, and the strictly increasing
cost functions that affect the strict algorithm are avoided.  Figure
\ref{fig:sloppy line search} illustrates the differences in the line
search between the sloppy and strict algorithms.

\begin{linefigure}
\begin{center}
\includegraphics{figures/sloppy_search.epsg}
\end{center}
\begin{capt}{Line search for sloppy $p$-boosting algorithm}{fig:sloppy line search}
Comparison between the line search for the sloppy and strict
algorithms.  The strict alrogithm searches directly along the
$p$-convex hull.  The sloppy algorithm searches along the same line as
AdaBoost, and then normalises to put the point back onto the
$p$-convex hull.
\end{capt}
\end{linefigure}

It should be noted that the sloppy algorithm is a straightforward
generalisation of the normed boosting algorithms surveyed in chapter
\ref{chapter:boosting}.


\section{Gravity algorithm}

The final variant of $p$-boosting considered in this thesis is
somewhat different to the other two.  This algorithm is less direct in
its approach: rather than restricting the gradient descent algorithm
to operate on a $p$-convex hull, it modifies the cost functional to
``pull'' the solution towards a low $p$-norm:
%
\begin{equation}
C(F) = \frac{1}{l} \sum_{i=1}^{l} \exp
\left\{ -\bfy_iF(\bfx_i) \right\} \quad + \quad \lambda \|b\|_p
\label{eqn:regularisation}
\end{equation}
%
where $\lambda$ indicates how important a small $p$-norm is.  This
formulation also allows some latitude in the universal set $\calX$
that is chosen: it would be sensible to choose $\calX = \co_2(\calH)$
or even $\calX = \lin(\calH)$ to ensure that no problems with gradient
descent occur.

There are several problems with this algorithm.  The most obvious is that
there are now \emph{two} parameters to optimise ($p$ and $\lambda$).
A less obvious problem is that rightmost term of
(\ref{eqn:regularisation}) in its written form will be strictly
increasing as $t \rightarrow \infty$, discounting the use of any
standard optimisation method.

This algorithm was only briefly considered during the course of this
project, due to the difficulties described above and lack of time.  As
a result, no experiments were performed using this algorithm; and it
will not be considered further.




