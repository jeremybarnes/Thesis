% pboosting.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\chapter{Development of $p$-boosting algorithms}
\label{chapter:pboosting}

This chapter describes the concepts behind and a theoretical
justification of \emph{$p$-boosting}, a generalisation of the boosting
algorithm.  The treatment is somewhat abstract; further chapters will
be more concrete as practical issues are considered.

\section{Avoiding overfitting of boosting algorithms}

Issues of overfitting have been addressed several times in this
thesis.  Recall that when given an algorithm, we avoid overfitting by
halting training at the point where the generalisation error is at a
minimum (figure \ref{fig:overfitting}).  Theoretically, we see that
guaranteed generalisation performance is the sum of the empirical risk
(which \emph{decreases} as $t \rightarrow \infty$) and a confidence
interval (which \emph{increases} as $t \rightarrow \infty$); figure
\ref{fig:boosting generalisation bound form} provides more detail.

\begin{linefigure}
Given a set of $m$ training samples $X$, a classifier $F \in \calX$
chosen by a boosting algorithm  with probability of at least $1 - \delta$,
%
\begin{equation}
\underbrace{\Pr(\mathrm{error}|\hat{f})}_{\mbox{\small{true risk}}}
\quad \leq \quad
\underbrace{R_{\emp}^{\gamma}(\hat{f})}_{\mbox{\small{empirical risk}}}
\quad + \quad
\underbrace{b(\delta, |\calH|, m)}_{\mbox{\small{confidence interval}}}
\label{eqn:boosting general bound}
\end{equation}
%
where the function $b(\delta, |\calH|, m)$ has the following
properties:
\begin{enumerate}
\item	$b(\delta, \cdot, \cdot)$ is nonincreasing with $\delta$;
\item	$b(\cdot, |\calH|, \cdot)$ is nondecreasing with $|\calH|$;
\item	$b(\cdot, \cdot, m)$ is nonincreasing with $m$.
\end{enumerate}
\caption{Form of generalisation performance bounds for boosting (after
figure \ref{fig:generalisation bound form})}
\label{fig:boosting generalisation bound form}
\end{linefigure}

We attack the problem by concentrating on the confidence interval.
It was shown in chapter \ref{chapter:booosting} that this confidence
interval increases with the complexity of the hypothesis class
$\calX$.  One obvious way, therefore, to avoid overfitting is to limit
the complexity of the class of hypotheses that our learning algorithm
can choose.  This process is known as \emph{capacity control}.
Specifically, instead of choosing $\calX = \co (\calH)$ as AdaBoost
does, we choose $\calX = \co_p(\calH)$ for some $p > 0$.%
\footnote{For $p=1$, the hypothesis space matches that of Boosting.}
Then, for $p < 1$, we have $|\co_p (\calH)| < |\co (\calH)|$.  In
effect, we have added a parameter $p$ to our algorithm which can be
adjusted to tune the capacity of $\calX$.  Using the language of
section \ref{sec:srm}, the $p$ parameter allows us to define a
structure on $\calX$.

By decreasing $p$, we decrease our confidence interval.  However, the
empirical risk is likely to \emph{increase} as $p \rightarrow 0$.
We have a tradeoff between the two terms; we need to use structural
risk minimisation to find the optimal $p$ value.  Figure
\ref{fig:optimal p value} illustrates this point.

\begin{linefigure}
\begin{center}
\includegraphics{figures/optimal_p_value.epsg}
\end{center}
\begin{capt}{The effect of $p$ on true risk}{fig:optimal p value}
The true risk (thick line) is bounded by the sum of the empirical risk
(solid line) and confidence interval (dashed line).  The point chosen
by the SRM principle is indicated; this is the optimal $p$ value in
the sense that it leads to the best generalisation performance.
\end{capt}
\end{linefigure}

\subsection{Generalisation performance of $p$-convex boosting
algorithms}

We now combine the covering-number generalisation performance bound
(\ref{eqn:covering number bound}) with the approximate values of
covering numbers for $p$-convex hulls (\ref{eqn:approx p-convex
bound}), to obtain the following theorem on the generalisation
performance of classifiers over $p$-convex hulls.

\begin{theorem}[Generalisation performance over $p$-convex hulls]
\label{thm:p convex generalisation}
Consider a a hypothesis $F \in \co_p(\calH)$ where the covering
numbers of $\calH$ grow as $\cover{\calH}{\epsilon} \approx \left
( \frac{1}{\epsilon} \right) ^d$.% 
\footnote{See theorem \ref{thm:p convex bound}.}
Then the generalisation error over a set of $m$ independent training
samples with probability at least $1 - \delta$ can be approximated by 
%
\begin{equation}
R(F) \lesssim R_{\emp}^{\gamma}(F) + \sqrt{ \frac{8}{m} \left[ \log 2
+ c(p) \: d \: \left( \frac{2}{\gamma} \right)^{\frac{2p}{2-p}} \log
\left( \frac{2}{\gamma} \right) - \log \delta \right]}
\end{equation}
where $c(p)$ is a constant that depends upon $p$. 
%
\end{theorem}

This result is necessarily an approximation, due to the approximate
nature of the theorems it is based upon.  However, it is clear from
the formulation that the confidence interval will decrease as $p
\rightarrow 0$; and thus we can use the $p$ parameter as an explicit
parameter for capacity control.


\section{Development of algorithms}

Having established that boosting algorithms which operate on
$p$-convex hulls have desireable property of a tunable capacity via
the $p$ parameter, we turn to
questions of how to modify AdaBoost to operate on a $p$-convex hull.
We first descrive a ``naive $p$-boosting'' algorithm, which attempts
to directly modify the classifier weights to produce this effect.
After considering shortcomings of the naive algorithm, we derive a	
gradient descent algorithm ``strict $p$-boosting'', using a restricted
domain $\calX = \co_p (\calH)$.  Unfortunately, the cost space
optimised by this algorithm has undesirable properties which leads to
the algorithm getting ``stuck'' for $p < 1$.  As a result, we develop
the ``sloppy $p$-boosting'' algorithm which sacrifices theoretical
integrity for practical viability.  Finally, we briefly consider a
different type of $p$-boosting algorithm (still based on gradient
descent) that uses regularisation to achieve $\calX \approx
\co_p(\calH)$.  This algorithm is called the ``gravity $p$-boosting
algorithm''.


\subsection{Na\"{\i}ve algorithm}

The na\"{\i}ve algorithm (named with the benefit of hindsight) attempts to
modify the $b$ values of AdaBoost in a manner that will produce a
``$p$-convex like'' distribution of parameters on $p$.

The key feature of the algorithm is the calculation of the classifier
weights $b$.  Denoting the classifier weight that AdaBoost would have
used as $b'_t$, we use the formula
%
\begin{equation}
b_t = (b'_t)^{\frac{1}{p}} = \left[ - 1/2 \log \left( \frac{\epsilon_t}{1
- \epsilon_t} \right) \right]^\frac{1}{p}
\end{equation}
%
Thus, for $p < 1$ the hypotheses with low training error (and hence
high classifier weights) have these weights ``stretched'' out.  The
effect of $p$ is plotted figure \ref{fig:naive b values}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/naive.epsg}
\end{center}
\begin{capt}{Effect of $p$ on classifier weights for naive algorithm}{fig:naive b values}
The three lines show $b_t$ against $\epsilon_t$ for $p \in \{ 0.5,
0.7. 1.0 \}$.
\end{capt}
\end{linefigure}

There are two options for updating the sample weights $w$: based upon
the value of $b_t$, or based upon $b'_t$ (unchanged from AdaBoost).
The first option yields the equation
%
\begin{equation}
w_i|_{t+1} = \left\{
\begin{array}{cl}
	w_i|_t / Z_t \exp \left\{ (b'_t)^{1/p} \right\} & \qquad \qquad \mbox{if
	$f_t(x_i) = y_i$} \\
	w_i|_t / Z_t \exp \left\{ -(b'_t)^{1/p} \right\} 	& \qquad \qquad
	\mbox{otherwise} \\
\end{array} \right.
\end{equation}
%
When using this first option, the $1/p$ power (which is
greater than one for $p < 1$) is inside an exponential; and could
(and does) lead to the exponential function getting extremely large
(outside the limits of IEEE floating point numbers).  Thus the sample
weights are updated based upon $b'_t$; they are identical to those
chosen by the boosting algorithm.

The na\"{\i}vity of the algorithm becomes apparent when considered in
the gradient descent framework.  The sample weights are calculated
the same as AdaBoost; thus given an identical initial state both
algorithms would proceed in the same ``direction''.  However the step
sizes are different: AdaBoost performs a line search for the minimum
in this direction and moves to that point; whereas the naive algorithm
also searches for that point \emph{and then purposely avoids it!}
Experiments with this algorithm prove to have indifferent performance.

\subsection{Strict algorithm}

The strict algorithm is the purest of all $p$-boosting algorithms
considered in this thesis.  It performs gradient descent, using the
same cost function and inner product as AdaBoost, but uses the the
universal set $\calX = \co_p(\calH)$.  It is called the ``strict''
algorithm because it also performs its line searches along lines
$\ell$ where each point $p$ on $\ell$, $p \in \calX$.  In other words,
it confines its line search to the $p$-convex hull.  This idea is
illustrated in figure \ref{fig:strict line search}.

\begin{linefigure}
\begin{center}
\includegraphics{figures/strict_search.epsg}
\end{center}
\begin{capt}{Line search for strict $p$-boosting algorithm}{fig:strict line search}
The strict $p$-boosting algorithm confines its line search to the
$p$-convex hull of points, whereas AdaBoost searches orthogonal to the
current hypothesis.
\end{capt}
\end{linefigure}

Algebraicaly, we are seeking a minimum of the expression
%
\begin{equation}
C(F_{t+1}) = \sum_{i=1}^{m} \exp \left\{ -y_i F_{t+1}(\bfx_i) \right\}
\end{equation}
%
subject to the constraint that
%
\begin{equation}
\| b \|_{p} = 1
\end{equation}

Substituting in the constraint, and performing some manipulations, we
are looking for
%
\begin{equation}
b_{t+1} = \frac{\alpha}{\left( 1 + \alpha^p \right) ^ {1/p}}
\end{equation}
%
where
%
\begin{equation}
\alpha = \argmin_{\alpha} \sum_{i=1}^{m} \exp \left\{ -y_i \left(
\frac{F_{t}(\bfx_i) + \alpha h_{t+1}(\bfx_i)}{\left( 1 + \alpha ^p
\right) ^{1/p}} \right) \right\}
\label{eqn:strict minimise function}
\end{equation}

Unfortunately, it is not possible to minimise this algeberically as
for AdaBoost; we instead have to be content with a numerical
optimisation.  The method chosen is Newton-Raphson iteration, which
searches for $\partial C(F) / \partial \alpha = 0$ by repeatedly
approximating it with a parabola until a tolerance is reached.
Several technical points concerned with making the Netwon-Raphson
method robust are addressed in appendix \ref{appendix:newton-raphson}
and \cite{Heath97}.

As we shall see in chapter \ref{chapter:results}, when $p < 1$ the
strict $p$-boosting algorithm suffers from a cost surface that is in
many cases strictly increasing along the $p$-convex hull after a
handfull of iterations.  The problem with a strictly increasing cost
function is that it results in $F_{t+1} = F_t$, and the algorithm must
be aborted (or continue in steady state endlessly).  Thus, a
simplification of the optimisation goal (\ref{eqn:strict minimise
function}) was required.


\subsection{Sloppy algorithm}

Given the problems with the strict boost algorithm, it is reasonable
to consider using an algorithm with a similar effect but a simpler
form for the cost function (in particular, one for which a minimum is
guaranteed to exist).

The \emph{sloppy $p$-boosting algorithm} is one such algorithm.  The
simplification involves searching along the same line as AdaBoost, and
then normalising to the $p$-convex hull \emph{after} a suitable point
has been found.  In this way, the hypothesis $F_t$ returned after
round $t$ is still $F_t \in \co_P(\calH)$, and the strictly increasing
cost functions that affect the strict algorithm are avoided.  Figure
\ref{fig:sloppy line search} illustrates the differences in the line
search between the sloppy and strict algorithms.

\begin{linefigure}
\begin{center}
\includegraphics{figures/sloppy_search.epsg}
\end{center}
\begin{capt}{Line search for sloppy $p$-boosting algorithm}{fig:sloppy line search}
Comparison between the line search for the sloppy and strict
algorithms.  The strict alrogithm searches directly along the
$p$-convex hull.  The sloppy algorithm searches along the same line as
AdaBoost, and then normalises to put the point back onto the
$p$-convex hull.
\end{capt}
\end{linefigure}

It should be noted that the sloppy algorithm is a straightforward
generalisation of the normed boosting algorithms surveyed in chapter
\ref{chapter:boosting}.


\subsection{Gravity algorithm}

The final variant of $p$-boosting considered in this thesis is
somewhat different to the other two.  This algorithm is less direct in
its approach: rather than restricting the gradient descent algorithm
to operate on a $p$-convex hull, it modifies the cost functional to
``pull'' the solution towards a low $p$-norm:
%
\begin{equation}
C(F) = \frac{1}{l} \sum_{i=1}^{l} \exp
\left\{ -\bfy_iF(\bfx_i) \right\} \quad + \quad \lambda \|b\|_p
\label{eqn:regularisation}
\end{equation}
%
where $\lambda$ indicates how important a small $p$-norm is.  This
formulation also allows some latitude in the universal set $\calX$
that is chosen: it would be sensible to choose $\calX = \co_2(\calH)$
or even $\calX = \lin(\calH)$ to ensure that no problems with gradient
descent occur.

There are several problems with this algorithm.  The most obvious is that
there are now \emph{two} parameters to optimise ($p$ and $\lambda$).
A less obvious problem is that rightmost term of
(\ref{eqn:regularisation}) in its written form will be strictly
increasing as $t \rightarrow \infty$, discounting the use of any
standard optimisation method.

This algorithm was only briefly considered during the course of this
project, due to the difficulties described above and lack of time.  As
a result, no experiments were performed using this algorithm; and it
will not be considered further.




