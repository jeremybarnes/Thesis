% conclusion.tex
% Jeremy Barnes, 6/10/1999
% $Id$

\chapter{Conclusion}
\label{chapter:conclusion}

This thesis has considered variants of the AdaBoost machine learning
algorithm, with the desirable property of explicit capacity control,
implemented by confining the combined hypotheses to a $p$-convex hull
of underlying hypotheses.  Theoretical results indicate that these
algorithms should outperform AdaBoost on noisy datasets.

Several ``$p$-boosting'' algorithms were developed: a na\"{\i}ve
approach (which showed no noticeable improvement over AdaBoost), a
``strict'' algorithm confined strictly to the $p$-convex hull at all
times, and a ``sloppy'' variant less restricted during the line
search.  A software toolbox was constructed to enable experimental
data to be collected, and this toolbox used to perform extensive
simulations.

The strict algorithm showed a small improvement over AdaBoost over the
difficult \ds{acacia} dataset, but in general performed poorly
(particularly for $p < 1$).  The poor performance was caused by the
cost functional proving to be strictly increasing, or to have a
minimum very close to the previous hypothesis (in effect the gradient
descent algorithm did not move significantly from its starting point).
Analysis of the features of the cost functional showed that ``easy''
samples produced hill-shaped sample cost functions that would
contribute to a strictly a increasing cost functional.

The sloppy algorithm performed well on noisy datasets, with the shape
of the capacity-vs-gener\-alis\-ation error curve matching the theoretical
prediction, and the generalisation error at the optimal
$p$ value significantly (up to 25\%) improved on AdaBoost (which
already performs \emph{very} well).  While it appears that this
algorithm did find a good solution, which was sparse as predicted by
the theory, it did so in a very inefficient manner (particularly for
$p < 1.2$), with up to several thousand iterations spent adding
hypotheses with weights several hundred orders of magnitude below a
significant level.  A form of the cost function designed to
accelerate the training process is proposed as a potential solution;
alternatively a different optimisation strategy could be used.

It can be concluded that the \emph{concept} of using a $p$-convex hull
has merit as a method of increasing the noise-tolerance of the already
very strong AdaBoost algorithm, but the algorithms that were developed
to \emph{implement} the idea were lacking.

No quantitative comparisons between $p$-boosting algorithms and
other AdaBoost derivatives with similar goals (soft margins
\cite{Ratsch98}, DOOM I/II \cite{Mason99, Mason99a}) were made.
However, these approaches do not appear incompatible with the $p$-convex
hull approach; it is likely that a combined approach would be superior
to both.

