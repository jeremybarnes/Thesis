% conclusion.tex
% Jeremy Barnes, 6/10/1999
% $Id$

\chapter{Conclusion}
\label{chapter:conclusion}

In this thesis we have considered variants of the AdaBoost machine
learning algorithm, with the desireable property of explicit capacity
control.  Capacity control is implemented by confining the class of
allowable combined hypotheses to lie on the $p$-convex hull of the
underlying hypotheses, and is adjustable via the $p$ parameter.

Theoretical results indicate that the performance of these adjustable
algorithms for the optimal $p$ value may be superior to AdaBoost,
especially in the presence of large amounts of label noise (where
AdaBoost is prone to overfitting).  The approximate and bounded-above
form of these theoretical results prevent stronger statements being
made.

Several variants of a ``$p$-boosting'' algorithm were developed.  The
first variant used a na\"{\i}ve approach, and its performance was not
significantly different from that of the AdaBoost algorithm.  The
second variant (``strict'') was confined strictly along the $p$-convex
hull at all times, but proved to be prone to early stopping for $p <
1$.  The third variant (``sloppy'') was less restricted during the
line search phase (searching along the same line as AdaBoost).

The strict algorithm performed poorly as compared to AdaBoost and the
sloppy algorithm, both in terms of generalisation performance and
training time.  This was a disappointing result as the strict
algorithm was the most direct implementation of the restriction to a
$p$-convex hull.  It is hypothesised that the observed behaviour is a 
result of the gradient descent optimisation not finding the global
minimum.  The sloppy algorithm performed well on noisy datasets, with
the shape of the capacity-vs-generalisation error curve matching the
theoretical prediction, and the generalisation error at the optimal
$p$ value significantly (up to 25\%) improved on AdaBoost--which
already performs \emph{very} well.  However, both the strict and the
sloppy algorithms required approximately an order of magnitude more
training iterations than AdaBoost to reach the optimal solution.  An
accelerated version is proposed that may improve this situation.

No quantitative comparisons between the $p$-boosting algorithm and
other AdaBoost derivatives with similar goals (soft margins
\cite{Ratsch98}, DOOM I/II \cite{Mason99, Mason99a}) were made.
However, the approaches used in these derivatives are not incompatible
with the $p$-convex hull approach, leaving open the possibility of a
combined approach.

In conclusion, the capacity controlled ``$p$-boosting'' algorithms
considered in this project have been shown to be a modest improvement
on the already excellent AdaBoost in some situations.  More extensive
testing is required before stronger conclusions can be drawn.
