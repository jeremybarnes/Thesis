\chapter{Results}



This chapter describes the progress made to date, with an emphasis on
the current situation (and not on how it was reached).  Some
history may be necessary to put the current situation in context.  The
chapter is divided into three sections: theoretical understanding,
practical verification, and tools.

The \emph{theoretical understanding} section describes the progress
made in gaining an understanding of the $p$-convex boosting algorithm
itself, the general theory behind all learning algorithms, and the
mathematical tools necessary to use this theory.

The \emph{practical verification} section describes the progress of
attempts to gain an understanding of the $p$-convex boosting algorithm
by running simulations.

Finally, the \emph{tools} section describes those tools (computer
software and hardware) that have been acquired or created in order to
perform the practical verification with.

\section{Theoretical understanding}

Progress in this area has been slow, due to the very recent
publication of most of the results that are relevant to the project
and the lack of material aimed at an audience outside the active
research community.  \marginpar{Should I put this in?}

The important areas seem to be the following: gradient descent,
margins, covering numbers, bounds on generalisation performance.
1.  


The following is a brief description of the key points that have been
identified concerning $p$-convex boosting:

\begin{itemize}
\item	The boosting algorithm (and most variants) implements gradient
	descent in the space of ($1$)-convex classifiers (Mason et al,
	1999) \cite{Mason99}.  The step size is calculated from the 

\end{itemize}

\section{Practical verification}

\section{Computer software and hardware}
Approximately 80 percent of activity on this project has involved
engineering of a suitable environment to perform the practical
experiments mentioned in the previous section.

\subsection{MATLAB toolbox}
The tools that have been developed are a MATLAB ``toolbox'' which
contains the following functionality:

\begin{itemize}
\item	An implementation of both the original boosting algorithm and
	the $p$-boosting algorithm, including functions to perform
	training, classification and testing.  These functions are
	completely general (no assumptions are made on the number of
	dimensions or classes) and have no arbitrary limitations
	(except those imposed by available system memory).

\item	Two weak learning algorithms: CART and Decision Stumps.
	Again, these are completely general.

\item	A dataset generator object, which can generate samples from a
	number of two dimensional ``toy'' distributions, and add a
	specified amount of attribute or classification noise.

\item	Functions useful for visualising two-dimensional classifiers.
	These include functions for plotting a dataset, the weight
	density of a dataset, and the decision boundaries and margin
	surface of classifiers.

\item	An automated test script, which will run many trials of a
	given test and statistically analyse the results.  It will
	also save its progress periodically to a fixed disk, which
	allows the tests to be stopped and restarted at will, and the
	data analysed or verified at a later date.
\end{itemize}

\subsubsection{Optimisation}

Much effort has been put into the optimisation of these functions.
The code that accounts for a significant proportion of CPU time has
been replaced by more efficient C code, and some algorithms have been
replaced by more efficient equivalents.  The C code has been written
in such a way that the compiler can perform register level
optimisation and other techniques such as loop unrolling.

\subsubsection{Documentation}

MATLAB includes a facility to allow documentation to be included with
the source file.
