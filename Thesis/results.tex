% results.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\chapter{Results and discussion}
\label{chapter:results}

This chapter summarises the outcome of the experiments described in
chapter \ref{chapter:method}, discusses their features, and compares
them with the results expected from the theory in chapters
\ref{chapter:slt} to \ref{chapter:pboosting}.

An opening observation is that Boosting algorithms are very erratic in
their behaviour.  As a result, it is difficult to make conclusive
judgments based on individual observations; at the very least
statistical measures need to be used, and even so a huge amount of
computing resources are required to obtain sufficient results to make
confident conclusions.

\section{Generalisation performance}

Figure \ref{fig:test err summary} is a high-level summary of results
on the generalisation performance of the $p$-boosting variants as
compared to AdaBoost.  The graph compares the result from the
hypothesis chosen via SRM (section \ref{sec:theoretical overfitting})
with AdaBoost. 

It can be seen that although the average generalisation performance of
the $p$-boosting algorithms is in most cases slightly better than that
of AdaBoost, only in a few cases is the difference significant.  The
only dataset upon which the strict algorithm generalises significantly
better than AdaBoost is \ds{acacia}.  The sloppy algorithm generalises
significantly better on \ds{ring10}, \ds{ring20}, \ds{ring30} and
\ds{acacia}; while the na\"{\i}ve algorithm always outperforms
AdaBoost by a small (never statistically significant) amount.

Thus, the sloppy algorithm appears to outperform AdaBoost on the noisy
datasets considered (recall that the \ds{acacia} dataset was chosen as
it was a difficult dataset).  This behaviour was expected--the
algorithm was specifically designed to implement capacity control.  It
also appears that the na\"{\i}ve algorithm has little effect on
generalisation performance (a somewhat surprising result--the
discussion in chapter \ref{chapter:pboosting} concluded that the
algorithm should be \emph{worse} than AdaBoost.  We shall return to
this result shortly).


\begin{linefigure}
\begin{center}
\includegraphics{figures/test_err_summary}
\end{center}
\begin{capt}{Comparison of AdaBoost and $p$-boosting test generalisation
performance}{fig:test err summary}
The diagonal line indicates points where the test error of AdaBoost
and the $p$-boosting algorithm are equal.  Points above line indicate
that the $p$-boosting algorithm generalises better than AdaBoost.  The
light bars indicate the spread of the trials, and are one standard
deviation in length either side of the mean.

\noindent{\bf Key:} $\bullet$ \ds{ring0}, $\times$ \ds{ring10}, $\circ$
\ds{ring20}, $\ast$ \ds{ring30}, $\Box$ \ds{sonar}, $\bigtriangleup$
\ds{wpbc}, $\Diamond$ \ds{acacia}.
\end{capt}
\end{linefigure}


\section{Training time}

Figure \ref{fig:test iter summary} details how the training times of
$p$-boosting algorithms compare with AdaBoost.  It is quite clear that
the sloppy and strict algorithms require slightly to substantially
more training iterations than AdaBoost.  Thus, the improved
generalisation performance of these algorithms comes at the cost of
increased training time.  In section \ref{sec:training curves} we will
examine the mechanisms that cause the increased training time, and
suggest remedies.

\begin{linefigure}
\begin{center}
\includegraphics{figures/test_iter_summary}
\end{center}
\begin{capt}{Comparison of AdaBoost and $p$-boosting training
times}{fig:test iter summary} 
Both axes measure numbers of iterations.  The $y$ axis measures the
average number of iterations required for AdaBoost to reach the
minimum of the test error; the $y$ axis is the same statistic for the
$p$-boosting variant (at $p^{\ast}$).  Points above the line indicate
that AdaBoost requires more iterations to train than the $p$-boosting
algorithm.  Uneven error bars are due to the logarithmetic scale.
Marker symbols indicate datasets: 

\noindent{\bf Key:} $\bullet$ \ds{ring0}, $\times$ \ds{ring10}, $\circ$
\ds{ring20}, $\ast$ \ds{ring30}, $\Box$ \ds{sonar}, $\bigtriangleup$
\ds{wpbc}, $\Diamond$ \ds{acacia}.
\end{capt}
\end{linefigure}


\section{Effect of $p$ value on generalisation performance}

Figure \ref{fig:effect of p} expands the information in figure
\ref{fig:test err summary} to include information on \emph{all} values
of $p$ (not just the best value $p^{\ast}$).

Consider first the sloppy algorithm (dashed line in figure
\ref{fig:test err summary}).  When trained on the noisy \ds{ring}
datasets, this algorithm appears to be implementing capacity control
as expected, with the generalisation error reaching a minimum at an
optimal value of $p$ (compare with figure \ref{fig:optimal p value} on
page \pageref{fig:optimal p value}).  It is difficult to pinpoint
exactly where this $p^{\ast}$ value is due to the coarse resolution
along the $p$ scale, but it appears that $0.8 \leq p^{\ast} \leq 1.2$.

The results on other datasets are less clear: the low-noise datasets
\ds{ring0} and \ds{sonar} show a flat curve with no obvious minima
(the slight downward slope indicating that such a minima may exist for
$p > 2$); whereas the curves for \ds{wpbc} and \ds{acacia} both appear
to be almost random.

The performance of the strict algorithm is very poor for $p < 1$ (due
to the algorithm terminating training after few iterations); for $1
\leq p < 2$ it appears to reach a reasonable hypothesis (and a
particularly good one in the case of the \ds{wpbc} and \ds{acacia}
datasets).  It shares the sloppy algorithm's general down-sloping
characteristic on low-noise datasets, and also shares the existence of
an optimal $p$ value on the higher noise datasets.  It is interesting
to note that the $p^{\ast}$ value differs substantially between the
strict and sloppy algorithms (1.7 vs 0.9 on the \ds{ring30} dataset.
This is a counter-intuitive result: the optimal capacity should be an
intrinsic property of the \emph{dataset}, not the algorithm, when both
algorithms are operating from the same hypothesis class $\calX =
\co_p(\calH)$.  One possible explanation is that deficiencies in the
strict algorithm bias the error values for low $p$.

The performance of the na\"{\i}ve algorithm does not deviate
significantly from that of AdaBoost.  The $p$ parameter appears to
have little effect on this algorithm.

In summary, it appears that the strict and sloppy algorithms match the
expected behaviour of an optimal $p$ value minimising test error for
the noisy datasets, but not for low-noise datasets (although there is
an indication that maybe $p^{\ast} > 2$, and thus is off the scale of
figure \ref{fig:effect of p}).

\begin{linefigure}
\begin{center}
\hspace*{-1cm}\includegraphics{figures/effect_of_p}
\end{center}
\begin{capt}{Effect of $p$ on generalisation error}{fig:effect of p}
Average test error of each algorithm over all trials: AdaBoost (thick
line), strict (thin solid), sloppy (dash-dot) and na\"{\i}ve (dotted)
is shown for each dataset.  The bullets $\bullet$ indicate the value
$p^{\ast}$ which is used in figure \ref{fig:test err summary}.  Note
that the sloppy algorithm was not trained on the \ds{wpbc} or
\ds{acacia} datasets.  The $y$ (test error) scale varies.
\end{capt}
\end{linefigure}

\section{Training curves}
\label{sec:training curves}

Figure \ref{fig:training curves} shows a selection of training curves
from the strict and sloppy algorithms, all trained on the \ds{ring30}
dataset.  There are three interesting observations concerning these
curves.  Firstly, part (a) shows that the training error for the
strict algorithm, $p=0.5$ is actually \emph{increasing} with the number
of iterations!  This behaviour often recurs immediately before
training is aborted; it is most likely a consequence of the difficult
cost functional discussed in chapter \ref{chapter:pboosting}.

Secondly, for $p=0.5$ (parts (a) and (d)), the test error and training
error curves follow each other quite closely.  This behaviour has a
theoretical explanation: for low values of $p$, the covering numbers
of $\co_p(\calH)$ (and thus the confidence interval of figure
\ref{fig:boosting generalisation bound form}) are small.  We would
therefore expect the test and training curves to match closely--which
they do.

The third observation concerns the training of the sloppy algorithm.
For $p \in \{0.5, 1\}$ the behaviour is quite erratic on a fine scale,
but steady on average (random oscillations about a steady average
value).  It appears that this algorithm is ``thrashing around'' in the
cost space, unable to find a direction (hypothesis) which will
significantly reduce the cost functional.  We will see shortly that
the algorithm eventually will converge; the $10^3$ iterations were not
sufficient for this to happen.  As a result, the results for the
sloppy algorithm may be somewhat skewed towards higher test error
values.

\begin{linefigure}
\begin{center}
\includegraphics{figures/training_curves_strict}
\includegraphics{figures/training_curves_sloppy}
\end{center}
\begin{capt}{Selected test/training error curves}{fig:training curves}
The curves show training error (grey line) and test error (black
line) against iteration number.  Each point of both curves is
averaged over all trials that had not aborted at the specified
iteration number.
\end{capt}
\end{linefigure}

Figure \ref{fig:weight distribution} is a more detailed look at the
mechanics of the training process for the AdaBoost algorithm (as a
reference) and the strict and sloppy algorithms.

\begin{linefigure}
\begin{center}
\hspace*{-1cm}\includegraphics{figures/weight_distributions}
\end{center}
\begin{capt}{Details of the training process}{fig:weight distribution}
The graphs detail a \emph{single} training run (not average
behaviour).  All three algorithms were trained to 10000 iterations,
using the \ds{ring30} distribution, for $p=1$ (black), $p=0.5$
(dark gray) and $p=1.5$ (light grey).  Parts (a) to (c) show
classifier weights; parts (d) through (f) are the corresponding
training error curves.  Note that (c) contains the \emph{same} data
plotted on two different scales--the two steep curves belong to the
right hand scale.  Note also the log scale on classifier weight graph.
\end{capt}
\end{linefigure}

We first describe the behaviour of AdaBoost, to provide a basis for
comparison with the other algorithms.  Part (a) shows that the
classifier weights of the AdaBoost algorithm decrease as a constant
power of $t$ (note the logarithmetic scale)%
\footnote{A straight line on a log-log plot indicates a relationship
of the form $y = x^{\kappa}$, where $\kappa$ is the slope of the line.}
until training error reaches zero, and then oscillate about this same
value.  Thus, the early weights are slightly larger than the later
weights, but all are of a significant size.

Contrast this behaviour with that of the strict algorithm in part (b).
Considering the $p=1$ curve in part (b)%
\footnote{The $p=0.5$ curve aborted training after 2 iterations.}
it is clear than these weights continue decreasing as $b_t =
t^{-\lambda}$.  The nearly flat training error curve is thus explained
by the classifier weights getting too small for the weak hypotheses
$h_t$ as $t$ increases to have any effect on the combined hypothesis
$H_t$.  The $p=1.5$ curve in part (b) matches the shape of the
AdaBoost curve until approximately 2000 iterations, where the weights
also begin to drop off.  The magnitude of the weights in the flat
section are however much smaller than those of AdaBoost; it may be a
consequence of this fact that the strict algorithm takes about 10
times longer to converge than AdaBoost. 

Part (c) contains some very interesting behaviour which goes some way
towards explaining the noisy part of the training error curve.  Noting
that the scale on the $y$ axis reaches $10^{-300}$, it is clear that
most of the classifier weights are so small as to be practically
zero.  The second obvious feature is the ``kink'' in the $p=1$ and
$p=0.5$ curves at approximately 800 iterations; this kink corresponds
to the end of the noisy sections of part (f) and a (comparatively)
rapid decrease in training error.  The curves in the lower right
corner of (c) ($p=1$ and $p=1.5$) are the \emph{same} data plotted on
the magnified scale (visible on the right hand scale of the graph; the
range is $10^0$ to $10^{-5}$.  The sharpness of the peaks indicate
that only a few iterations have a significant amount of weight (in
other words, $b_t$ is sparse).  This behaviour was expected by design.

In summary, these results indicate that the \emph{concept} of using a
$p$-convex hull is a good one: we saw in figure \ref{fig:test err
summary} that the final hypotheses generated by the sloppy algorithm
often generalise better than AdaBoost, and figure \ref{fig:weight
distribution} shows that this final hypothesis is indeed sparse in
its parameter weights.  The problem is in the \emph{implementation}:
a very large number of iterations are required to generate this sparse
hypothesis; when the weights of hypotheses generated by most
iterations are several \emph{hundred} orders of magnitude below being
significant.


\subsection{Remedies for inefficient training}

Observations in the previous section would suggest that the starting
point for the gradient descent algorithm is particularly bad; as a
result a lot of iterations are wasted in getting to a good starting
point (the kink in part (c)) from where the optimisation can converge
quite quickly. Another optimisation strategy that is less sensitive to
starting point may prove to be more efficient (such as annealing);
this algorithm could prove to be \emph{very} useful if the slow
initial training could be avoided.

Of course, it is not at all clear how to ``start at a different
point'', as the algorithms as currently implemented start in a
$1$-dimensional space and add one dimension per iteration.

Alternatively, it may be possible to accelerate the training of the
sloppy algorithm by choosing a more aggressive cost function.  For
example, the cost function
%
\begin{equation}
c(\alpha) = e^{-\alpha t^\lambda}
\end{equation}
%
where $t$ is the iteration number, and $\lambda>0$ a constant
parameter would serve to accelerate training, where $\lambda$ controls
the degree of acceleration ($\lambda=0$ is equivalent to the sloppy 
algorithm).  The main feature of this cost function is that it gets
steeper as the number of iterations increases, and thus has the
property of guaranteed zero training error.  However, it may be that
this algorithm would have its own problems.  Insufficient time was
available to further investigate this refinement.

\section{Further observations}

The following discussion concerns noteworthy features that were
observed but have not been described previously.

\subsection{Hard datasets}

The full set of training curves in appendix \ref{appendix:allgraphs}
show that very rarely were the learning machines considered here
(including AdaBoost) able to train for more than about 100 iterations
on the \ds{acacia} dataset.  In the case of AdaBoost and the strict
algorithm, the weak learner was not sufficiently good to continue
generating hypotheses with a training error of less than
$\frac{1}{2}$.

The \ds{wpbc} dataset was also difficult, with only about one half of
the trials making it through 1000 training iterations without
terminating.  A stronger weak learning algorithm (such as a deeper
decision tree), or a method of restarting training after it has
aborted (one such is described in \cite{Bauer99}) could be used to 
avoid termination of the algorithm.  Only the strict algorithm
terminated on any other datasets.

\subsection{Na\"{\i}ve $p$-boost}

Observation of figures \ref{fig:test err summary} and \ref{fig:effect
of p} shows that the performance of the na\"{\i}ve algorithm closely
matches that of the AdaBoost algorithm; the variation between the two
being insignificant (the two algorithms are equivalent when $p = 1$.
This is a surprising result--in chapter \ref{chapter:pboosting} it was
shown that the na\"{\i}ve algorithm systematically chooses a
\emph{less than optimal solution} by modifying distance chosen by the
line search.

Thus, we observe that AdaBoost is quite robust to modifications to the
\emph{classifier} weights (of course, these modifications were in the
specific form of a monotonic transform--arbitrary modifications would
likely have an adverse effect).  It would be expected, therefore, that
the \emph{sample} weights must cause the success of the algorithm.
This assertion was also made by Breiman:

\begin{quotation}
After testing [an algorithm equivalent to AdaBoost] I suspected that
its success lay not in its specific form but in its adaptive
resampling property, where increasing weight was placed on those cases
more frequently misclassified.  \cite{Breiman96}
\end{quotation}
