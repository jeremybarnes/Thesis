% stumps.tex
% Jeremy Barnes, 22/9/1999
% $Id$

\input{commands}

\chapter{Decision Stumps}

\section{Description of Decision Stumps}

\section{Performance of Decision Stumps}



\section{Decision stumps and CART}

Both of the unboosted learning algorithms used in this thesis are
tree-based algorithms.  A tree-based algorithm splits the input space
$\calI$ into a number of disjoint regions.  Each of these regions may
again be split, and the algorithm continues in this recursive manner
until a termination condition is met.  Thus a tree structure is built
up.

\subsection{Decision stumps}

The decision stumps algorithm divides the input space into exactly two
regions, each of which is assigned a category.  The tree created is
very small, with only two nodes (hence the name ``decision
\emph{stumps}''.

\subsubsection{Classification}

An example of a decision boundary generated by a decision stumps
classifier is shown in figu


\subsubsection{Training}



The algorithm for the training of a decision stump is given below

\subsection{CART}

\subsubsection{Classification}

The CART algorithm splits its input space into 


  This is then repeated recursively to refine
the classification.

Both 

\subsubsection{Training}







