\input{commands}

\chapter{Introduction}
\label{chapter:intro}

This document describes the status of the honours project
``Boosting on a $p$-complex hull'' being undertaken by Jeremy
Barnes under the supervision of Dr Bob Williamson in the
Engineering Department of the Australian National University.

The investigation concerns a modification to Freund and Schapire's
AdaBoost algorithm (1996) \cite{Freund96}.  The hypothesis $F(\cdot)$
produced by this algorithm improves the performance of a ``weak''
learning algorithm (the \emph{weak learner}) by combining $n$
hypotheses generated by that algorithm $f_1, \ldots, f_n$
in a linear combination $F(\bfx) = \sum_{i=1}^{n} b_i f_i(\bfx)$
subject to the condition that it is a convex combination:
%
\begin{equation}
\label{eqn:intro:p=1}
\|b\|_1 = \sum_{i=1}^{n} b_i = 1 \qquad \mbox{where} \qquad b_i \geq 0
\end{equation}
%
The modification, which is called the ``$p$-boosting'' algorithm
throughout this report, uses a $p$-convex hull
%
\begin{equation}
\|b\|_p = \left( \sum_{i=1}^{n} \left(b_i\right)^p
\right)^{\frac{1}{p}} = 1
\label{eqn:intro:p-convex}
\end{equation}
%
(and is equivalent to boosting when $p=1$).  It
is anticipated that adjusting $p$ will limit the ``richness'' (or
``capacity'') of the class of functions \calF\ that $F$ is a member
of.  This is desireable as richer classes have a tendancy to
\emph{overfit} noisy data (fit the noise).  In particular, it is
expected that there will be an optimal value of $p$ for each
\{dataset, weak learner\} combination that leads to the best
generalisation performance. 

The current implementation (which will be called the \emph{implemented
$p$-boosting algorithm}) does not
implement (\ref{eqn:intro:p-convex}) directly.  Instead, the weights
$b'_i$ generated by AdaBoost are modified to
%
\begin{equation}
b_i = \left( b'_i \right) ^{1/p}
\end{equation}
%
and then combined as a convex combination (\ref{eqn:intro:p=1}); it was
hoped that this would have a similar effect to (\ref{eqn:intro:p-convex}).
The implications of implementing the algorithm in this way are
explored in chapter \ref{chapter:current}.

This document includes chapters on theory, the current status of the
project, and the anticipated path to completion.

\enlargethispage{\baselineskip}
\section*{Acknowledgements}

Thanks to Peter Bartlett for several useful and stimulating conversations.

